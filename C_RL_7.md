

# C-7: Planning and Learning with Tabular Methods

1. Models and Planning
   - Distribution vs. Sample Models
   - Planning with Models
   - Learning and Planning Integration

2. Dyna: Integrated Planning, Acting, and Learning
   - Dyna Architecture
   - Dyna-Q Algorithm
   - Performance Characteristics
   - Maze Example

3. When the Model is Wrong
   - Changing Environments
   - Model Inaccuracies
   - Exploration Bonuses
   - Dyna-Q+ Algorithm

4. Prioritized Sweeping
   - Efficient State Selection
   - Backward Focusing
   - Implementation Details
   - Performance Advantages

5. Expected vs. Sample Updates
   - Computational Trade-offs
   - Relative Advantages
   - Empirical Comparisons
   - Application Guidelines

6. Trajectory Sampling
   - State Distribution for Updates
   - On-policy Distribution
   - Efficiency Considerations
   - Empirical Results

7. Real-time Dynamic Programming
   - RTDP Algorithm
   - On-policy Trajectory Sampling
   - Convergence Properties
   - Example Applications

8. Planning at Decision Time
   - Decision-time vs. Background Planning
   - Computational Considerations
   - Implementation Approaches

9. Heuristic Search
   - State-space Planning
   - Lookahead Search
   - Depth of Search
   - Value Function Role

10. Rollout Algorithms
    - Algorithm Description
    - Policy Improvement Property
    - Implementation Considerations
    - Examples and Applications

11. Monte Carlo Tree Search
    - MCTS Algorithm Components
    - UCT Selection Strategy
    - Applications in Game Playing
    - AlphaGo Connection