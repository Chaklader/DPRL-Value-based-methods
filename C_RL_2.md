

# C-2: Markov Decision Processes

1. The Agent-Environment Interface
   - States, Actions, and Rewards
   - Episodic vs. Continuing Tasks
   - The Markov Property
   - Formal MDP Definition

2. Goals and Rewards
   - Reward Hypothesis
   - Designing Reward Functions
   - Examples and Applications

3. Returns and Episodes
   - Episodic Tasks
   - Continuing Tasks
   - Discounting
   - Mathematical Formulations

4. Unified Notation
   - Episodic and Continuing Tasks
   - Terminal States
   - Time Indexing Conventions

5. Policies and Value Functions
   - Deterministic and Stochastic Policies
   - State-Value Functions
   - Action-Value Functions
   - Bellman Equations for Prediction

6. Optimal Policies and Value Functions
   - Optimal Value Functions
   - Policy Improvement Theorem
   - Bellman Optimality Equation
   - Finding Optimal Policies

7. Optimality and Approximation
   - Computational Complexity
   - Practical Limitations
   - Tabular Methods vs. Approximation

