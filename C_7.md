# C-7: Explainable Deep Reinforcement Learning - Interpretability and Safety Analysis

1. The Black-Box Problem in Neural Networks
    - Why Neural Network Decisions Are Opaque
    - The Need for Explainability in Safety-Critical Systems
    - Taxonomy of Explanation Methods
    - Trade-offs Between Accuracy and Interpretability
2. Gradient-Based Attribution Methods
    - Saliency Maps for Feature Importance
    - Mathematical Foundation of Gradient Attribution
    - Computing and Interpreting Saliency
    - Visualizing Feature Contributions
    - Limitations and Caveats
3. Attention Mechanisms for Interpretability
    - Attention Weights as Feature Importance
    - Softmax Attention Over State Dimensions
    - Attention vs. Saliency: Complementary Perspectives
    - Group-Level Feature Analysis
    - Temporal Attention Patterns
4. Counterfactual Explanation Generation
    - The Logic of Counterfactual Reasoning
    - Optimization-Based Counterfactual Search
    - Constraint Satisfaction for Realistic Counterfactuals
    - Decision Boundary Identification
    - Minimal Perturbation Analysis
5. Policy Distillation and Tree Extraction
    - From Neural Networks to Decision Trees
    - VIPER Algorithm: Iterative Policy Approximation
    - Fidelity vs. Simplicity Trade-offs
    - Rule Extraction and Interpretation
    - Validation and Verification
6. Behavioral Safety Analysis
    - Beyond Average Reward: Safety Metrics
    - Scenario-Based Testing Protocols
    - Maximum Waiting Time Analysis
    - Action Blocking Patterns
    - Modal Service Quality Assessment

---

##### Introduction: The Explainability Imperative

Deep reinforcement learning achieves remarkable performance in complex control tasks, yet the learned policies remain
fundamentally opaque. A DQN agent controlling traffic signals makes thousands of decisions daily, but **why** does it
choose to extend a green phase here, activate bus priority there, or switch to pedestrian phase elsewhere? The neural
network's decision-making process exists as millions of floating-point weights distributed across hidden layers—utterly
inscrutable to human operators.

This opacity creates critical barriers to real-world deployment. Traffic engineers cannot validate decisions they don't
understand. Operators cannot debug failures they cannot trace. Regulators cannot certify systems they cannot audit. Most
fundamentally, **trust** requires transparency: stakeholders must understand not just what the agent does, but why it
does it.

This chapter develops a comprehensive framework for explaining and analyzing DRL agent behavior through multiple
complementary techniques. We progress from gradient-based attribution (revealing which state features influence
decisions) through counterfactual analysis (identifying decision boundaries) to policy distillation (extracting
human-readable rules) and finally behavioral safety analysis (characterizing operational safety across diverse
scenarios). Together, these methods transform the "black box" into a transparent, verifiable system suitable for
safety-critical applications.

---

##### 1. The Black-Box Problem in Neural Networks

###### Why Neural Network Decisions Are Opaque

A trained DQN agent encodes its policy within the weights and biases of its neural network. Consider a traffic control
network with architecture: $45 \to 256 \to 256 \to 128 \to 4$. This network contains:

$$
\begin{align} \text{Parameters} &= (45 \times 256) + 256 + (256 \times 256) + 256 \\
&\quad + (256 \times 128) + 128 + (128 \times 4) + 4 \\
&= 11{,}520 + 65{,}536 + 32{,}768 + 512 \\
&= 110{,}336 \text{ weights and biases} \end{align}
$$

When the agent observes state $\mathbf{s} = [5, 3, 2, 8, \ldots]$ (queue lengths, phase durations, detector readings),
it computes:

$$
\begin{align} \mathbf{h}\_1 &= \text{ReLU}(\mathbf{W}\_1 \mathbf{s} + \mathbf{b}\_1) \\
\mathbf{h}\_2 &= \text{ReLU}(\mathbf{W}\_2 \mathbf{h}\_1 + \mathbf{b}\_2) \\
\mathbf{h}\_3 &= \text{ReLU}(\mathbf{W}\_3 \mathbf{h}\_2 + \mathbf{b}\_3) \\
\mathbf{Q}(\mathbf{s};\theta) &= \mathbf{W}\_4 \mathbf{h}\_3 + \mathbf{b}\_4 = \begin{bmatrix} Q(s,a_0)
\ Q(s,a_1) \ Q(s,a_2) \ Q(s,a_3) \end{bmatrix} \end{align}
$$

The agent selects action $a^* = \arg\max_a Q(\mathbf{s}, a; \theta)$. But **which features** of state $\mathbf{s}$ drove
this decision? Did the agent prioritize the 8-vehicle queue on the east approach, or the 30-second phase duration, or
the pedestrian detector activation? The computation involves 110,000+ multiply-add operations through non-linear
activations—the causal path from input features to output decision is hopelessly entangled.

**The Distributed Representation Problem**: Unlike rule-based controllers where logic is explicit ("IF queue > 10 THEN
extend green"), neural networks distribute knowledge across many weights. A single weight contributes infinitesimally to
many decisions; a single decision depends on thousands of weights. This **distributed representation** provides powerful
generalization but destroys interpretability.

**Example: Two States, Similar Decisions, Different Reasons**:

State A:

$$
\mathbf{s}_A = [\underbrace{12}_{\text{NS queue}}, \underbrace{3}_{\text{EW queue}}, \underbrace{35s}_{\text{duration}}, \ldots]
$$

→ Agent chooses "Continue"

State B:

$$
\mathbf{s}_B = [\underbrace{5}_{\text{NS queue}}, \underbrace{11}_{\text{EW queue}}, \underbrace{15s}_{\text{duration}}, \ldots]
$$

→ Agent chooses "Continue"

Both lead to the same action, but for different reasons:

- State A: Continue because NS queue is large and still being served
- State B: Continue because phase just started (15s duration) and minimum green time not yet met

A human observer seeing only the actions would miss this crucial distinction. The neural network "knows" the difference
(it's encoded in its weights), but this knowledge is implicit and inaccessible.

```mermaid
flowchart TD
    A["Input State s<br>45-dimensional vector<br>[queue lengths, phase info, detectors, ...]"] --> B["Hidden Layer 1<br>256 neurons<br>11,776 parameters"]
    B --> C["Hidden Layer 2<br>256 neurons<br>65,792 parameters"]
    C --> D["Hidden Layer 3<br>128 neurons<br>32,896 parameters"]
    D --> E["Output Q-values<br>4 actions<br>516 parameters"]

    E --> F["Action Selection<br>a = argmax Q(s,a)"]

    F --> G{"Why this action?"}
    G --> H["❓ Which input features<br>were most important?"]
    G --> I["❓ How would changing<br>a feature affect the decision?"]
    G --> J["❓ What rule does<br>the network follow?"]

    style A fill:#E3F2FD
    style B fill:#BBDEFB
    style C fill:#90CAF9
    style D fill:#64B5F6
    style E fill:#42A5F5
    style F fill:#2196F3
    style G fill:#1E88E5
    style H fill:#FCE4EC
    style I fill:#F8BBD9
    style J fill:#F48FB1
```

---

##### The Need for Explainability in Safety-Critical Systems

Traffic signal control exemplifies **safety-critical AI** where decisions directly affect human safety, mobility, and
environmental outcomes. Deploying unexplainable systems creates unacceptable risks:

**1. Validation and Verification**: Traditional traffic controllers undergo rigorous testing against safety standards.
Engineers verify that minimum green times are respected, conflicting movements never receive simultaneous green,
pedestrian demands are served within acceptable delays. How can we verify a neural network satisfies these constraints
when we cannot examine its decision logic?

**2. Debugging and Failure Analysis**: When a rule-based controller causes excessive delays, engineers trace the fault:
"The bus priority logic triggered incorrectly due to faulty sensor data." With a neural network, diagnosis becomes: "The
network output Q-value 3.2 for 'Continue' instead of 2.8 for 'Next Phase'." This provides no actionable insight into the
root cause.

**3. Operator Trust and Acceptance**: Traffic management centers require operators to monitor and occasionally override
automated systems. Operators cannot effectively supervise a system whose reasoning they cannot follow. Research shows
that users disengage from systems they don't understand, defeating the purpose of human oversight.

**4. Regulatory Approval**: Transportation agencies must certify that signal control systems meet safety and performance
standards. Regulatory frameworks assume inspectable, documented control logic. Neural networks provide neither—they
cannot produce the control logic flowcharts and timing diagrams regulators require.

**5. Accountability and Liability**: If an AI-controlled intersection experiences an incident, legal and ethical
questions arise: Was the AI at fault? Did it violate design specifications? Without explainability, these questions
become unanswerable.

**Real-World Consequence Example**:

Consider a pedestrian who waited 90 seconds for a crossing signal—far exceeding the 60-second maximum specified in
design requirements. Without explainability tools:

**Black-Box Response**: "The neural network learned through trial and error in simulation. It optimizes a complex reward
function. We cannot explain why it made this specific decision, but average performance exceeds the baseline."

**Explainable System Response**: "Analysis reveals the agent assigned low attention weight (α = 0.08) to pedestrian
waiting time in this state because heavy vehicle queues (24 cars) dominated the state representation. Counterfactual
analysis shows that if the vehicle queue were 15 instead of 24, the agent would have activated the pedestrian phase.
This suggests the reward function underweights pedestrian priority relative to vehicle throughput. We are retraining
with adjusted reward weights."

The latter enables correction; the former does not.

---

##### Taxonomy of Explanation Methods

Explainability techniques fall into several categories based on their approach and the type of insight they provide:

**1. Local vs. Global Explanations**:

- **Local**: Explain a single decision in a specific state
    - "Why did the agent choose 'Continue' in this particular traffic situation?"
    - Examples: Saliency maps, counterfactuals, attention weights for specific states
- **Global**: Explain the overall policy or decision boundaries
    - "What general rules does the agent follow?"
    - Examples: Decision tree extraction, policy summarization, feature importance across all states

**2. Model-Specific vs. Model-Agnostic**:

- **Model-Specific**: Exploit internal structure of the specific model architecture
    - Gradient-based methods (saliency), attention mechanisms
    - Require access to model weights and gradients
- **Model-Agnostic**: Treat the model as a black box, analyzing only inputs and outputs
    - Counterfactuals, decision tree approximation, behavioral analysis
    - Work with any model, even non-differentiable ones

**3. Intrinsic vs. Post-Hoc**:

- **Intrinsic**: Explainability built into the model architecture
    - Attention mechanisms, linear models, decision trees
    - Explanation is part of the model's computation
- **Post-Hoc**: Explanation generated after training by analyzing the trained model
    - Saliency, counterfactuals, VIPER distillation
    - Model trained for performance; explanation added afterward

**4. Example-Based vs. Feature-Based**:

- **Example-Based**: Explain by reference to similar examples or cases
    - "This decision resembles what the agent did in previous situation X"
    - Counterfactuals: "If conditions were different, the decision would change"
- **Feature-Based**: Identify which input features drive the decision
    - "Queue length was the most important factor"
    - Saliency, attention weights, feature importance

```mermaid
flowchart TD
    A["Explainability Methods<br>for DRL Traffic Control"] --> B["Local Explanations"]
    A --> C["Global Explanations"]

    B --> D["Saliency Maps<br>Feature importance<br>for specific state"]
    B --> E["Attention Weights<br>Which features<br>agent focuses on"]
    B --> F["Counterfactuals<br>What-if scenarios<br>for this state"]

    C --> G["Decision Trees<br>Interpretable rules<br>approximating policy"]
    C --> H["Behavioral Analysis<br>Performance patterns<br>across scenarios"]
    C --> I["Feature Importance<br>Aggregate statistics<br>across all states"]

    D --> J["Combined<br>Multi-Method<br>Framework"]
    E --> J
    F --> J
    G --> J
    H --> J
    I --> J

    J --> K["Comprehensive<br>Understanding<br>of Agent Behavior"]

style A fill:#FCE4EC
style B fill:#F8BBD9
style C fill:#F48FB1
style D fill:#F06292
style E fill:#EC407A
style F fill:#E91E63
style G fill:#D81B60
style H fill:#E1BEE7
style I fill:#CE93D8
style J fill:#BA68C8
style K fill:#AB47BC
```

---

##### Trade-offs Between Accuracy and Interpretability

A fundamental tension exists between model performance and interpretability. We can visualize this relationship:

**Interpretability Spectrum**:

| Model Type          | Interpretability | Accuracy (Complex Problems) | Example                            |
| ------------------- | ---------------- | --------------------------- | ---------------------------------- |
| Linear Models       | ★★★★★            | ★☆☆☆☆                       | $Q(s,a) = \mathbf{w}^T \mathbf{s}$ |
| Decision Trees      | ★★★★☆            | ★★☆☆☆                       | IF-THEN rules                      |
| Shallow Neural Nets | ★★☆☆☆            | ★★★☆☆                       | 1-2 hidden layers                  |
| Deep Neural Nets    | ★☆☆☆☆            | ★★★★★                       | DQN with 3+ layers                 |

**Why This Trade-Off Exists**:

**Expressiveness Requires Complexity**: Simple, interpretable models (linear, small trees) cannot capture the intricate,
non-linear relationships in complex domains. Traffic dynamics involve:

- Non-linear queue evolution: $q_{t+1} = \max(0, q_t + \lambda - \mu_t)$
- Multiplicative interactions: bus arrival × queue length × phase duration
- Temporal dependencies: decisions at time $t$ affect outcomes at $t+10$

Deep networks can represent these complex functions; linear models cannot.

**Distributed Representations Obscure Logic**: Deep learning's power comes from distributing knowledge across many
weights. But this distribution destroys the one-to-one correspondence between model parameters and human-interpretable
concepts.

**The Explainability Solution**: Rather than restricting models to interpretable architectures (sacrificing
performance), we train powerful but opaque models and then **explain them post-hoc**. This "best of both worlds"
approach achieves:

- **High Performance**: Use the most effective model architecture (deep neural networks)
- **Interpretability**: Apply explanation techniques to understand decisions

**Methodological Framework**:

$$
\text{Deployment Pipeline} = \underbrace{\text{Train Complex Model}}_{\text{Optimize Performance}} \to
\underbrace{\text{Generate Explanations}}_{\text{Build Understanding}} \to \underbrace{\text{Validate\_\&\_Verify}}\_{\text{Ensure Safety}}
$$

This framework underlies modern explainable AI: optimize first, explain second, validate third.

---

#### 2. Gradient-Based Attribution Methods

##### Saliency Maps for Feature Importance

**Core Idea**: If a small change to input feature $s_i$ causes a large change in output $Q(s,a)$, then feature $s_i$
must be important for that decision. Mathematically, this sensitivity is captured by the **gradient**:

$$
\text{Saliency}\_i(s,a) = \left|\frac{\partial Q(s,a;\theta)}{\partial s_i}\right|
$$

The magnitude $|\partial Q / \partial s_i|$ measures how strongly Q-value depends on feature $i$. High magnitude → high
importance; low magnitude → low importance.

**Intuition Through Analogy**: Imagine adjusting the temperature in a room. The thermostat display shows 20°C. If
turning the dial 1° causes the display to jump to 25°C, temperature is highly sensitive to the dial. If turning the dial
1° changes the display to 20.2°C, temperature is insensitive. The rate of change (gradient) reveals sensitivity.

Similarly, if increasing queue length by 1 vehicle causes $Q(s,\text{Continue})$ to drop from 5.2 to 3.1, queue length
is highly important. If the same increase only changes Q from 5.2 to 5.15, queue length is less important.

##### Mathematical Foundation of Gradient Attribution

Given a trained DQN $Q(s,a;\theta)$, the gradient with respect to input state features is:

$$
\nabla_s Q(s,a;\theta) = \begin{bmatrix} \frac{\partial Q}{\partial s_1} \ \frac{\partial Q}{\partial s_2} \\ \vdots \\
\frac{\partial Q}{\partial s_n} \end{bmatrix}
$$

Each component $\frac{\partial Q}{\partial s_i}$ can be computed efficiently via backpropagation—the same algorithm used
for training, but applied to the input instead of the weights.

**Forward Pass** (standard Q-value computation):

$$
\begin{align} \mathbf{h}\_1 &= \text{ReLU}(\mathbf{W}\_1 \mathbf{s} + \mathbf{b}\_1) \\
\mathbf{h}\_2 &= \text{ReLU}(\mathbf{W}\_2 \mathbf{h}\_1 + \mathbf{b}\_2) \\
\mathbf{h}\_3 &= \text{ReLU}(\mathbf{W}\_3 \mathbf{h}\_2 + \mathbf{b}\_3) \\
Q(s,a) &= [\mathbf{W}_4 \mathbf{h}_3 + \mathbf{b}_4]\_a \quad \text{select component for action } a \end{align}
$$

**Backward Pass** (gradient computation using chain rule):

$$
\frac{\partial Q}{\partial \mathbf{s}} = \frac{\partial Q}{\partial \mathbf{h}\_3} \frac{\partial
\mathbf{h}\_3}{\partial \mathbf{h}\_2} \frac{\partial \mathbf{h}\_2}{\partial \mathbf{h}\_1} \frac{\partial
\mathbf{h}\_1}{\partial \mathbf{s}}
$$

Where:

$$
\begin{align}
\frac{\partial Q}{\partial \mathbf{h}\_3} &= \mathbf{W}\_4[a,:] \quad \text{(row } a \text{ of }
\mathbf{W}_4 \text{)} \\
\frac{\partial \mathbf{h}_k}{\partial \mathbf{h}_{k-1}} &= \text{diag}(\mathbf{1}_{[\mathbf{h}_k > 0]}) \mathbf{W}\_k \quad \text{(ReLU derivative)}
\end{align}
$$

The ReLU derivative creates sparsity:

$$
\frac{\partial \text{ReLU}(x)}{\partial x} = \begin{cases} 1 & x > 0 \\ 0 & x \leq 0 \end{cases}
$$

Only neurons that were activated ($h > 0$) contribute to the gradient.

**Computational Efficiency**: Modern deep learning frameworks (PyTorch, TensorFlow) compute these gradients
automatically via automatic differentiation. A single backward pass computes gradients for all input features
simultaneously—no need to perturb each feature individually.

---

##### Computing and Interpreting Saliency

**Algorithm for Saliency Computation**:

1. **Input**: State $\mathbf{s}$, action $a$, trained DQN $Q(s,a;\theta)$
2. **Forward Pass**: Compute $Q(\mathbf{s}, a; \theta)$
3. **Backward Pass**: Compute gradient $\nabla_{\mathbf{s}} Q(\mathbf{s}, a; \theta)$
4. **Saliency Vector**: $\mathbf{S}(s,a) = |\nabla_{\mathbf{s}} Q(\mathbf{s}, a; \theta)|$ (element-wise absolute value)
5. **Normalize** (optional): $\mathbf{S}_{\text{norm}} = \frac{\mathbf{S}}{|\mathbf{S}|_1}$ so values sum to 1

**Worked Example**: Traffic State with 4 Features

State:
$\mathbf{s} = \begin{bmatrix} 10 \ 3 \ 25 \ 1 \end{bmatrix} = \begin{bmatrix} \text{NS queue} \ \text{EW queue} \ \text{Phase duration (s)} \ \text{Bus present (0/1)} \end{bmatrix}$

Action: $a = \text{Continue}$

Suppose the trained network gives:

$$
Q(s, \text{Continue}) = 4.2
$$

Computing gradients:

$$
\nabla\_{\mathbf{s}} Q = \begin{bmatrix} -0.82 \ -0.15 \ -0.31 \ +2.14 \end{bmatrix}
$$

Saliency (absolute values):

$$
\mathbf{S} = \begin{bmatrix} 0.82 \\ 0.15 \\ 0.31 \\ 2.14 \end{bmatrix}
$$

Normalized:

$$
\mathbf{S}\_{\text{norm}} = \frac{1}{0.82 + 0.15 + 0.31 + 2.14} \begin{bmatrix} 0.82 \\ 0.15 \\ 0.31 \\ 2.14
\end{bmatrix} = \begin{bmatrix} 0.24 \\ 0.04 \\ 0.09 \\ 0.63 \end{bmatrix}
$$

**Interpretation**:

- **Bus presence (63%)**: Dominates the decision. The network is highly sensitive to whether a bus is present.
- **NS queue (24%)**: Second most important. Larger queues influence the Continue decision.
- **Phase duration (9%)**: Moderate importance. How long the phase has run affects continuation.
- **EW queue (4%)**: Minimal importance. The minor approach queue has little influence in this state.

**Key Insight**: The agent prioritizes bus presence and serving the major approach (NS). The negative gradient for NS
queue means: if NS queue increases, $Q(\text{Continue})$ decreases—the agent becomes less inclined to continue when
queues grow too large. The positive gradient for bus presence means: if a bus appears, $Q(\text{Continue})$
increases—the agent wants to serve the bus.

---

##### Visualizing Feature Contributions

Saliency maps are often visualized as heatmaps or bar charts to aid interpretation:

**Heatmap Visualization**: For spatial/structured inputs (like images or gridded data), represent saliency as color
intensity. Brighter colors indicate higher importance.

**Bar Chart Visualization**: For vector inputs (like traffic states), display saliency values as bars:

```
Feature Importance (Saliency)
───────────────────────────────────────
Bus Present      ████████████████████████████ 63%
NS Queue         ████████████ 24%
Phase Duration   ████ 9%
EW Queue         █ 4%
```

**Time-Series Visualization**: Track how saliency changes over time as traffic conditions evolve:

| Timestep | NS Queue Sal. | Bus Present Sal. | Phase Dur. Sal. |
| -------- | ------------- | ---------------- | --------------- |
| t=100    | 0.35          | 0.15             | 0.40            |
| t=200    | 0.28          | 0.52             | 0.12            |
| t=300    | 0.41          | 0.08             | 0.33            |

At $t=200$, bus presence spikes to 52% saliency—the agent strongly reacted to bus arrival.

**Action-Specific Saliency**: Compare saliency across different actions:

For state $\mathbf{s}$ with actions: Continue, Skip-to-P1, Next Phase

|             | Continue | Skip-to-P1 | Next Phase |
| ----------- | -------- | ---------- | ---------- |
| NS Queue    | 0.24     | 0.18       | 0.39       |
| EW Queue    | 0.04     | 0.05       | 0.21       |
| Phase Dur   | 0.09     | 0.08       | 0.31       |
| Bus Present | 0.63     | 0.69       | 0.09       |

**Analysis**:

- **Bus presence** dominates Continue and Skip-to-P1 (bus priority actions)
- **Phase duration** becomes most important for Next Phase (timing-based decision)
- **EW queue** gains importance for Next Phase (switching to serve minor approach)

This reveals the agent's decision structure: bus-related actions depend on bus state, while phase progression depends on
timing and cross-approach demand.

---

##### Limitations and Caveats

While powerful, gradient-based saliency has important limitations:

**1. Local Linearity Assumption**: Saliency measures the **local** gradient—the instantaneous rate of change at the
current state. It assumes a linear approximation:

$$
Q(s + \Delta s, a) \approx Q(s,a) + \nabla_s Q(s,a)^T \Delta s
$$

This is accurate for small perturbations but breaks down for large changes. If queue length increases from 10 to 30
vehicles (large change), the linear approximation may be poor.

**2. Saturation Effects**: ReLU activations create "dead zones" where gradients vanish. If a neuron's input is deeply
negative (ReLU outputs 0), the gradient through that neuron is zero, even though the feature might be important if we
perturbed it enough to activate the neuron.

**Example**:

$$
h = \text{ReLU}(w_1 s_1 + w_2 s_2 + b) = \text{ReLU}(-10 + 0.5 s_1 + 0.3 s_2)
$$

If $s_1 = 5, s_2 = 3$, then input $= -10 + 2.5 + 0.9 = -6.6 < 0$, so $h = 0$ and
$\frac{\partial h}{\partial s_1} = \frac{\partial h}{\partial s_2} = 0$.

Saliency suggests $s_1, s_2$ are unimportant, but if we increased $s_1$ to 15, the neuron would activate and influence
the output.

**3. Correlation vs. Causation**: High saliency indicates **association**, not necessarily **causation**. If two
features are highly correlated (e.g., NS queue length and NS average waiting time), both may have high saliency even if
only one is truly causal.

**4. Adversarial Brittleness**: Neural networks can be highly sensitive to imperceptible input perturbations
(adversarial examples). A saliency map might assign high importance to a feature that changes the Q-value dramatically
but doesn't reflect genuine decision logic—it's an artifact of the network's learned function.

**5. Lack of Context**: Saliency identifies important features but doesn't explain **how** they interact or **why** they
matter. Knowing "bus presence has 63% saliency" doesn't tell us whether the network activates bus priority or ignores
buses in this state.

**Mitigation Strategies**:

- **Multiple Methods**: Combine saliency with other techniques (attention, counterfactuals) for triangulation
- **Integrated Gradients**: A refinement of saliency that addresses some saturation issues by integrating gradients
  along a path from a baseline to the input
- **SmoothGrad**: Average saliency over small random perturbations to reduce noise
- **Validation**: Test saliency insights through ablation—does zeroing high-saliency features actually change the
  decision?

Despite limitations, saliency maps remain one of the most computationally efficient and widely applicable explainability
tools for neural networks.

```mermaid
flowchart TD
    A["Input State s"] --> B["Compute Q(s,a;θ)<br>Forward Pass"]
    B --> C["Compute Gradient<br>∇_s Q(s,a;θ)<br>Backward Pass"]
    C --> D["Saliency Map<br>S = |∇_s Q|"]

    D --> E["Interpretation"]
    E --> E1["High Saliency<br>Feature strongly<br>influences decision"]
    E --> E2["Low Saliency<br>Feature weakly<br>influences decision"]
    E --> E3["Sign of Gradient<br>Direction of influence:<br>positive or negative"]

    E1 --> F["Validate<br>Interpretation"]
    E2 --> F
    E3 --> F

    F --> F1["Ablation Test:<br>Zero high-saliency features"]
    F --> F2["Perturbation Test:<br>Change features slightly"]
    F --> F3["Cross-Method:<br>Compare with attention,<br>counterfactuals"]

    style A fill:#E3F2FD
    style B fill:#BBDEFB
    style C fill:#90CAF9
    style D fill:#64B5F6
    style E fill:#42A5F5
    style E1 fill:#2196F3
    style E2 fill:#1E88E5
    style E3 fill:#1976D2
    style F fill:#1565C0
    style F1 fill:#0D47A1
    style F2 fill:#01579B
    style F3 fill:#0277BD
```

---

#### 3. Attention Mechanisms for Interpretability

##### Attention Weights as Feature Importance

While saliency measures sensitivity through gradients, **attention mechanisms** provide an alternative perspective: they
compute explicit weights $\alpha_i$ that represent how much the network "focuses on" each input feature when making
decisions.

**Core Concept**: Instead of asking "how sensitive is the output to this feature?" (saliency), attention asks "how much
does the network prioritize this feature?" The distinction is subtle but important:

- **Saliency**: Measures marginal impact if feature changes
- **Attention**: Measures relative importance in current computation

**Biological Motivation**: Human attention is selective—we cannot process all sensory information equally, so we focus
on relevant stimuli. Similarly, not all state features are equally relevant for every decision. In a traffic state with
32 dimensions, perhaps only 5-6 features drive the current decision; the rest are contextual or irrelevant.

Attention mechanisms formalize this selective focus by computing a probability distribution over features:

$$
\alpha_i = \mathbb{P}(\text{feature } i \text{ is relevant for current decision})
$$

where $\sum_{i=1}^{n} \alpha_i = 1$ and $\alpha_i \geq 0$.

---

##### Softmax Attention Over State Dimensions

The standard attention mechanism uses softmax normalization to convert arbitrary scores into a probability distribution:

$$
\alpha_i = \frac{\exp(e_i)}{\sum_{j=1}^{n} \exp(e_j)}
$$

where $e_i$ is an **attention score** for feature $i$. The question becomes: how do we compute $e_i$?

**Gradient-Based Attention Score**: A simple, effective approach uses gradient magnitude as the attention score:

$$
e_i = \left|\frac{\partial Q(s,a;\theta)}{\partial s_i}\right|
$$

This connects attention to saliency, but the softmax normalization provides interpretability advantages:

1. **Bounded**: $\alpha_i \in [0,1]$ (unlike gradients which can be arbitrarily large)
2. **Normalized**: $\sum_i \alpha_i = 1$ (allocates 100% "attention budget" across features)
3. **Relative**: High $\alpha_i$ means feature $i$ is more important than others in this state, even if absolute
   gradient is small

**Worked Example**: Same traffic state as before

State: $\mathbf{s} = [10, 3, 25, 1]$ (NS queue, EW queue, duration, bus)

Gradients: $\nabla_s Q = [-0.82, -0.15, -0.31, +2.14]$

Attention scores (absolute gradients): $\mathbf{e} = [0.82, 0.15, 0.31, 2.14]$

Softmax attention weights:

$$
\begin{align}
\alpha_1 &= \frac{\exp(0.82)}{\exp(0.82) + \exp(0.15) + \exp(0.31) + \exp(2.14)} = \frac{2.27}{2.27 + 1.16 + 1.36 +
8.50} = \frac{2.27}{13.29} = 0.17 \\
\alpha_2 &= \frac{\exp(0.15)}{13.29} = \frac{1.16}{13.29} = 0.09 \\
\alpha_3 &= \frac{\exp(0.31)}{13.29} = \frac{1.36}{13.29} = 0.10 \\
\alpha_4 &= \frac{\exp(2.14)}{13.29} = \frac{8.50}{13.29} = 0.64
\end{align}
$$

**Interpretation**: The agent allocates 64% of its "attention" to bus presence, 17% to NS queue, 10% to phase duration,
and 9% to EW queue. This quantifies the relative importance more intuitively than raw gradients.

**Temperature Scaling**: We can adjust the "sharpness" of attention via temperature $\tau$:

$$
\alpha_i = \frac{\exp(e_i/\tau)}{\sum_j \exp(e_j/\tau)}
$$

- **Low $\tau$ (< 1)**: Makes attention more concentrated (high $\alpha$ for max $e$, low $\alpha$ for others)
- **High $\tau$ (> 1)**: Smooths attention (more uniform distribution)
- **$\tau \to 0$**: Attention becomes one-hot (all weight on largest $e_i$)
- **$\tau \to \infty$**: Attention becomes uniform ($\alpha_i = 1/n$ for all $i$)

For interpretability, $\tau = 1$ (standard softmax) is typical.

---

##### Attention vs. Saliency: Complementary Perspectives

While both methods use gradients, they provide different insights:

| Aspect             | Saliency                            | Attention                                      |
| ------------------ | ----------------------------------- | ---------------------------------------------- |
| **Output**         | Gradient magnitude                  | Normalized probability distribution            |
| **Scale**          | Unbounded                           | $[0, 1]$, sum to 1                             |
| **Interpretation** | Absolute importance                 | Relative importance                            |
| **Comparison**     | Hard to compare across states       | Easy to compare (percentages)                  |
| **Sparsity**       | Can be very sparse (many near-zero) | Softmax naturally concentrates on top features |

**Example Highlighting the Difference**:

State A: Gradients $[0.5, 0.3, 0.1, 0.05]$ → Saliency $[0.5, 0.3, 0.1, 0.05]$ → Attention $[0.39, 0.29, 0.19, 0.13]$

State B: Gradients $[5.0, 3.0, 1.0, 0.5]$ → Saliency $[5.0, 3.0, 1.0, 0.5]$ → Attention $[0.56, 0.31, 0.08, 0.05]$

**Saliency**: State B has much larger gradients (10× larger). Suggests network is much more sensitive in State B.

**Attention**: State B has only slightly higher concentration on top feature (56% vs 39%). The **relative** importance
structure is similar, though absolute sensitivity differs.

For comparing across states with different scales, attention is more interpretable. For understanding absolute
sensitivity, saliency is clearer.

---

##### Group-Level Feature Analysis

Traffic states often contain semantically related features that should be analyzed together. Rather than examining
individual features in isolation, we can aggregate attention over **feature groups**:

**Example Feature Grouping** (32-dimensional state):

- **TLS3 Phase Encoding**: Features 0-3 (one-hot: P1, P2, P3, P4)
- **TLS3 Timing**: Features 4, 15 (duration, simulation time)
- **TLS3 Vehicle Detectors**: Features 5-8 (4 approaches)
- **TLS3 Bicycle Detectors**: Features 9-12 (4 approaches)
- **TLS3 Bus Info**: Features 13-14 (presence, waiting time)
- **TLS6 Phase Encoding**: Features 16-19
- **TLS6 Timing**: Features 20, 31
- **TLS6 Vehicle Detectors**: Features 21-24
- **TLS6 Bicycle Detectors**: Features 25-28
- **TLS6 Bus Info**: Features 29-30

**Group Attention**: Sum attention weights within each group:

$$
\alpha_{\text{group}} = \sum_{i \in \text{group}} \alpha_i
$$

**Worked Example**:

Individual attention weights (32 features):

$$
\boldsymbol{\alpha} = [0.05, 0.02, 0.03, 0.01, \underbrace{0.12}_{\text{TLS3 duration}}, 0.08, 0.06, 0.04, 0.02,
\ldots]
$$

Grouped attention:

| Group               | Features | Group Attention                    |
| ------------------- | -------- | ---------------------------------- |
| TLS3 Phase Encoding | 0-3      | $0.05 + 0.02 + 0.03 + 0.01 = 0.11$ |
| TLS3 Timing         | 4, 15    | $0.12 + 0.08 = 0.20$               |
| TLS3 Vehicle Det    | 5-8      | $0.08 + 0.06 + 0.04 + 0.02 = 0.20$ |
| TLS3 Bus Info       | 13-14    | $0.03 + 0.02 = 0.05$               |
| TLS6 Phase Encoding | 16-19    | $0.06 + 0.04 + 0.02 + 0.01 = 0.13$ |
| TLS6 Timing         | 20, 31   | $0.10 + 0.06 = 0.16$               |
| TLS6 Vehicle Det    | 21-24    | $0.05 + 0.04 + 0.03 + 0.02 = 0.14$ |
| TLS6 Bus Info       | 29-30    | $0.01 + 0.00 = 0.01$               |

**Interpretation**:

- **TLS3 timing (20%)** and **TLS3 vehicle detectors (20%)** receive equal top priority
- **TLS6 timing (16%)** and **TLS6 vehicle detectors (14%)** are secondary
- **Bus information** receives minimal attention (5% + 1% = 6% total)
- **Phase encoding** (which phase is active) gets moderate attention (11% + 13% = 24% total)

This reveals that the agent primarily considers timing and vehicle demand at both intersections, with limited focus on
bus priority in this particular state.

---

##### Temporal Attention Patterns

Tracking attention over time reveals how the agent's focus shifts as traffic conditions evolve:

**Example Scenario**: Rush hour approaching

| Time    | TLS3 Veh Det | TLS3 Timing | Bus Info | TLS6 Veh Det |
| ------- | ------------ | ----------- | -------- | ------------ |
| 7:00 AM | 15%          | 25%         | 5%       | 18%          |
| 7:30 AM | 28%          | 22%         | 8%       | 25%          |
| 8:00 AM | 35%          | 18%         | 12%      | 27%          |
| 8:30 AM | 42%          | 15%         | 6%       | 30%          |

**Pattern Analysis**:

- **Vehicle detector attention increases** from 15%→42% and 18%→30% as traffic builds
- **Timing attention decreases** from 25%→15% as immediate queue demands override phase duration concerns
- **Bus attention spikes** briefly at 8:00 AM (bus arrival) then drops when bus departs

This temporal analysis reveals the agent's **adaptive attention allocation**: during light traffic, timing dominates;
during congestion, queue lengths dominate; when buses appear, bus priority briefly gains attention.

**Visualization**:

```
Attention Over Time
TLS3 Vehicle Detectors: [████████░░] 15% → [████████████████████░░] 42%
TLS3 Timing:           [█████████████░░] 25% → [███████░░] 15%
Bus Info:              [███░░] 5% → [██████░░] 12% → [███░░] 6%
```

The shifting bars illustrate dynamic prioritization—a key advantage of learning-based control over fixed-priority
rule-based systems.

---

```mermaid
flowchart TD
    A["State s<br>32 features"] --> B["Compute Gradients<br>∇_s Q(s,a;θ)"]
    B --> C["Attention Scores<br>e_i = |∂Q/∂s_i|"]
    C --> D["Softmax Normalization<br>α_i = exp(e_i) / Σ exp(e_j)"]

    D --> E["Feature-Level<br>Attention Weights<br>α = [α₁, α₂, ..., α₃₂]"]

    E --> F["Group-Level<br>Aggregation"]
    F --> F1["TLS3 Detectors:<br>Σ α_i for i∈{5,6,7,8}"]
    F --> F2["TLS3 Timing:<br>Σ α_i for i∈{4,15}"]
    F --> F3["Bus Info:<br>Σ α_i for i∈{13,14,29,30}"]

    F1 --> G["Interpretation"]
    F2 --> G
    F3 --> G

    G --> H["Which modality<br>dominates?"]
    G --> I["Which intersection<br>(TLS3 vs TLS6)?"]
    G --> J["Temporal vs Spatial<br>features?"]

    style A fill:#E3F2FD
    style B fill:#BBDEFB
    style C fill:#90CAF9
    style D fill:#64B5F6
    style E fill:#42A5F5
    style F fill:#2196F3
    style F1 fill:#1E88E5
    style F2 fill:#1976D2
    style F3 fill:#1565C0
    style G fill:#0D47A1
    style H fill:#01579B
    style I fill:#0277BD
    style J fill:#039BE5
```

---

#### 4. Counterfactual Explanation Generation

##### The Logic of Counterfactual Reasoning

Counterfactual explanations answer "what-if" questions: **"What would need to be different for the agent to choose a
different action?"** This form of explanation is both intuitive and actionable:

**Factual Statement**: "The agent chose to Continue the current phase."

**Counterfactual Statement**: "If the NS queue were 8 vehicles instead of 14, the agent would have chosen Next Phase."

Counterfactuals identify **decision boundaries**—the precise thresholds where agent behavior changes. This reveals the
agent's implicit rules more directly than saliency or attention alone.

**Formal Definition**: Given state $\mathbf{s}$ where agent selects action $a^* = \arg\max_a Q(\mathbf{s}, a; \theta)$,
a counterfactual state $\mathbf{s}'$ satisfies:

1. **Different Decision**: $\arg\max_a Q(\mathbf{s}', a; \theta) \neq a^*$
2. **Minimal Perturbation**: $|\mathbf{s}' - \mathbf{s}|$ is minimized
3. **Realism**: $\mathbf{s}' \in \mathcal{S}_{\text{feasible}}$ (satisfies domain constraints)

The counterfactual reveals: "The minimal change to state that would flip the decision is..."

**Why Counterfactuals Matter**:

**Interpretability**: Counterfactuals provide concrete, testable explanations. "If queue decreased by 6 vehicles" is
more actionable than "queue has 24% saliency."

**Decision Boundaries**: Counterfactuals map the boundaries between different actions in state space. This reveals where
the agent's policy is sensitive vs. robust.

**Validation**: Engineers can test whether identified thresholds align with traffic engineering principles. "Agent
switches at queue = 8" can be validated against theoretical queue capacity.

**Debugging**: If a counterfactual reveals unrealistic thresholds ("agent would switch if queue = 0.2 vehicles"), this
signals a training problem.

---

##### Optimization-Based Counterfactual Search

Finding counterfactuals is an optimization problem. We seek:

$$
\mathbf{s}'_{\text{CF}} = \arg\min_{\mathbf{s}' \in \mathcal{S}} |\mathbf{s}' - \mathbf{s}|_2 \quad \text{subject to}
\quad \arg\max_a Q(\mathbf{s}', a; \theta) = a_{\text{target}}
$$

where $a_{\text{target}} \neq a^*$ is the desired alternative action.

**Challenge**: The constraint $\arg\max_a Q(\mathbf{s}', a) = a_{\text{target}}$ is discrete and non-differentiable. We
cannot directly optimize with gradient descent.

**Solution**: Replace the hard constraint with a soft penalty that encourages
$Q(\mathbf{s}', a_{\text{target}}) > Q(\mathbf{s}', a^*)$:

$$
\mathcal{L}(\mathbf{s}') = \underbrace{|\mathbf{s}' - \mathbf{s}|_2^2}_{\text{Proximity}} + \lambda
\underbrace{\max(0, Q(\mathbf{s}', a^*; \theta) - Q(\mathbf{s}', a_{\text{target}}; \theta) +
\epsilon)}_{\text{Decision Flip}}
$$

**Loss Breakdown**:

**1. Proximity Term**: $|\mathbf{s}' - \mathbf{s}|_2^2$ penalizes large deviations. Smaller perturbations are preferred.

**2. Decision Flip Term**: $\max(0, Q(\mathbf{s}', a^*) - Q(\mathbf{s}', a_{\text{target}}) + \epsilon)$

- If $Q(\mathbf{s}', a_{\text{target}}) > Q(\mathbf{s}', a^*) + \epsilon$: Term = 0 (constraint satisfied)
- Otherwise: Positive penalty proportional to how far $Q(a_{\text{target}})$ needs to increase

**3. Margin $\epsilon$**: Ensures $a_{\text{target}}$ wins by a clear margin, not just ties. Typical: $\epsilon = 0.1$
to $0.5$.

**4. Trade-off Parameter $\lambda$**: Controls balance between proximity and constraint satisfaction. Large $\lambda$
prioritizes flipping the decision; small $\lambda$ prioritizes minimal change.

**Gradient-Based Optimization**: Use gradient descent on $\mathbf{s}'$:

$$
\mathbf{s}'\_{t+1} = \mathbf{s}'_t - \eta \nabla_{\mathbf{s}'} \mathcal{L}(\mathbf{s}'\_t)
$$

The gradient is:

$$
\nabla_{\mathbf{s}'} \mathcal{L} = 2(\mathbf{s}' - \mathbf{s}) + \lambda \mathbf{1}_{[\text{constraint violated}]}
\left[\nabla_{\mathbf{s}'} Q(\mathbf{s}', a^*) - \nabla_{\mathbf{s}'} Q(\mathbf{s}', a\_{\text{target}})\right]
$$

**Algorithm**:

1. Initialize: $\mathbf{s}'_0 = \mathbf{s}$ (start at original state)
2. For $t = 1, 2, \ldots, T_{\max}$:
    - Compute loss $\mathcal{L}(\mathbf{s}'_t)$
    - Compute gradient $\nabla_{\mathbf{s}'} \mathcal{L}(\mathbf{s}'_t)$
    - Update: $\mathbf{s}'_{t+1} = \mathbf{s}'*t - \eta \nabla*{\mathbf{s}'} \mathcal{L}$
    - Project onto feasible set (enforce constraints)
    - Check convergence: if $Q(\mathbf{s}', a_{\text{target}}) > Q(\mathbf{s}', a^*) + \epsilon$, stop
3. Return: $\mathbf{s}'_{\text{CF}} = \mathbf{s}'_T$

---

##### Constraint Satisfaction for Realistic Counterfactuals

Not all perturbations produce realistic traffic states. We must enforce **domain constraints**:

**1. Non-Negativity**: Queue lengths, waiting times, phase durations cannot be negative:

$$ s'\_i \geq 0 \quad \forall i \in {\text{queues, times, durations}} $$

**2. Bounded Values**: Features have physical limits:

- Queue lengths: $s'*{\text{queue}} \in [0, Q*{\max}]$ (capacity constraint)
- Phase duration: $s'*{\text{duration}} \in [T*{\min}, T_{\max}]$
- Phase encoding: Must be valid one-hot vector

**3. Discrete Constraints**: Some features take discrete values:

- Bus present: $s'_{\text{bus}} \in {0, 1}$
- Phase ID: $s'_{\text{phase}} \in {1, 2, 3, 4}$

**4. Consistency**: Related features must remain consistent:

- If queue length = 0, waiting time must = 0
- If phase just started (duration = 2s), certain actions may be invalid

**Projection Operator**: After each gradient step, project $\mathbf{s}'$ onto the feasible set
$\mathcal{S}_{\text{feasible}}$:

$$
\mathbf{s}' \leftarrow \text{Project}(\mathbf{s}')
$$

**Projection Methods**:

**Clipping**:

$$
s'\_i \leftarrow \begin{cases} 0 & \text{if } s'_i < 0 \ Q_{\max} & \text{if } s'_i > Q_{\max} \ s'\_i
& \text{otherwise} \end{cases}
$$

**Rounding** (for discrete features):

$$
s'_{\text{bus}} \leftarrow \text{round}(s'_{\text{bus}}) \in {0, 1}
$$

**One-Hot Correction** (for categorical features):

$$
\mathbf{s}'_{\text{phase}} \leftarrow \text{one\_hot}(\arg\max_k
s'_{\text{phase},k})
$$

**Worked Example**:

Original state: $\mathbf{s} = [12, 3, 25, 1]$ (NS queue, EW queue, duration, bus)

After gradient step: $\mathbf{s}' = [7.3, 3.1, 24.8, 0.7]$

Projection:

- NS queue: $7.3 \to 7.3$ (valid, keep)
- EW queue: $3.1 \to 3.1$ (valid)
- Duration: $24.8 \to 24.8$ (valid)
- Bus: $0.7 \to 1$ (round to nearest discrete value)

Final: $\mathbf{s}'_{\text{proj}} = [7.3, 3.1, 24.8, 1]$

---

##### Decision Boundary Identification

Counterfactuals map decision boundaries in state space. By generating multiple counterfactuals for different target
actions, we construct a local **decision diagram**:

**Example**: For state $\mathbf{s}$ with action Continue selected:

**Counterfactual 1** (target: Next Phase):

- Original: NS queue = 14, duration = 28s
- Counterfactual: NS queue = 8, duration = 28s
- **Boundary**: At NS queue ≈ 8, agent switches from Continue to Next Phase

**Counterfactual 2** (target: Skip-to-P1):

- Original: Bus waiting = 0s
- Counterfactual: Bus waiting = 18s
- **Boundary**: At bus wait ≥ 18s, agent activates Skip-to-P1

**Decision Map**:

```
NS Queue (vehicles)
   ↑
   |
20 |        ┌─────────────┐
   |        │  Continue   │
15 |        │             │
   |   Bus  └─────────────┘
14 |  Priority    │
   |   activated  │  ← Boundary
10 | ←─────┐      │
   |       │      ↓
 8 | ─────────────┼──────────────
   |       │   Next Phase
 5 |       │
   |       │
 0 └───────┴──────────────────→ Duration (s)
           18s              40s
```

**Interpretation**:

- **Horizontal boundary** at queue ≈ 8: Below this, insufficient demand for continuing
- **Vertical boundary** at bus wait ≈ 18s: Above this, bus priority overrides queue considerations
- **Region "Continue"**: High queues + reasonable duration
- **Region "Next Phase"**: Low queues (already served) + moderate duration
- **Region "Skip-to-P1"**: Bus present + excessive bus wait time

This visualization reveals the agent's implicit decision rules—the thresholds and trade-offs learned through training.

---

##### Minimal Perturbation Analysis

The **magnitude** of perturbation required for counterfactuals reveals decision **sensitivity**:

**Small Perturbation Required**: Decision is sensitive; agent is near a boundary.

**Example**: Original Continue, counterfactual Next Phase requires changing queue from 14 → 13 vehicles.

**Interpretation**: Agent is on the edge of switching. Decision could easily flip with minor traffic changes. This is a
**sensitive state**.

**Large Perturbation Required**: Decision is robust; agent is deep in a decision region.

**Example**: Original Continue, counterfactual Next Phase requires changing queue from 24 → 6 vehicles.

**Interpretation**: Agent is confident in Continue. Would need major traffic changes to switch. This is a **stable
state**.

**Sensitivity Map**:

| State             | Current Action | Minimum Δ for Next Phase | Sensitivity |
| ----------------- | -------------- | ------------------------ | ----------- |
| High queue, early | Continue       | Δqueue = -1              | Very High   |
| High queue, late  | Continue       | Δqueue = -10             | Low         |
| Low queue, early  | Next Phase     | Δqueue = +5              | Moderate    |
| Bus present       | Skip-to-P1     | Δbus = N/A               | Stable      |

**Application**: Sensitive states require careful monitoring; stable states allow operator confidence.

```mermaid
flowchart TD
    A["Original State s<br>Agent selects a*"] --> B["Define Target Action<br>a_target ≠ a*"]

    B --> C["Initialize s' = s"]

    C --> D["Compute Loss<br>L = ||s'-s||² + λ·max(0, Q(s',a*)-Q(s',a_t)+ε)"]

    D --> E["Compute Gradient<br>∇_{s'} L"]

    E --> F["Update Counterfactual<br>s' ← s' - η·∇_{s'} L"]

    F --> G["Project to Feasible Set<br>Enforce constraints:<br>non-negative, bounded, discrete"]

    G --> H{"Decision<br>Flipped?"}

    H -->|<span style='background-color:khaki; color:black; padding:2px 6px; border-radius:3px'>No</span>| I{"Max iterations<br>reached?"}
    I -->|<span style='background-color:khaki; color:black; padding:2px 6px; border-radius:3px'>No</span>| D
    I -->|<span style='background-color:khaki; color:black; padding:2px 6px; border-radius:3px'>Yes</span>| J["No Counterfactual Found<br>Target action unreachable"]

    H -->|<span style='background-color:khaki; color:black; padding:2px 6px; border-radius:3px'>Yes</span>| K["Counterfactual Found<br>s'_CF with<br>argmax Q(s'_CF,a) = a_target"]

    K --> L["Analyze Perturbation<br>Δs = s'_CF - s"]

    L --> M["Extract Decision Boundary<br>Which features changed?<br>By how much?"]

    M --> N["Generate Explanation<br>'If feature_i were X instead of Y,<br>agent would choose a_target'"]

    style A fill:#E3F2FD
    style B fill:#BBDEFB
    style C fill:#90CAF9
    style D fill:#64B5F6
    style E fill:#42A5F5
    style F fill:#2196F3
    style G fill:#1E88E5
    style H fill:#1976D2
    style I fill:#1565C0
    style J fill:#FCE4EC
    style K fill:#0D47A1
    style L fill:#01579B
    style M fill:#0277BD
    style N fill:#039BE5
```

---

#### 5. Policy Distillation and Tree Extraction

##### From Neural Networks to Decision Trees

While neural networks excel at learning complex policies, their opacity limits deployment in safety-critical domains.
**Policy distillation** addresses this by training an interpretable model (decision tree) to mimic the neural network's
behavior. The result: a human-readable set of rules that approximates the complex learned policy.

**Core Insight**: We don't need the **perfect** policy—we need a **good enough, interpretable** policy. If a decision
tree can capture 85-95% of the neural network's performance while being fully transparent, this trade-off is often
acceptable for real-world deployment.

**Decision Trees as Universal Approximators**: While neural networks are universal function approximators in theory,
decision trees can approximate **any finite-valued function** given sufficient depth. The challenge is finding a compact
tree that generalizes well.

**Mathematical Formulation**: Given neural policy $\pi_{\text{NN}}(s) = \arg\max_a Q(s,a;\theta)$, learn decision tree
$\pi_{\text{tree}}$ that minimizes disagreement:

$$
\pi_{\text{tree}}^\* = \arg\min_{\pi' \in \Pi\_{\text{trees}}} \mathbb{E}_{s \sim \rho}[\mathbb{1}[\pi'(s) \neq
\pi_{\text{NN}}(s)]]
$$

where $\rho$ is a state distribution (ideally, the distribution the agent encounters during operation).

**Why Trees?**: Among interpretable models, decision trees offer unique advantages:

- **Sequential Logic**: Trees naturally represent IF-THEN reasoning familiar to engineers
- **Feature Selection**: Trees automatically identify most important features through splits
- **Non-Linearity**: Can capture non-linear relationships (unlike linear models)
- **Visualization**: Can be drawn as flowcharts for human inspection
- **Verification**: Formal verification tools exist for tree-based models

---

##### VIPER Algorithm: Iterative Policy Approximation

VIPER (Verifiable In-Policy Experience Replay) provides a principled approach to policy distillation. The key
innovation: iteratively improve the tree by querying states generated by the tree itself, not just the neural network.

**Motivation**: If we train a tree only on states from the neural network's trajectory, the tree may fail on states it
visits when deployed (since its policy differs). VIPER addresses this by using **DAGGER** (Dataset Aggregation): collect
data under the current tree policy, label with neural network actions, retrain.

**VIPER Algorithm**:

**Input**: Trained DQN $Q(s,a;\theta)$, environment, tree depth limit $d_{\max}$

**Output**: Decision tree $\pi_{\text{tree}}$ approximating $\pi_{\text{NN}}$

**Initialization**:

1. Collect initial dataset $\mathcal{D}_0$ by rolling out $\pi_{\text{NN}}$ in environment
2. Label each state $s \in \mathcal{D}_0$ with $\pi_{\text{NN}}(s) = \arg\max_a Q(s,a;\theta)$
3. Train initial tree $\pi_{\text{tree}}^{(0)}$ on $\mathcal{D}_0$

**Iteration** (for $k = 1, 2, \ldots, K$):

1. **Roll-out Tree Policy**: Execute $\pi_{\text{tree}}^{(k-1)}$ in environment to collect states $\mathcal{S}_k$
2. **Query Oracle**: For each $s \in \mathcal{S}_k$, label with $a = \pi_{\text{NN}}(s)$
3. **Aggregate Dataset**: $\mathcal{D}_k = \mathcal{D}_{k-1} \cup {(s, \pi_{\text{NN}}(s)) : s \in \mathcal{S}_k}$
4. **Train Tree**: Fit $\pi_{\text{tree}}^{(k)}$ to $\mathcal{D}_k$ using CART (Classification and Regression Trees)
5. **Evaluate Fidelity**:
    $$
    \text{Fidelity}_k = \mathbb{E}_{s \sim \rho}[\mathbb{1}[\pi_{\text{tree}}^{(k)}(s) =
        \pi_{\text{NN}}(s)]]
    $$
6. **Stopping Criterion**: If fidelity > threshold or $k = K_{\max}$, stop

**Return**: Best tree $\pi_{\text{tree}}^{(k^*)}$

**Why Aggregation Helps**: Without aggregation, the tree trained on neural network states may generalize poorly:

**Iteration 0**: Tree trained on NN states → Tree visits slightly different states → Errors accumulate

**With Aggregation**: Tree trained on own states → Corrects errors on states tree actually visits → Better
generalization

**Visual Representation of VIPER**:

```mermaid
flowchart TD
    A["Initialize:<br>Collect states from π_NN<br>Train initial tree π_tree⁽⁰⁾"] --> B["Iteration k"]

    B --> C["Roll out π_tree⁽ᵏ⁻¹⁾<br>in environment<br>Collect states S_k"]

    C --> D["Query Oracle π_NN<br>Label each s∈S_k<br>with a = π_NN(s)"]

    D --> E["Aggregate Dataset<br>D_k = D_{k-1} ∪ {(s,a) : s∈S_k}"]

    E --> F["Train New Tree π_tree⁽ᵏ⁾<br>on aggregated dataset D_k"]

    F --> G["Evaluate Fidelity<br>Fraction of states where<br>π_tree = π_NN"]

    G --> H{"Fidelity<br>threshold<br>reached?"}

    H -->|<span style='background-color:khaki; color:black; padding:2px 6px; border-radius:3px'>No</span>| I{"Max iterations?"}
    I -->|<span style='background-color:khaki; color:black; padding:2px 6px; border-radius:3px'>No</span>| B
    I -->|<span style='background-color:khaki; color:black; padding:2px 6px; border-radius:3px'>Yes</span>| J["Return Best Tree"]

    H -->|<span style='background-color:khaki; color:black; padding:2px 6px; border-radius:3px'>Yes</span>| K["Tree Successfully<br>Approximates Policy"]

    style A fill:#E3F2FD
    style B fill:#BBDEFB
    style C fill:#90CAF9
    style D fill:#64B5F6
    style E fill:#42A5F5
    style F fill:#2196F3
    style G fill:#1E88E5
    style H fill:#1976D2
    style I fill:#1565C0
    style J fill:#FCE4EC
    style K fill:#0D47A1
```

---

##### Fidelity vs. Simplicity Trade-offs

Decision trees have a fundamental trade-off: **deeper trees** → higher fidelity but lower interpretability; **shallow
trees** → lower fidelity but higher interpretability.

**Tree Depth and Complexity**:

| Depth | Max Leaves | Max Rules | Interpretability | Typical Fidelity |
| ----- | ---------- | --------- | ---------------- | ---------------- |
| 2     | 4          | 4         | ★★★★★            | 60-70%           |
| 4     | 16         | 16        | ★★★★☆            | 75-85%           |
| 6     | 64         | 64        | ★★★☆☆            | 85-92%           |
| 8     | 256        | 256       | ★★☆☆☆            | 90-95%           |
| 10+   | 1024+      | 1024+     | ★☆☆☆☆            | 95%+             |

**Interpretability Assessment**: A tree is "interpretable" if a human can trace through its logic for a specific input
within reasonable time:

- **Depth 2**: 4 rules, easily memorized
- **Depth 4**: 16 rules, still traceable on paper
- **Depth 6**: 64 rules, requires computer assistance but rules are individually understandable
- **Depth 8+**: Too complex for manual analysis, defeats the purpose of distillation

**Fidelity Measurement**: Percentage of states where tree matches neural network:

$$
\text{Fidelity} = \frac{1}{|\mathcal{T}|}\sum_{s \in \mathcal{T}} \mathbb{1}[\pi_{\text{tree}}(s) =
\pi_{\text{NN}}(s)]
$$

where $\mathcal{T}$ is a test set of states.

**Performance vs. Fidelity**: High fidelity doesn't guarantee high performance. The tree might copy the neural network's
mistakes. More important: tree's actual reward when deployed:

$$
\text{Performance} = \mathbb{E}_{\text{episodes}}[\text{Cumulative Reward using } \pi_{\text{tree}}]
$$

**Acceptable Trade-Off**: In practice, fidelity of 85-92% with depth 4-6 often provides the best balance. This captures
most of the neural network's knowledge while remaining interpretable.

**Example Trade-Off Analysis**:

| Tree Depth | Fidelity | Avg Reward | Interpretability | Choice           |
| ---------- | -------- | ---------- | ---------------- | ---------------- |
| 3          | 72%      | -245       | High             | Too low fidelity |
| 5          | 88%      | -198       | Medium           | ✓ Good balance   |
| 7          | 93%      | -185       | Low              | Marginal gain    |
| 10         | 96%      | -178       | Very Low         | Not worth it     |
| NN         | 100%     | -175       | None             | Baseline         |

Tree with depth 5 achieves 88% fidelity, 13% worse reward than NN, but is interpretable. Depth 10 improves fidelity to
96% but loses interpretability with minimal reward gain. **Depth 5 is optimal.**

---

##### Rule Extraction and Interpretation

Once trained, the decision tree provides explicit rules. Each path from root to leaf is a rule:

**Example Tree Structure**:

```
Root
├─ IF NS_queue > 12
│  ├─ IF phase_duration > 30
│  │  └─ Action: Next Phase
│  └─ IF phase_duration ≤ 30
│     └─ Action: Continue
└─ IF NS_queue ≤ 12
   ├─ IF bus_present = 1
   │  └─ Action: Skip-to-P1
   └─ IF bus_present = 0
      └─ Action: Continue
```

**Extracted Rules**:

**Rule 1**: IF (NS_queue > 12) AND (phase_duration > 30) THEN Next Phase

**Interpretation**: When the major approach has heavy queue and phase has run long, advance to next phase to serve other
approaches.

**Rule 2**: IF (NS_queue > 12) AND (phase_duration ≤ 30) THEN Continue

**Interpretation**: Heavy queue but phase just started—continue serving to clear backlog.

**Rule 3**: IF (NS_queue ≤ 12) AND (bus_present = 1) THEN Skip-to-P1

**Interpretation**: Light traffic, bus detected—activate bus priority by skipping to major through phase.

**Rule 4**: IF (NS_queue ≤ 12) AND (bus_present = 0) THEN Continue

**Interpretation**: Light traffic, no special conditions—maintain current phase.

**Validation with Domain Expertise**:

Traffic engineers can review these rules for reasonableness:

- **Rule 1**: Reasonable. Threshold of 12 vehicles aligns with typical approach capacity (600-900 veh/hr ≈ 10-15
  veh/cycle).
- **Rule 2**: Correct. Minimum green time (often 30s) must be respected before switching.
- **Rule 3**: Good bus priority logic. Activates only when it won't cause excessive delays (queue ≤ 12).
- **Rule 4**: Default behavior—safe continuation when no special conditions apply.

Engineers might suggest refinements: "Rule 3 should also check pedestrian demand to avoid conflicts." This feedback
improves the reward function for retraining.

---

##### Validation and Verification

Extracted trees enable formal verification—proving safety properties that neural networks cannot guarantee.

**Property 1: Minimum Green Time**: Prove that tree never selects "Next Phase" when phase*duration < $T*{\min}$.

**Verification**: Traverse all tree paths. Check each leaf with action "Next Phase":

- Path 1: NS*queue > 12 AND phase_duration > 30 → Next Phase (duration > 30 ≥ $T*{\min}$ = 10, ✓)
- Path 2: ... (check all paths)

If all paths satisfy the property, the tree is **verified** to respect minimum green time. This provides formal safety
guarantees impossible with neural networks.

**Property 2: No Conflicting Movements**: Prove tree never selects phases that allow conflicting traffic simultaneously.

**Property 3: Maximum Delay Bound**: Prove that under tree policy, no approach waits longer than $W_{\max}$ seconds
(probabilistic property, harder to verify).

**Model Checking Tools**: Formal verification tools (UPPAAL, PRISM) can automatically check temporal logic properties on
tree policies:

$$
\square \left(\text{phase*duration} < T_{\min} \implies \neg \text{action = Next}\right)
$$

"Always, if duration below minimum, action is not Next Phase."

This level of verification is critical for safety certification in autonomous systems.

```mermaid
flowchart TD
    A["Trained DQN Policy<br>π_NN(s) = argmax Q(s,a;θ)"] --> B["Policy Distillation<br>via VIPER"]

    B --> C["Decision Tree<br>π_tree(s)"]

    C --> D["Evaluation"]

    D --> D1["Fidelity Test<br>π_tree vs π_NN<br>on test states"]
    D --> D2["Performance Test<br>Actual reward<br>in environment"]
    D --> D3["Interpretability<br>Analysis<br>Tree depth, complexity"]

    D1 --> E{"Acceptable<br>Trade-Off?"}
    D2 --> E
    D3 --> E

    E -->|<span style='background-color:khaki; color:black; padding:2px 6px; border-radius:3px'>No</span>| F["Refine:<br>Adjust tree depth<br>More VIPER iterations"]
    F --> B

    E -->|<span style='background-color:khaki; color:black; padding:2px 6px; border-radius:3px'>Yes</span>| G["Extract Rules<br>from Tree"]

    G --> H["Human Review<br>Traffic engineers<br>validate logic"]

    H --> I["Formal Verification<br>Prove safety properties<br>(min green, max delay)"]

    I --> J["Deployment-Ready<br>Interpretable Policy<br>with Safety Guarantees"]

    style A fill:#E3F2FD
    style B fill:#BBDEFB
    style C fill:#90CAF9
    style D fill:#64B5F6
    style D1 fill:#42A5F5
    style D2 fill:#2196F3
    style D3 fill:#1E88E5
    style E fill:#1976D2
    style F fill:#FCE4EC
    style G fill:#1565C0
    style H fill:#0D47A1
    style I fill:#01579B
    style J fill:#0277BD
```

---

#### 6. Behavioral Safety Analysis

##### Beyond Average Reward: Safety Metrics

Traditional reinforcement learning optimizes **average cumulative reward**. But safety-critical systems require
guarantees about **worst-case behavior**, not just average performance:

**Problem with Average Reward**: An agent with average waiting time 20s might seem excellent, but if 5% of pedestrians
wait 120s (unsafe jaywalking risk), the system is unacceptable.

**Safety-Oriented Metrics**:

**1. Maximum Waiting Time Per Mode**:

$$
W_{\max}^{(\text{mode})} = \max_{t, \text{vehicle } v} {\text{waiting time of vehicle } v \text{ of type mode at time
} t}
$$

Separate maximums for cars, bicycles, pedestrians, buses. Thresholds:

- Cars: $W_{\max} < 180$ s (3 minutes tolerable)
- Bicycles: $W_{\max} < 120$ s (2 minutes)
- Pedestrians: $W_{\max} < 90$ s (1.5 minutes, beyond which jaywalking increases)
- Buses: $W_{\max} < 60$ s (priority requirement)

**2. Percentile Waiting Times**:

$$
W\_{95}^{(\text{mode})} = \text{95th percentile of waiting times for mode}
$$

Characterizes typical worst-case, filtering out rare extreme outliers.

**3. Modal Equity**:

$$
\text{Equity Ratio} = \frac{\max_{\text{mode}} \mathbb{E}[W^{(\text{mode})}]}{\min_{\text{mode}}
\mathbb{E}[W^{(\text{mode})}]}
$$

Measures fairness. If cars average 10s while pedestrians average 50s, equity ratio = 5.0 (high inequity). Target: ratio
< 2.0.

**4. Action Blocking Rate**:

$$
\text{Block Rate} = \frac{\text{ actions blocked by safety constraints}}{\text{ total action attempts}} \times 100%
$$

High blocking (>20%) indicates agent hasn't internalized operational constraints. Low blocking (<5%) suggests
well-learned policy.

**5. Constraint Violation Frequency**:

- **Minimum Green Violations**: # times agent tried to switch before $T_{\min}$ elapsed
- **Maximum Green Violations**: # times phase exceeded $T_{\max}$
- **Conflicting Movement Violations**: # times agent attempted illegal phase transitions

Target: Zero violations for critical safety constraints.

---

##### Scenario-Based Testing Protocols

Rather than relying solely on aggregate statistics, systematic testing across **diverse scenarios** characterizes
behavioral boundaries:

**Scenario Design Principles**:

**1. Demand Variation**: Systematically vary traffic volumes for each mode

- Low (200 veh/hr), Medium (400), High (600), Peak (800), Extreme (1000)
- Create combinations: High car + Low pedestrian, Balanced, Peak all modes

**2. Modal Priority Conflicts**: Design scenarios with competing demands

- Heavy vehicles + High pedestrian demand
- Bus arrival + Bicycle platoon
- Emergency vehicle + Congestion

**3. Time-of-Day Patterns**: Morning peak, evening peak, midday, overnight

**4. Special Events**: School dismissal (pedestrian surge), sporting event (traffic spike), construction (reduced
capacity)

**Example Test Suite** (30 scenarios):

| Scenario Set | Description                         | # Scenarios | Purpose                     |
| ------------ | ----------------------------------- | ----------- | --------------------------- |
| Pr_0 to Pr_9 | Private car demand: 100-1000 veh/hr | 10          | Test vehicle prioritization |
| Bi_0 to Bi_9 | Bicycle demand: 100-1000 veh/hr     | 10          | Test bicycle accommodation  |
| Pe_0 to Pe_9 | Pedestrian demand: 100-1000 ped/hr  | 10          | Test pedestrian safety      |

Each scenario runs 3600s (1 hour) simulation with fixed random seed for reproducibility.

**Logging Protocol**:

For each timestep $t$:

- **State**: Complete 32-dimensional state vector
- **Q-Values**: $Q(s,a)$ for all actions
- **Selected Action**: $a^* = \arg\max_a Q(s,a)$
- **Blocked**: Boolean indicating if action was rejected by constraints
- **Reward Components**: Breakdown of waiting time, sync bonus, penalties
- **Traffic Outcomes**: Queue lengths, waiting times after action execution

Post-process logs to compute safety metrics, identify problematic patterns.

---

##### Maximum Waiting Time Analysis

Maximum waiting time is the most critical safety metric. We analyze:

**Per-Scenario Analysis**:

| Scenario        | $W_{\max}^{\text{car}}$ | $W_{\max}^{\text{bike}}$ | $W_{\max}^{\text{ped}}$ | $W_{\max}^{\text{bus}}$ |
| --------------- | ----------------------- | ------------------------ | ----------------------- | ----------------------- |
| Pr_0 (light)    | 45s                     | 38s                      | 42s                     | 28s                     |
| Pr_5 (moderate) | 78s                     | 62s                      | 71s                     | 35s                     |
| Pr_9 (extreme)  | 142s                    | 98s                      | 85s                     | 52s                     |
| Pe_7 (high ped) | 68s                     | 55s                      | **103s**                | 41s                     |
| Pe_9 (peak ped) | 72s                     | 58s                      | **118s**                | 38s                     |

**Findings**:

- **Pe_7, Pe_9**: Pedestrian max wait exceeds 90s threshold → Safety concern
- **Pr_9**: Car max wait 142s approaches but doesn't exceed 180s threshold → Acceptable
- **All scenarios**: Bus max wait well below 60s → Bus priority working

**Root Cause Analysis for Pe_7, Pe_9**: Query explanation methods:

**Saliency Analysis**: In high ped scenarios, pedestrian waiting time has only 12% saliency (low priority)

**Counterfactual**: "If pedestrian queue were 15 instead of 8, agent would activate pedestrian phase" → Threshold too
high

**Recommendation**: Modify reward function to increase pedestrian priority weight, retrain.

---

##### Action Blocking Patterns

Actions get blocked when they violate operational constraints. Analyzing blocking patterns reveals whether the agent
learned valid control strategies:

**Blocking Statistics**:

| Scenario | Total Actions | Blocked | Block Rate | Most Blocked Action |
| -------- | ------------- | ------- | ---------- | ------------------- |
| Pr_0     | 3600          | 42      | 1.2%       | Skip-to-P1          |
| Pr_5     | 3600          | 128     | 3.6%       | Next Phase          |
| Pr_9     | 3600          | 89      | 2.5%       | Skip-to-P1          |
| Bi_7     | 3600          | 156     | 4.3%       | Next Phase          |
| Pe_9     | 3600          | 203     | 5.6%       | Skip-to-P1          |

**Interpretation**:

- **Overall low blocking** (1-6%): Agent learned constraints reasonably well
- **Pe_9 highest** (5.6%): Heavy pedestrian demand causes more constraint violations, possibly from aggressive attempts
  to serve vehicles
- **"Next Phase" and "Skip-to-P1" most blocked**: These phase changes have strict timing requirements (min green, phase
  sequence)

**Temporal Analysis**: When do blockings occur?

```
Blockings by Phase Duration (Pe_9 scenario)
Duration:  [0-10s]  [10-20s]  [20-30s]  [30-40s]  [40-50s]  [50+s]
Blocked:     78       95        22        6         2         0
```

**Pattern**: 85% of blockings occur in first 20s of phase → Agent attempting premature phase changes before minimum
green satisfied.

**Mitigation**:

1. Stronger penalty for early phase change attempts in reward function
2. Explicit min-green masking in action selection (prevent illegal actions a priori)

---

##### Modal Service Quality Assessment

Beyond maximum waiting time, analyze **service quality distribution**:

**Waiting Time Distributions (Pr_5 scenario)**:

| Mode  | Mean  | Median | 75th % | 90th % | 95th % | Max |
| ----- | ----- | ------ | ------ | ------ | ------ | --- |
| Cars  | 18.2s | 15.0s  | 24.0s  | 35.0s  | 48.0s  | 78s |
| Bikes | 21.5s | 18.0s  | 28.0s  | 42.0s  | 58.0s  | 62s |
| Peds  | 24.8s | 20.0s  | 32.0s  | 48.0s  | 65.0s  | 71s |
| Buses | 12.3s | 10.0s  | 16.0s  | 22.0s  | 28.0s  | 35s |

**Analysis**:

**1. Modal Ordering**: Bus < Cars < Bikes < Pedestrians → Bus priority effective, pedestrians receive longest waits

**2. Equity Ratio**: $\frac{24.8}{12.3} = 2.02$ (marginally acceptable, target < 2.0)

**3. Tail Behavior**: Pedestrian 95th percentile (65s) close to safety threshold (90s) → Limited safety margin

**4. Bus Performance**: Mean 12.3s well below target (< 20s) → Over-prioritization? Could slightly relax bus priority to
improve pedestrian service.

**Comparison Across Scenarios**:

```
Mean Waiting Time vs. Traffic Volume
Cars:    ─────▲───────────▲────────────▲─────▲
Bikes:   ────────▲───────────▲────────────▲──
Peds:    ───────────▲───────────▲─────────────▲

Volume:  Light  Moderate  High  Extreme
         (200)   (400)    (600)  (800) veh/hr
```

All modes show expected increase with volume. Pedestrians increase most steeply → Agent prioritizes vehicles under
congestion.

**Fairness Intervention**: Adjust reward to -

$$
r = -\alpha_{\text{car}} W_{\text{car}} - \alpha_{\text{bike}} W_{\text{bike}} - \alpha_{\text{ped}} W_{\text{ped}} - \alpha_{\text{bus}} W_{\text{bus}}
$$

Increase $\alpha_{\text{ped}}$ from 1.0 to 1.5 to boost pedestrian priority, retrain, and re-evaluate.

```mermaid
flowchart TD
    A["Trained DQN Agent"] --> B["Deploy Across<br>30 Test Scenarios<br>(Pr_0-9, Bi_0-9, Pe_0-9)"]

    B --> C["Collect Operational Data<br>• States, actions, Q-values<br>• Waiting times per mode<br>• Blocked actions<br>• Phase transitions"]

    C --> D["Compute Safety Metrics"]

    D --> D1["Maximum Waiting Time<br>W_max per mode<br>Identify threshold violations"]

    D --> D2["Percentile Analysis<br>W_50, W_75, W_95<br>Characterize distributions"]

    D --> D3["Action Blocking<br>Block rate, patterns<br>Constraint learning assessment"]

    D --> D4["Modal Equity<br>Service fairness<br>Equity ratio"]

    D1 --> E["Identify Safety Issues"]
    D2 --> E
    D3 --> E
    D4 --> E

    E --> F["Pe_7, Pe_9:<br>Pedestrian W_max > 90s<br>(Safety threshold exceeded)"]

    F --> G["Root Cause Analysis<br>Why threshold exceeded?"]

    G --> G1["Saliency Analysis:<br>Pedestrian features<br>have low importance"]

    G --> G2["Counterfactual:<br>Pedestrian threshold<br>set too high"]

    G --> G3["Action Logs:<br>Ped phase activated<br>too infrequently"]

    G1 --> H["Diagnosis:<br>Reward underweights<br>pedestrian priority"]
    G2 --> H
    G3 --> H

    H --> I["Corrective Action:<br>Increase α_ped in reward<br>from 1.0 to 1.5<br>Retrain agent"]

    I --> J["Validation:<br>Re-test on Pe_7, Pe_9<br>Verify W_max_ped < 90s"]

    style A fill:#E3F2FD
    style B fill:#BBDEFB
    style C fill:#90CAF9
    style D fill:#64B5F6
    style D1 fill:#42A5F5
    style D2 fill:#2196F3
    style D3 fill:#1E88E5
    style D4 fill:#1976D2
    style E fill:#1565C0
    style F fill:#FCE4EC
    style G fill:#0D47A1
    style G1 fill:#01579B
    style G2 fill:#0277BD
    style G3 fill:#039BE5
    style H fill:#F8BBD9
    style I fill:#F48FB1
    style J fill:#F06292
```

---

##### Synthesis: Multi-Method Explainability Framework

Individual explanation methods provide partial insights. Combining them creates a comprehensive understanding:

**Step 1: Local Decision Explanation** (specific state)

- **Saliency**: Which features are most sensitive?
- **Attention**: Which features does agent focus on?
- **Result**: "In this state, agent prioritizes bus waiting time (63% attention, 2.14 saliency) over vehicle queues."

**Step 2: Decision Boundary Analysis**

- **Counterfactual**: What would need to change to flip the decision?
- **Result**: "If bus waiting decreased from 21s to 10s, agent would Continue instead of Skip-to-P1. Threshold ≈ 18s."

**Step 3: Global Policy Understanding**

- **VIPER Tree**: What general rules does agent follow?
- **Result**: "Agent follows rules: (1) IF queue > 12 AND duration > 30 THEN Next, (2) IF bus present THEN Skip-to-P1,
  etc."

**Step 4: Safety Validation**

- **Behavioral Analysis**: Does agent behave safely across all scenarios?
- **Result**: "Agent respects min green (0 violations) but allows excessive pedestrian waits in Pe_7-9 (max 118s > 90s
  threshold)."

**Step 5: Iterative Refinement**

- **Feedback Loop**: Use explanations to identify issues, modify reward/training, re-evaluate
- **Result**: "Increased pedestrian reward weight → New agent achieves max ped wait 82s < 90s while maintaining vehicle
  performance."

This integrated framework transforms the "black box" into a **glass box**: decisions are transparent, boundaries are
mapped, rules are explicit, and safety is validated. The result is a reinforcement learning system suitable for
real-world deployment in safety-critical traffic control.
