# C-1: Multi-armed Bandits: Exploration vs. Exploitation

1. The Exploration-Exploitation Dilemma
   - Nature of the k-armed Bandit Problem
   - The Value of Exploration
   - Regret and Reward Maximization
   - Formal Problem Definition

2. Action-Value Methods
   - Sample-Average Methods
   - Greedy and ε-greedy Policies
   - The 10-armed Testbed Environment
   - Performance Evaluation Metrics

3. Incremental Implementation
   - Incremental Update Rules
   - Relationship to Stochastic Approximation
   - Computational Efficiency Considerations

4. Tracking Non-stationary Problems
   - Constant Step-Size Parameters
   - Weighted Averages
   - Adapting to Changing Environments

5. Optimistic Initial Values
   - Encouraging Exploration
   - Limitations and Practical Considerations
   - Comparison with ε-greedy Methods

6. Upper Confidence Bound Action Selection
   - UCB Algorithm
   - Theoretical Guarantees
   - Performance Characteristics

7. Gradient Bandit Algorithms
   - Preference-Based Action Selection
   - Stochastic Gradient Ascent
   - Baseline Comparison
   - Theoretical Properties

