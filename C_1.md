# S-1: Foundations and Classical Methods in Reinforcement Learning

# C-1: Foundations of Reinforcement Learning

1. Reinforcement Learning Fundamentals

    - The RL Framework and Core Components
    - Agent-Environment Interaction
    - Real-World Applications of RL
    - Reward Signals and Design Principles

2. Mathematical Foundations

    - Returns and Discounting
    - State and Action Value Functions
    - Policies and Optimal Behavior
    - The Bellman Equations

3. Markov Decision Processes
    - The Markov Property
    - MDP Formalization
    - One-Step Dynamics
    - Episodic vs Continuing Tasks

---

#### Reinforcement Learning Fundamentals

##### The RL Framework and Core Components

Reinforcement learning (RL) represents a distinctive machine learning paradigm fundamentally different from supervised
or unsupervised approaches. While supervised learning trains on labeled examples and unsupervised learning identifies
patterns in unlabeled data, reinforcement learning focuses on how agents should act in an environment to maximize
cumulative rewards.

This learning paradigm draws inspiration from behavioral psychology, particularly the concept of operant conditioning,
where behavior is shaped through rewards and punishments. The central question in reinforcement learning is: "How should
an agent behave in an environment to maximize its cumulative reward?"

<div align="center"> <img src="images/agent.png" width="600" height="auto"> <p style="color: #555;">Figure: Agent-Environment interaction in reinforcement learning</p> </div>

The core components of the reinforcement learning framework include:

**1. Agent**: The decision-making entity that interacts with and learns from the environment. The agent implements a
learning algorithm that enables it to improve its behavior over time based on experience.

**2. Environment**: The external system with which the agent interacts. The environment presents states to the agent,
receives its actions, and provides feedback in the form of rewards. The environment may represent a physical system
(e.g., a robot's surroundings), a simulation (e.g., a game), or an abstract system (e.g., a financial market).

**3. State** $(S_t)$: A representation of the current situation within the environment at time $t$. The state
encapsulates all relevant information needed for decision-making. States can be: - Fully or partially observable -
Discrete (finite set of distinct states) or continuous (infinite, smoothly varying states) - Low-dimensional (few
variables) or high-dimensional (many variables)

**4. Action** $(A_t)$: The set of possible decisions the agent can make. Like states, actions can be discrete (e.g.,
move left/right/up/down) or continuous (e.g., applying specific torques to robot joints). The set of available actions
may depend on the current state.

**5. Reward** $(R_t)$: The feedback signal that indicates the immediate value of the current state-action transition.
The reward is a scalar value that guides the learning process—higher values indicate more desirable outcomes. The reward
function is the formal specification of the agent's goal.

**6. Policy** $(\pi)$: The agent's strategy for selecting actions based on perceived states. A policy maps states to
actions (or probability distributions over actions). It can be deterministic ($\pi(s) = a$) or stochastic
($\pi(a|s) = P(A_t = a | S_t = s)$).

**7. Value Function**: An estimation of the expected cumulative future reward from a given state or state-action pair.
Value functions help agents evaluate the long-term desirability of states and actions beyond immediate rewards.

**8. Model**: A representation of the agent's understanding of the environment dynamics—how states transition and
rewards are generated. Not all RL approaches require a model (model-free methods), while others leverage models for
planning (model-based methods).

The reinforcement learning process follows a cyclical pattern:

- The agent observes the current state $S_t$ of the environment.
- Based on this state, the agent selects an action $A_t$ according to its policy.
- The environment responds by transitioning to a new state $S_{t+1}$ and providing a reward $R_{t+1}$.
- The agent updates its knowledge (policy and/or value functions) based on this experience.
- The process repeats, with the agent continuously refining its behavior to maximize long-term reward.

This framework encompasses a wide range of algorithms and applications, from simple tabular methods suitable for small,
discrete environments to sophisticated deep reinforcement learning approaches capable of mastering complex games and
controlling robotic systems in continuous, high-dimensional spaces.

##### Agent-Environment Interaction

The interaction between the agent and environment constitutes the foundation of the reinforcement learning paradigm.
This interaction is sequential and continuous, forming what we call the agent-environment loop—a fundamental process
that generates the experiences from which learning occurs.

In formal terms, at each discrete time step $t$, the following sequence occurs:

- The agent receives a representation of the environment's state $S_t \in \mathcal{S}$, where $\mathcal{S}$ represents
  the complete set of possible states.
- Based on this state, the agent selects an action $A_t \in \mathcal{A}(S_t)$, where $\mathcal{A}(S_t)$ represents the
  set of actions available in state $S_t$.
- As a consequence of this action, the environment transitions to a new state $S_{t+1}$ according to its dynamics and
  generates a reward $R_{t+1}$.
- The agent receives the new state $S_{t+1}$ and reward $R_{t+1}$, which it uses to update its policy or value
  estimates.

This interaction generates a trajectory or history that can be represented as a sequence:

$$
S_0, A_0, R_1, S_1, A_1, R_2, S_2, A_2, \ldots
$$

A crucial aspect of this interaction is the timing of events. The reward $R_{t+1}$ is the consequence of the action
$A_t$ taken in state $S_t$. It is received alongside the next state $S_{t+1}$. This temporal relationship is essential
for understanding the credit assignment problem in RL—determining which actions in a sequence were responsible for which
rewards.

Several important characteristics of this interaction merit further discussion:

**1. Temporal Nature**: Unlike supervised learning, where training examples can be treated independently, RL involves a
sequence of dependent decisions. Current actions affect not only immediate rewards but also future states and,
consequently, future opportunities for rewards.

**2. Delayed Feedback**: The effects of an action might not be immediately apparent. A chess move might seem neutral at
first but prove decisive many moves later. This delay between action and consequence creates challenges for learning.

**3. Non-stationarity**: As the agent learns and its policy changes, the distribution of states it encounters also
changes. This non-stationarity distinguishes RL from many other machine learning paradigms where training data is
assumed to be independently and identically distributed.

**4. Exploration-Exploitation Dilemma**: The agent must balance two competing objectives:

    - **Exploitation**: Using current knowledge to maximize immediate reward
    - **Exploration**: Gathering more information that might lead to better rewards in the future

    This fundamental tension has no perfect solution and requires careful management strategies.

The agent-environment boundary is conceptual rather than physical. For example, in a robot learning to walk, the motors
and mechanical components might be considered part of the agent, while the external physical world constitutes the
environment. However, the boundary could be drawn differently depending on the problem formulation.

A distinctive feature of reinforcement learning is that the agent must learn from its own experiences—it does not have
access to labeled examples of correct behavior as in supervised learning. Instead, it must explore the environment,
observe the consequences of its actions, and adapt its behavior accordingly. This self-guided learning process makes RL
particularly powerful for problems where optimal behavior is difficult to demonstrate but success can be measured.

##### Real-World Applications of RL

Reinforcement learning has demonstrated remarkable success across diverse domains, showcasing its versatility and
potential for solving complex sequential decision-making problems. Understanding these applications provides insight
into the capabilities and limitations of RL approaches.

**Game Playing**

Games provide well-defined environments with clear rules and objectives, making them ideal testbeds for reinforcement
learning algorithms:

- **Classic Board Games**: RL has achieved superhuman performance in games long considered the pinnacle of human
  strategic thinking:
    - **Chess**: DeepMind's AlphaZero mastered chess through self-play without human knowledge beyond the rules,
      developing novel strategies and defeating world-champion programs.
    - **Go**: AlphaGo and its successors overcame what many considered AI's greatest challenge by defeating world
      champions in a game with more possible positions than atoms in the universe.
    - **Backgammon**: TD-Gammon pioneered the use of temporal difference learning for board games in the early 1990s.
- **Video Games**: RL agents have achieved remarkable results in complex video games:
    - **Atari Games**: Deep Q-Networks (DQN) learned to play multiple Atari games at superhuman levels directly from
      pixel inputs.
    - **StarCraft II**: AlphaStar mastered this complex real-time strategy game, which requires managing economy,
      technology, and combat simultaneously.
    - **Dota 2**: OpenAI Five defeated professional teams in a game requiring teamwork and long-term planning.

These game environments allow researchers to test and improve algorithms in controlled settings before applying them to
more complex real-world problems.

**Robotics and Control**

Reinforcement learning offers a powerful framework for developing robotic systems that can learn complex behaviors
through interaction:

- **Manipulation Tasks**: RL enables robots to learn dexterous manipulation skills such as:
    - Grasping objects of various shapes and textures
    - In-hand manipulation of objects
    - Tool use and assembly tasks
- **Locomotion**: RL has been used to develop walking, running, and jumping behaviors for legged robots that can adapt
  to various terrains and disturbances.
- **Autonomous Vehicles**: RL contributes to decision-making systems for autonomous navigation in complex environments:
    - Lane changing and merging in traffic
    - Navigation through construction zones
    - Adapting to adverse weather conditions

The ability to learn from trial and error makes RL particularly valuable for robotics problems where explicit
programming is impractical due to the complexity of the task or environment.

**Healthcare Applications**

Reinforcement learning offers promising approaches to personalized medicine and clinical decision support:

- **Treatment Optimization**: RL can help optimize treatment strategies for chronic conditions:
    - Determining optimal dosing schedules for patients with diabetes
    - Personalizing treatment plans for cancer patients
    - Managing sedation in intensive care units
- **Clinical Trials**: Adaptive trial designs use RL principles to allocate patients to treatments dynamically based on
  observed outcomes.
- **Medical Imaging**: RL methods improve medical image acquisition and analysis:
    - Optimizing MRI scan parameters to reduce scan time while maintaining quality
    - Automated diagnosis from medical images with efficient region scanning

These applications require careful consideration of safety constraints and interpretability requirements.

**Resource Management**

RL excels at optimizing complex systems with many interacting components:

- **Data Center Management**: Google reduced cooling energy in their data centers by 40% using RL to optimize cooling
  systems based on hundreds of sensors and actuators.
- **Power Grid Management**: RL approaches help integrate renewable energy sources and manage demand response in modern
  power grids.
- **Network Routing**: RL algorithms adaptively route network traffic to minimize congestion and maximize throughput in
  telecommunications networks.
- **Computing Resource Allocation**: RL optimizes the allocation of computational resources in cloud computing
  environments.

**Financial Applications**

The financial sector has adopted RL for various applications:

- **Algorithmic Trading**: RL strategies can adapt to changing market conditions and learn complex patterns in market
  data.
- **Portfolio Management**: RL helps optimize asset allocation across multiple investment options while balancing risk
  and return.
- **Risk Management**: RL models identify and respond to emerging risks in complex financial systems.
- **Market Making**: RL optimizes bid-ask spreads and inventory management for market makers.

**Recommendation Systems**

Modern digital platforms use RL to optimize user experience:

- **Content Recommendation**: Services like Netflix and YouTube use RL to recommend videos that maximize long-term user
  engagement.
- **E-commerce Recommendations**: Online retailers employ RL to suggest products based on browsing history and purchase
  patterns.
- **Advertisement Placement**: RL optimizes ad selection and placement to maximize click-through rates and conversion.

These applications demonstrate that RL is particularly effective in domains with the following characteristics:

- Sequential decision-making problems
- Clear reward signals (or constructible reward functions)
- Environments that are too complex to model explicitly
- Problems where exploration can be safely conducted
- Tasks that benefit from continuous improvement through experience

As algorithms improve and computational resources expand, we can expect reinforcement learning to address increasingly
complex real-world challenges across even more diverse domains.

##### Reward Signals and Design Principles

The reward signal constitutes the fundamental mechanism through which we communicate the objective of a reinforcement
learning problem to an agent. It defines what we want the agent to achieve, not how to achieve it. The design of
appropriate reward functions is perhaps the most crucial aspect of formulating a reinforcement learning problem, as it
directly shapes the behavior the agent will learn.

**Reward Function Formalization**

A reward function maps state-action-next state triplets to a scalar value:

$$
R(s, a, s') = \mathbb{E}[R_{t+1} | S_t=s, A_t=a, S_{t+1}=s']
$$

This value represents the expected immediate reward for transitioning from state $s$ to state $s'$ after taking action
$a$. In many cases, the reward function is simplified to depend only on the state and action $R(s, a)$, or even just the
state $R(s)$.

<div align="center"> <img src="images/reward.png" width="600" height="auto"> <p style="color: #555;">Figure: Reward function decomposition for robot locomotion</p> </div>

**Temporal Characteristics of Rewards**

Unlike supervised learning, where feedback is immediate and direct, rewards in RL may exhibit various temporal
characteristics:

1. **Immediate Rewards**: Feedback provided directly after each action (e.g., points scored in a game).
2. **Delayed Rewards**: Feedback that comes only after a sequence of actions (e.g., winning or losing a chess game).
3. **Sparse Rewards**: Significant rewards that occur infrequently (e.g., reaching a goal state).
4. **Dense Rewards**: Frequent feedback provided throughout the learning process (e.g., distance-based rewards in
   navigation).

The temporal distribution of rewards creates the credit assignment problem: determining which actions in a sequence
contributed to an eventual reward. Methods like temporal difference learning and eligibility traces help address this
challenge.

**Reward Shaping and Decomposition**

Complex behaviors often require carefully structured rewards. Consider this locomotion reward function for a bipedal
robot:

$$
r = \min(v_x, v_{max}) - 0.005(v_y^2 + v_z^2) - 0.05y^2 - 0.02||u||^2 + 0.02
$$

This function elegantly combines multiple objectives:

1. $\min(v_x, v_{max})$: Encourages forward velocity up to a maximum speed
2. $-0.005(v_y^2 + v_z^2)$: Penalizes sideways and vertical motion
3. $-0.05y^2$: Penalizes deviation from the center path
4. $-0.02||u||^2$: Minimizes energy consumption through torque minimization
5. $+0.02$: Provides a small bonus for remaining upright

Each term addresses a specific aspect of the desired behavior, creating a balanced overall objective that, when
optimized, results in efficient, stable walking.

**Reward Design Challenges**

Several common challenges arise in reward function design:

1. **Reward Hacking**: Agents often find unexpected ways to maximize reward without achieving the designer's intent. For
   example, if a cleaning robot is rewarded for not seeing dirt, it might learn to close its cameras or avoid rooms
   altogether.
2. **Misspecification**: The reward function may not accurately capture what we truly value. For instance, rewarding a
   chess agent solely for capturing pieces might lead to reckless play rather than winning the game.
3. **Reward Sparsity**: When significant rewards occur rarely, learning becomes extremely difficult. Navigation tasks
   where reward is given only upon reaching the goal suffer from this problem.
4. **Competing Objectives**: Multiple goals may conflict with each other, creating confusion for the learning agent. A
   self-driving car balancing safety (drive slowly) with efficiency (reach destination quickly) exemplifies this
   challenge.
5. **Reward Scale**: The magnitude of rewards affects learning dynamics; improper scaling can lead to unstable learning
   or misplaced emphasis on certain aspects of behavior.

**Effective Reward Design Principles**

To address these challenges, several principles guide effective reward design:

1. **Alignment with True Objectives**: Ensure the reward genuinely represents what you want the agent to accomplish, not
   a proxy that can be exploited.
2. **Simplicity**: When possible, use simple reward functions that are less prone to misspecification and exploitation.
   The reward function should be the simplest one that correctly captures the task.
3. **Careful Shaping**: If using reward shaping (providing intermediate rewards to guide learning), ensure shaped
   rewards don't introduce behaviors that conflict with the original goal. Potential-based shaping provides theoretical
   guarantees of policy invariance.
4. **Robustness to Exploitation**: Anticipate how agents might exploit the reward function and design safeguards against
   unintended behaviors.
5. **Curriculum Design**: Consider starting with simpler tasks and gradually increasing difficulty, adjusting rewards
   accordingly to facilitate learning of complex behaviors.
6. **Normalization**: Keep reward scales consistent across different aspects of the task to prevent overemphasis on
   certain behaviors due merely to their larger reward magnitude.

**Intrinsic Motivation**

Beyond task-specific rewards, RL agents can benefit from intrinsic motivation—rewards generated by the agent itself:

- **Curiosity**: Rewarding exploration of uncertain states or states where prediction error is high
- **Novelty**: Encouraging visiting rarely-seen states
- **Empowerment**: Rewarding actions that increase the agent's control over future states

These intrinsic rewards can help agents explore effectively in sparse-reward environments, facilitating the discovery of
valuable behaviors that might otherwise be unlikely to be found through random exploration.

Understanding and applying these principles for reward signal design is essential for developing effective reinforcement
learning systems that achieve their intended goals while avoiding unintended consequences. The reward function serves as
the fundamental interface between human designers and learning agents, making its careful design perhaps the most
important aspect of applying reinforcement learning to real-world problems.

#### Mathematical Foundations

##### Returns and Discounting

In reinforcement learning, an agent's objective is to maximize the expected cumulative reward over time, not merely
immediate rewards. This cumulative measure, called the **return**, provides a formal definition of the agent's long-term
goal and forms the mathematical basis for defining value functions and optimal policies.

**Mathematical Definition of Returns**

The return at time $t$, denoted $G_t$, represents the sum of rewards received from that point forward. Depending on the
nature of the task, returns can be formulated in several ways:

**1. Finite-Horizon Undiscounted Return**:

$$
G_t = R_{t+1} + R_{t+2} + R_{t+3} + ... + R_T
$$

Where $T$ is the final time step of the episode.

This formulation is appropriate for **episodic tasks** with well-defined endpoints, such as games with finite duration
or navigation tasks that terminate upon reaching a goal state. The optimization objective becomes:

$$
J(\pi) = \mathbb{E}*{\pi}[G_0] = \mathbb{E}*{\pi}[R_1 + R_2 + ... + R_T]
$$

Where $J(\pi)$ represents the expected return under policy $\pi$.

**2. Infinite-Horizon Discounted Return**:

$$
G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + ... = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}
$$

Where $\gamma \in [0,1]$ is the discount factor.

This formulation is suitable for **continuing tasks** with no natural endpoint or when we want to prioritize near-term
rewards. The optimization objective becomes:

$$
J(\pi) = \mathbb{E}*{\pi}[G_0] = \mathbb{E}*{\pi}\left[\sum_{k=0}^{\infty} \gamma^k R_{k+1}\right]
$$

**3. Average Reward Formulation** (alternative for continuing tasks):

$$
J(\pi) = \lim_{h\rightarrow\infty} \frac{1}{h}\mathbb{E}*{\pi}\left[\sum*{t=1}^{h} R_t\right]
$$

This approach optimizes the long-term average reward per time step, avoiding the need for discounting but introducing
different mathematical complexities.

**The Discount Factor: Theoretical and Practical Significance**

The discount factor $\gamma$ serves multiple crucial purposes in reinforcement learning:

**1. Mathematical Convergence**: For continuing tasks in environments with non-zero rewards, an undiscounted sum could
be infinite, making comparisons between policies meaningless. With $\gamma < 1$, the infinite sum converges to a finite
value (provided rewards are bounded), enabling meaningful policy comparison. For a bounded reward $|R_t| \leq R_{max}$,
the discounted return is bounded by:

$$
|G_t| \leq \sum_{k=0}^{\infty} \gamma^k R_{max} = \frac{R_{max}}{1-\gamma}
$$

**2. Uncertainty Representation**: Future rewards are inherently more uncertain than immediate ones due to:

- Stochasticity in the environment
- Imperfect models of the environment
- Potential changes in the environment

Discounting captures this increasing uncertainty about the future.

**3. Economic Motivation**: In economics, immediate rewards are typically valued more highly than delayed rewards of the
same magnitude—a principle known as temporal discounting. The discount factor can be interpreted as $(1-\gamma)$ being
the probability that the process terminates at each step, or as a continuous discount rate $r$ where $\gamma = e^{-r}$.

**4. Computational Practicality**: Limiting the effective planning horizon makes the learning problem more tractable.
With $\gamma = 0.9$, rewards 10 steps away are discounted by about 35%, and rewards 50 steps away by over 99%.

**Recursive Formulation of Returns**

A key property of the discounted return is its recursive structure:

$$
G_t = R_{t+1} + \gamma G_{t+1}
$$

This recursive relationship forms the foundation for many reinforcement learning algorithms, particularly those based on
dynamic programming and temporal difference learning. It allows value functions to be updated incrementally based on
subsequent estimates, leading to efficient learning algorithms.

**Effects of Different Discount Factors**

The choice of discount factor profoundly influences the agent's behavior:

- **$\gamma = 0$**: The agent is completely myopic, considering only immediate rewards. It would choose actions purely
  for their immediate consequences, ignoring any long-term effects.
- **$\gamma = 0.5$**: Rewards 5 steps in the future are weighted at approximately 3% of their nominal value. The agent
  is still quite short-sighted but considers some future consequences.
- **$\gamma = 0.9$**: Rewards 10 steps in the future retain about 35% of their value. The agent has moderate foresight,
  balancing immediate and future rewards.
- **$\gamma = 0.99$**: Rewards 100 steps in the future still retain about 37% of their value. The agent is quite
  far-sighted, valuing distant rewards significantly.
- **$\gamma = 1$**: The agent values all rewards equally regardless of temporal distance. This is only appropriate for
  episodic tasks with a finite horizon, as it can lead to infinite returns in continuing tasks.

**Mathematical Example: Geometric Series in Returns**

For a continuing task with constant reward $r$ per step, the return with discount factor $\gamma < 1$ is:

$$
G_0 = r + \gamma r + \gamma^2 r + ... = r \sum_{k=0}^{\infty} \gamma^k = \frac{r}{1-\gamma}
$$

This demonstrates how discounting converts an infinite sum to a finite value using the convergence of geometric series.

**Practical Implications for Algorithm Design**

The choice of return formulation directly impacts algorithm design:

1. For episodic tasks with guaranteed termination, $\gamma = 1$ can be used, simplifying the learning process.
2. For continuing tasks or episodic tasks with potentially very long episodes, $\gamma < 1$ is necessary, typically in
   the range [0.9, 0.99].
3. The precision required in estimating value functions increases as $\gamma$ approaches 1, as small errors compound
   over longer effective horizons.
4. Higher discount factors typically require more samples and longer training times to achieve stable learning.

Understanding returns and discounting provides the mathematical foundation for defining what we want RL agents to
optimize, which is essential for developing effective learning algorithms. The choice of return formulation and discount
factor represents a crucial design decision that significantly influences the behavior and learning dynamics of
reinforcement learning agents.

##### State and Action Value Functions

Value functions form the theoretical cornerstone of many reinforcement learning algorithms by providing a principled way
to evaluate states and actions based on their expected long-term returns. These functions translate the immediate reward
signal into estimates of long-term value, enabling agents to make decisions that optimize cumulative future rewards.

**State-Value Function**

The state-value function $V_\pi(s)$ represents the expected return when starting in state $s$ and following policy $\pi$
thereafter:

$$
V_\pi(s) = \mathbb{E}*\pi[G_t | S_t = s] = \mathbb{E}*\pi\left[\sum_{k=0}^{\infty} \gamma^k R_{t+k+1} | S_t = s\right]
$$

This function addresses the fundamental question: "How good is it to be in state $s$ when following policy $\pi$?" It
quantifies the long-term desirability of states, accounting for both immediate rewards and the value of successor states
according to the current policy.

**Action-Value Function**

The action-value function $Q_\pi(s,a)$ represents the expected return when starting in state $s$, taking action $a$, and
thereafter following policy $\pi$:

$$
Q_\pi(s,a) = \mathbb{E}*\pi[G_t | S_t = s, A_t = a] = \mathbb{E}*\pi\left[\sum_{k=0}^{\infty} \gamma^k R_{t+k+1} | S_t = s, A_t = a\right]
$$

This function answers the question: "How good is it to take action $a$ in state $s$ when following policy $\pi$
afterward?" It provides a direct measure of the value of each action in each state, facilitating action selection
without requiring a model of the environment.

<div align="center"> <img src="images/policy.png" width="600" height="auto"> <p style="color: #555;">Figure: Comparison of state-value and action-value representations</p> </div>

**Mathematical Relationship Between Value Functions**

These two value functions are intimately related through the policy $\pi$:

$$
V_\pi(s) = \sum_{a \in \mathcal{A}(s)} \pi(a|s) Q_\pi(s,a)
$$

This equation shows that the value of a state under policy $\pi$ is the expected value of taking actions according to
$\pi$ in that state. It represents the policy-weighted average of the action values.

Conversely, we can express the action-value function in terms of the state-value function:

$$
Q_\pi(s,a) = R(s,a) + \gamma \sum_{s' \in \mathcal{S}} P(s'|s,a) V_\pi(s')
$$

Where $R(s,a)$ is the expected immediate reward and $P(s'|s,a)$ is the transition probability to state $s'$ after taking
action $a$ in state $s$. This equation decomposes the action value into the immediate reward plus the discounted
expected value of the next state.

**Optimal Value Functions**

The optimal value functions, denoted $V_*$ and $Q_*$, represent the maximum possible expected return achievable by any
policy:

$$
\begin{align}
V_*(s) &= \max_\pi V_\pi(s) = \max_a Q_*(s,a) \\
Q_*(s,a) &= \max_\pi Q_\pi(s,a) = \mathbb{E}[R_{t+1} + \gamma \max_{a'} Q_*(S_{t+1},a') | S_t = s, A_t = a]
\end{align}
$$

These optimal functions satisfy the Bellman optimality equations, which capture the principle that an optimal policy
must choose actions that maximize the sum of immediate reward and the discounted value of the next state.

**Theoretical Representations**

For theoretical analysis, value functions can be viewed as vectors in a high-dimensional space:

- $V_\pi$ is a vector in $\mathbb{R}^{|\mathcal{S}|}$ (one dimension per state)
- $Q_\pi$ is a vector in $\mathbb{R}^{|\mathcal{S}| \times |\mathcal{A}|}$ (one dimension per state-action pair)

This vector representation allows us to analyze convergence properties of iterative methods for computing value
functions using linear algebra and contraction mapping principles.

**Practical Representations of Value Functions**

In practice, value functions can be represented in several ways, depending on the size and structure of the state and
action spaces:

1. **Tabular Representation**: For small, discrete state spaces, value functions can be stored as tables with one entry
   per state or state-action pair. This approach provides exact representation but becomes infeasible as the state space
   grows.

2. **Linear Function Approximation**: For larger spaces, value functions can be approximated as a linear combination of
   features:

$$
V_\pi(s) \approx \hat{V}(s,\mathbf{w}) = \sum_{i=1}^{n} w_i \phi_i(s) = \mathbf{w}^T \boldsymbol{\phi}(s)
$$

Where $\boldsymbol{\phi}(s)$ is a feature vector and $\mathbf{w}$ is a weight vector. This approach generalizes across
states and requires fewer parameters than tabular methods.

3. **Non-linear Function Approximation**: For complex or continuous spaces, non-linear approximators like neural
   networks can represent value functions:

$$
V_\pi(s) \approx \hat{V}(s,\boldsymbol{\theta})
$$

Where $\boldsymbol{\theta}$ represents the parameters of the neural network. Deep reinforcement learning leverages this
approach to handle high-dimensional state spaces.

**Comparative Analysis of Value Functions**

| Aspect                 | State-Value Function (V)            | Action-Value Function (Q)               |
| ---------------------- | ----------------------------------- | --------------------------------------- |
| Input                  | State only                          | State-action pair                       |
| Output                 | Expected return from state          | Expected return from state-action       |
| Usage                  | Better for policy evaluation        | Better for action selection             |
| Information            | Requires model for action selection | Can select actions directly             |
| Application            | Often used in model-based methods   | Often used in model-free methods        |
| Storage                | Requires storage for \|S\| values   | Requires storage for \|S\|×\|A\| values |
| Function Approximation | Typically requires fewer parameters | Requires more parameters to approximate |

**Mathematical Properties of Value Functions**

Value functions exhibit several important mathematical properties:

1. **Contraction Mapping**: The Bellman operator is a contraction mapping in the supremum norm, guaranteeing convergence
   of iterative methods under certain conditions:

$$
||T(V) - T(V')|| \leq \gamma ||V - V'||
$$

Where $T$ is the Bellman operator.

2. **Fixed Point Solution**: The true value functions are the fixed points of their respective Bellman operators:

$$
V_\pi = T^\pi V_\pi \quad \text{and} \quad V_* = T^* V_*
$$

3. **Policy Improvement Guarantee**: For any sub-optimal policy $\pi$, the greedy policy $\pi'$ with respect to $V_\pi$
   improves the value function:

$$
V_{\pi'}(s) \geq V_\pi(s) \quad \forall s \in \mathcal{S}
$$

These properties provide the theoretical foundation for algorithms like policy iteration, value iteration, and their
approximate versions.

Value functions transform the complex problem of sequential decision-making into one of function estimation and
maximization. By learning accurate value functions, agents can make decisions that optimize long-term cumulative reward,
even in environments with delayed feedback and complex dynamics. Understanding the mathematical properties and
representations of value functions is therefore essential for developing effective reinforcement learning algorithms.

##### Policies: Strategies for Decision Making

A policy constitutes the core of an agent's decision-making framework—it defines the mapping from states to actions that
determines the agent's behavior. In the context of reinforcement learning, the policy represents the solution to the
sequential decision problem and the ultimate output of the learning process.

**Mathematical Formulation of Policies**

Policies come in two primary forms:

1. **Deterministic Policy**: A deterministic policy $\pi$ maps each state to a single action:

$$
\pi: \mathcal{S} \rightarrow \mathcal{A}
$$

$$
\pi(s) = a
$$

This represents a definitive strategy where the agent always selects the same action in a given state.

2. **Stochastic Policy**: A stochastic policy maps each state to a probability distribution over actions:

$$
\pi: \mathcal{S} \times \mathcal{A} \rightarrow [0,1]
$$

$$
\pi(a|s) = P(A_t = a | S_t = s)
$$

Where $\sum_{a \in \mathcal{A}(s)} \pi(a|s) = 1$ for all states $s$.

Stochastic policies offer several advantages over deterministic ones, including:

- Better exploration during learning
- Robustness to partial observability
- Optimality in certain adversarial environments
- Smoother behavior in continuous action spaces

<div align="center"> <img src="images/optimal_policy.png" width="600" height="auto"> <p style="color: #555;">Figure: Optimal policy and value function in the game of Blackjack</p> </div>

**The Exploration-Exploitation Dilemma**

A fundamental challenge in reinforcement learning is balancing:

- **Exploitation**: Selecting actions that maximize reward based on current knowledge
- **Exploration**: Selecting actions to improve knowledge about the environment

This dilemma is often addressed through specially designed policies:

**1. ε-greedy Policy**: A simple yet effective approach that combines exploitation with random exploration:

$$
\pi(a|s) = \begin{cases} 1 - \varepsilon + \frac{\varepsilon}{|\mathcal{A}(s)|}, & \text{if } a = \arg\max_{a'} Q(s,a') \ \frac{\varepsilon}{|\mathcal{A}(s)|}, & \text{otherwise} \end{cases}
$$

With probability $1-\varepsilon$, the agent selects the action with the highest estimated value (exploitation), and with
probability $\varepsilon$, it selects a random action (exploration).

**2. Softmax Policy**: Actions are selected with probabilities proportional to their estimated values using the
Boltzmann distribution:

$$
\pi(a|s) = \frac{e^{Q(s,a)/\tau}}{\sum_{a' \in \mathcal{A}(s)} e^{Q(s,a')/\tau}}
$$

Where $\tau$ is a "temperature" parameter that controls the randomness of the policy:

- High temperature ($\tau \gg 0$): Actions are selected with nearly equal probability (more exploration)
- Low temperature ($\tau \approx 0$): Higher-valued actions are selected with much higher probability (more
  exploitation)

The softmax policy provides a more nuanced approach to exploration than ε-greedy, as it explores intelligently by
favoring actions with higher estimated values.

**3. Upper Confidence Bound (UCB) Policy**: This approach explicitly balances exploration and exploitation by selecting
actions based on an optimistic estimate of their value:

$$
\pi(s) = \arg\max_a \left( Q(s,a) + c \sqrt{\frac{\ln(t)}{N(s,a)}} \right)
$$

Where $N(s,a)$ is the number of times action $a$ has been selected in state $s$, $t$ is the total number of time steps,
and $c$ is a parameter that controls the degree of exploration. This policy incorporates the principle of "optimism in
the face of uncertainty" by adding an exploration bonus to actions that have been selected less frequently.

**Policy Evaluation and Improvement**

The process of learning in RL often involves two interleaved steps:

**1. Policy Evaluation**: Computing the value function for the current policy:

Given $\pi$, find $V_\pi$ such that:

$$
V_\pi(s) = \sum_{a \in \mathcal{A}(s)} \pi(a|s) \left( R(s,a) + \gamma \sum_{s' \in \mathcal{S}} P(s'|s,a) V_\pi(s') \right)
$$

This system of linear equations can be solved directly for small MDPs or iteratively approximated for larger ones.

**2. Policy Improvement**: Updating the policy to be greedy with respect to the current value function:

$$
\pi'(s) = \arg\max_a \left( R(s,a) + \gamma \sum_{s' \in \mathcal{S}} P(s'|s,a) V_\pi(s') \right)
$$

Or equivalently, using the action-value function:

$$
\pi'(s) = \arg\max_a Q_\pi(s,a)
$$

The Policy Improvement Theorem guarantees that this new policy is at least as good as, and typically better than, the
previous policy:

$$
V_{\pi'}(s) \geq V_\pi(s) \quad \forall s \in \mathcal{S}
$$

This inequality becomes strict for at least one state unless the policy is already optimal.

These two steps form the foundation of policy iteration, a fundamental algorithm in dynamic programming approaches to
reinforcement learning.

**Optimal Policy**

An optimal policy, denoted $\pi_*$, maximizes the expected return from all states:

$$
\pi_* = \arg\max_\pi V_\pi(s), \quad \forall s \in \mathcal{S}
$$

For any MDP, there exists at least one deterministic optimal policy, though multiple optimal policies may exist. These
optimal policies all share the same optimal value functions $V_*$ and $Q_*$.

If we know the optimal action-value function $Q_*$, we can directly derive an optimal deterministic policy:

$$
\pi_*(s) = \arg\max_a Q_*(s,a)
$$

This policy will provably attain the maximum possible expected return in the MDP.

**Policy Parametrization**

In many practical applications, especially those with large or continuous state spaces, policies are parametrized using
function approximators:

$$
\pi_\theta(a|s) = P(a|s,\theta)
$$

Where $\theta$ is a parameter vector that defines the policy. Common parametrizations include:

**1. Linear Parametrization**:

$$
\pi_\theta(a|s) = \frac{e^{\theta^T \phi(s,a)}}{\sum_{a' \in \mathcal{A}(s)} e^{\theta^T \phi(s,a')}}
$$

Where $\phi(s,a)$ is a feature vector for state-action pairs.

**2. Neural Network Parametrization**:

$$
\pi_\theta(a|s) = f_\theta(s,a)
$$

Where $f_\theta$ is a neural network with parameters $\theta$ that outputs action probabilities or deterministic
actions.

**Policy Gradient Methods**

Instead of learning value functions and deriving policies from them, policy gradient methods directly optimize
parameterized policies:

$$
\theta_{t+1} = \theta_t + \alpha \nabla_\theta J(\theta_t)
$$

Where $J(\theta) = \mathbb{E}*{\pi*\theta}[G_0]$ is the expected return and $\nabla_\theta J(\theta)$ is the gradient of
the expected return with respect to the policy parameters.

The policy gradient theorem provides a fundamental result for calculating this gradient:

$$
\nabla_\theta J(\theta) = \mathbb{E}*{\pi*\theta} \left[ \nabla_\theta \ln \pi_\theta(A_t|S_t) G_t \right]
$$

This approach offers several advantages:

- It works naturally with continuous action spaces
- It can learn stochastic policies
- It can converge to a locally optimal policy with fewer samples than value-based methods in some problems

**Constrained Policies**

In practical applications, we often need to add constraints to policies for safety, resource limits, or other
considerations. Constrained RL formulates this as:

$$
\max_\pi \mathbb{E}*\pi[G_0] \quad \text{subject to} \quad \mathbb{E}*\pi[C_i] \leq d_i, \quad i = 1,2,...,m
$$

Where $C_i$ are constraint functions and $d_i$ are threshold values. Methods like Constrained Policy Optimization (CPO)
and Lagrangian approaches solve these constrained optimization problems.

**Convergence and Optimality Guarantees**

Under appropriate conditions, policy-based methods offer various theoretical guarantees:

1. **Policy Iteration**: Converges to an optimal policy in finite iterations for finite MDPs
2. **Policy Gradient**: Converges to a locally optimal policy under standard stochastic approximation conditions
3. **Natural Policy Gradient**: Follows the steepest ascent direction in the policy space under the Fisher information
   metric, potentially leading to more efficient learning

Policies form the core of an agent's decision-making process in reinforcement learning. Understanding their properties,
how to represent them, and how to optimize them is essential for developing effective RL solutions across a wide range
of applications, from robotics and game playing to resource management and personalized recommendations.

##### The Bellman Equations: Recursive Relationships

The Bellman equations, named after mathematician Richard Bellman, constitute the foundational recursive relationships
that characterize value functions in reinforcement learning. These equations express the relationship between the value
of a state (or state-action pair) and the values of successor states, capturing the essential temporal structure of
sequential decision processes.

**Bellman Expectation Equations**

For a given policy $\pi$, the Bellman expectation equations describe how the value functions for that policy relate to
themselves recursively:

**1. For State Values**:

$$
V_\pi(s) = \sum_{a \in \mathcal{A}(s)} \pi(a|s) \sum_{s' \in \mathcal{S}} P(s'|s,a) \left[ R(s,a,s') + \gamma V_\pi(s') \right]
$$

This equation can be decomposed to understand its components:

- $\pi(a|s)$ is the probability of taking action $a$ in state $s$ under policy $\pi$
- $P(s'|s,a)$ is the probability of transitioning to state $s'$ after taking action $a$ in state $s$
- $R(s,a,s')$ is the expected immediate reward for that transition
- $\gamma V_\pi(s')$ is the discounted value of the next state

In essence, this equation states: "The value of a state is the expected immediate reward plus the expected discounted
value of the next state, given that we follow policy $\pi$."

**2. For Action Values**:

$$
Q_\pi(s,a) = \sum_{s' \in \mathcal{S}} P(s'|s,a) \left[ R(s,a,s') + \gamma \sum_{a' \in \mathcal{A}(s')} \pi(a'|s') Q_\pi(s',a') \right]
$$

This equation states: "The value of taking action $a$ in state $s$ is the expected immediate reward plus the expected
discounted value of taking actions according to policy $\pi$ in the next state."

The Bellman expectation equations can be expressed more compactly using the Bellman expectation operator $T^\pi$:

$$
V_\pi = T^\pi V_\pi \quad \text{and} \quad Q_\pi = T^\pi Q_\pi
$$

This formulation highlights that the true value functions are fixed points of their respective Bellman operators.

**Bellman Optimality Equations**

The Bellman optimality equations describe the recursive relationships for the optimal value functions:

**1. For Optimal State Values**:

$$
V_*(s) = \max_{a \in \mathcal{A}(s)} \sum_{s' \in \mathcal{S}} P(s'|s,a) \left[ R(s,a,s') + \gamma V_*(s') \right]
$$

This equation states that the optimal value of a state is the expected return of taking the best action in that state.

**2. For Optimal Action Values**:

$$
Q_*(s,a) = \sum_{s' \in \mathcal{S}} P(s'|s,a) \left[ R(s,a,s') + \gamma \max_{a' \in \mathcal{A}(s')} Q_*(s',a') \right]
$$

This equation states that the optimal value of a state-action pair is the expected immediate reward plus the expected
discounted value of the next state's best action.

The Bellman optimality equations can be expressed using the Bellman optimality operator $T^*$:

$$
V_* = T^* V_* \quad \text{and} \quad Q_* = T^* Q_*
$$

Importantly, these equations are nonlinear due to the max operation, making them more challenging to solve than the
expectation equations.

**Mathematical Properties of Bellman Operators**

The Bellman operators possess several important mathematical properties:

**1. Contraction Mapping**: Both $T^\pi$ and $T^*$ are contraction mappings in the supremum norm:

$$
|T^\pi V - T^\pi U|*\infty \leq \gamma |V - U|*\infty \quad \text{and} \quad |T^* V - T^* U|*\infty \leq \gamma |V - U|*\infty
$$

Where $|V|_\infty = \max_s |V(s)|$.

This property ensures that repeatedly applying these operators to any initial value function will eventually converge to
the true value function, with a convergence rate determined by $\gamma$.

**2. Monotonicity**: If $V \leq U$ (pointwise), then $T^\pi V \leq T^\pi U$ and $T^* V \leq T^* U$.

This property ensures that value iteration maintains proper ordering of value estimates throughout the iterative
process.

**3. Unique Fixed Point**: Due to the contraction mapping property, both operators have unique fixed points
(respectively $V_\pi$ and $V_*$), guaranteeing a unique solution to the Bellman equations.

**Solving Bellman Equations**

Several approaches exist for solving the Bellman equations:

**1. Direct Solution**: For small MDPs with known dynamics, the Bellman expectation equations form a system of linear
equations that can be solved directly:

    $$(I - \gamma P^\pi) V_\pi = R^\pi$$

    Where $P^\pi$ is the state transition matrix under policy $\pi$, and $R^\pi$ is the expected reward vector.

**2. Iterative Solution**: Methods like value iteration repeatedly apply the Bellman operator to converge to the
solution:

$$
V_{k+1} = T^* V_k
$$

This approach converges at a geometric rate:

$$
|V_k - V_*|\*\infty \leq \gamma^k |V_0 - V**|_\infty
$$

**3. Sampling-Based Approximation**: When the model (transition probabilities and rewards) is unknown, methods like
Q-learning use sample transitions to approximate the solution:

$$
Q(s,a) \leftarrow Q(s,a) + \alpha \left[ R + \gamma \max_{a'} Q(s',a') - Q(s,a) \right]
$$

Where $\alpha$ is a learning rate and $(s,a,r,s')$ is a sampled transition.

**4. Function Approximation**: For large or continuous state spaces, parametrized functions approximate value functions:

$$
V_\theta(s) \approx V_\pi(s) \quad \text{or} \quad Q_\theta(s,a) \approx Q_\pi(s,a)
$$

Where $\theta$ is a parameter vector optimized to minimize approximation error.

**Bellman Error and Residual**

The quality of an approximate value function can be assessed using Bellman error:

$$
\delta_V(s) = \left| V(s) - \left( \sum_{a} \pi(a|s) \sum_{s'} P(s'|s,a) \left[ R(s,a,s') + \gamma V(s') \right] \right) \right|
$$

The mean squared Bellman error is often used as an optimization objective:

$$
MSBE = \mathbb{E}_\mu \left[ \delta_V(s)^2 \right]
$$

Where $\mu$ is a state distribution.

**The Role of Bellman Equations in RL Algorithms**

The Bellman equations underpin various reinforcement learning algorithms:

**1. Dynamic Programming**: Methods like policy iteration and value iteration directly leverage the Bellman equations to
compute optimal policies when the model is known.

**2. Temporal Difference Learning**: Algorithms like TD(0), SARSA, and Q-learning use sampled experiences to approximate
solutions to the Bellman equations without requiring a model.

**3. Function Approximation Methods**: Deep Q-Networks (DQN) and other deep RL approaches minimize various forms of
Bellman error to learn approximate value functions.

**4. Policy Gradient Methods**: While these methods optimize policies directly, they often utilize value functions that
satisfy Bellman-like equations as baselines or critics.

The Bellman equations provide more than just a mathematical formulation; they offer insight into the structure of
sequential decision problems and guide the development of efficient algorithms for solving them. Understanding these
equations and their properties is fundamental to mastering reinforcement learning theory and practice.

#### Markov Decision Processes: The Formal Framework

##### The Markov Property: Why History Matters (or Doesn't)

The Markov property represents a fundamental assumption in reinforcement learning that enables mathematical tractability
while still capturing the essential dynamics of many real-world sequential decision problems. This property defines a
specific constraint on how the future depends on the past, with profound implications for algorithm design and
theoretical analysis.

**Formal Definition of the Markov Property**

A state $S_t$ is said to have the Markov property if and only if:

$$
P(S_{t+1} | S_t, A_t, S_{t-1}, A_{t-1}, ..., S_0, A_0) = P(S_{t+1} | S_t, A_t)
$$

In other words, the future state $S_{t+1}$ depends only on the current state $S_t$ and action $A_t$, not on the history
of states and actions that preceded them. The current state encapsulates all relevant information from the history that
is necessary for predicting the future.

Similarly, for the reward signal:

$$
P(R_{t+1} | S_t, A_t, S_{t-1}, A_{t-1}, ..., S_0, A_0) = P(R_{t+1} | S_t, A_t)
$$

**Information-Theoretic Interpretation**

From an information-theoretic perspective, a Markovian state serves as a sufficient statistic for the history. This
means that:

$$
I(S_{t+1}; H_t | S_t, A_t) = 0
$$

Where $I$ denotes mutual information and $H_t = {S_0, A_0, ..., S_{t-1}, A_{t-1}}$ represents the history up to time
$t$. This equation indicates that, conditioned on the current state and action, the history provides no additional
information about the next state.

**Implications of the Markov Property**

**1. State Sufficiency**: The current state provides sufficient statistics for the future. Once we know the current
state, the history becomes irrelevant for predicting the future. This dramatically simplifies the decision-making
problem, as the agent need only consider the current state rather than the entire history.

**2. Memoryless System**: The system has no "memory" beyond what is captured in the state. This does not mean that
history is unimportant, but rather that all relevant historical information must be encoded in the state representation.

**3. Transition Independence**: Each state transition depends only on the most recent state and action, not on how the
system arrived at that state. This enables us to model transitions using simple conditional probabilities $P(s'|s,a)$.

**4. Representation Efficiency**: Markovian states allow for compact representations of environment dynamics, as we need
only specify one-step transition probabilities rather than conditional distributions over entire sequences.

**5. Policy Simplification**: Optimal policies for Markovian systems need only map states to actions, not entire
histories to actions, significantly reducing the policy search space.

**Example: Markovian vs. Non-Markovian States**

Consider a robot navigating a corridor with the goal of reaching a specific room:

- **Non-Markovian State**: If the state only includes the robot's current position (e.g., "Corridor A"), it lacks the
  Markov property. Knowing the robot is in "Corridor A" is insufficient to predict its next location if the corridors
  form a loop and the robot's movement depends on its direction.
- **Markovian State**: If the state includes both position and orientation (e.g., "Corridor A, facing north"), it
  possesses the Markov property. This state contains sufficient information to predict the next state given an action,
  regardless of how the robot arrived at this state-orientation combination.

**Handling Non-Markovian Environments**

In practice, many real-world problems do not naturally present Markovian states. Several approaches address this
challenge:

**1. State Augmentation**: Including additional information in the state to make it Markovian. For example:

- Adding velocity to position for physical systems
- Including derivatives of sensor readings
- Incorporating time-of-day information for time-dependent problems

**2. History-Based State Construction**: Explicitly incorporating history into the state representation:

- Using the last $n$ observations: $S_t = (O_t, O_{t-1}, ..., O_{t-n+1})$
- Using eligibility traces to maintain a decaying memory of past events

**3. State Inference**: For partially observable environments, maintaining a belief state (probability distribution over
possible states) based on observation history.

**4. Recurrent Neural Networks**: Using architectures with internal memory (e.g., LSTM, GRU) to implicitly learn
temporal dependencies and construct Markovian representations from sequences of observations.

**5. Predictive State Representations**: Representing state as predictions of future observations, which can capture the
information needed for Markovian dynamics.

**The Markov Property and Value Functions**

The Markov property is essential for the definition of value functions in reinforcement learning. For a state to have a
well-defined value, independent of history, it must be Markovian. The value function equations assume that future
rewards depend only on the current state and subsequent actions, not on the history of states and actions.

The Bellman equations, which form the basis for many RL algorithms, rely explicitly on the Markov property:

$$
V_\pi(s) = \sum_{a} \pi(a|s) \sum_{s',r} p(s',r|s,a) [r + \gamma V_\pi(s')]
$$

This recursive definition only holds if the future (represented by $s'$ and $r$) depends solely on the current state $s$
and action $a$.

**Theoretical Impacts on Algorithm Design**

The Markov property influences algorithm design in several ways:

**1. Convergence Guarantees**: Many RL algorithms, including TD learning and Q-learning, have theoretical convergence
guarantees only for Markovian environments.

**2. Sample Efficiency**: In Markovian environments, experiences can be treated as independent samples from the
transition distribution, enabling more efficient learning.

**3. Bootstrapping**: The technique of using value estimates to update other value estimates (bootstrapping) relies on
the Markov property to ensure consistency.

**4. Planning Horizon**: For Markovian problems, planning algorithms need only consider future trajectories from the
current state, not conditioning on how that state was reached.

The Markov property provides the mathematical foundation for modeling sequential decision processes as Markov Decision
Processes (MDPs), enabling the development of efficient algorithms with strong theoretical guarantees. Understanding
this property is essential for designing appropriate state representations and selecting suitable RL algorithms for a
given problem domain.

##### MDP Formalization: Structuring Sequential Decision Problems

A Markov Decision Process (MDP) provides a rigorous mathematical framework for modeling sequential decision-making
problems where outcomes are partly random and partly under the control of a decision-maker. MDPs formalize the
interaction between an agent and its environment, serving as the theoretical foundation for most reinforcement learning
algorithms.

**The MDP Quintuple: Formal Definition**

An MDP is defined as a 5-tuple $(S, A, P, R, \gamma)$ where:

**1. State Space $S$**: The set of all possible states the environment can be in. The state space may be:

- **Finite**: Contains a countable number of states (e.g., positions on a game board)
- **Infinite**: Contains an uncountable number of states (e.g., continuous physical systems)
- **Discrete**: States are distinct and separate (e.g., discrete grid cells)
- **Continuous**: States vary continuously (e.g., joint angles of a robot arm)

The state at time $t$ is denoted $S_t \in S$.

**2. Action Space $A$**: The set of all possible actions available to the agent. Like the state space, this can be
finite or infinite, discrete or continuous. The action at time $t$ is denoted $A_t \in A(S_t)$, where $A(s)$ represents
the set of actions available in state $s$.

**3. Transition Function $P$**: Defines the dynamics of the environment as a conditional probability distribution:

$$
P(s'|s,a) = Pr(S_{t+1}=s'|S_t=s, A_t=a)
$$

This function specifies the probability of transitioning to state $s'$ after taking action $a$ in state $s$. It captures
the underlying dynamics of the environment, including any stochasticity.

**4. Reward Function $R$**: Defines the immediate reward received after transitions:

$$
R(s,a,s') = \mathbb{E}[R_{t+1}|S_t=s, A_t=a, S_{t+1}=s']
$$

This function specifies the expected immediate reward when transitioning from state $s$ to state $s'$ after taking
action $a$. Sometimes simplified as $R(s,a)$ when the reward depends only on the current state and action.

**5. Discount Factor $\gamma$**: A value in $[0,1]$ that determines the present value of future rewards. It balances
immediate versus future rewards and ensures that the infinite sum of rewards remains finite in continuing tasks.

**Mathematical Properties of MDPs**

The MDP framework relies on several important mathematical properties:

**1. Markov Property**: As discussed in the previous section, the state transitions satisfy:

$$
P(S_{t+1}=s'|S_t=s, A_t=a, S_{t-1}, A_{t-1}, ...) = P(S_{t+1}=s'|S_t=s, A_t=a)
$$

This property allows the process to be modeled using just the current state and action, without requiring the entire
history.

**2. Stationarity**: The transition and reward functions do not change over time. For all $t$:

$$
P(S_{t+1}=s'|S_t=s, A_t=a) = P(S_{t'+1}=s'|S_{t'}=s, A_{t'}=a)
$$

$$
R(S_t=s, A_t=a, S_{t+1}=s') = R(S_{t'}=s, A_{t'}=a, S_{t'+1}=s')
$$

This property ensures that the environment dynamics remain consistent, allowing for coherent learning and planning.

**3. Completeness**: The state space $S$ contains all possible states of the environment, and the action space $A(s)$
contains all possible actions in each state.

**Types of MDPs**

MDPs can be categorized based on several characteristics:

**1. Finite vs. Infinite MDPs**: In finite MDPs, both the state and action spaces are finite. Most theoretical
guarantees in RL apply specifically to finite MDPs. Infinite MDPs (with continuous states or actions) typically require
function approximation techniques.

**2. Deterministic vs. Stochastic MDPs**: In deterministic MDPs, the transition function maps each state-action pair to
exactly one next state: $P(s'|s,a) \in {0,1}$. In stochastic MDPs, transitions have probability distributions over next
states.

**3. Episodic vs. Continuing MDPs**: Episodic MDPs model tasks that terminate in certain states, while continuing MDPs
model tasks that proceed indefinitely without termination.

**4. Fully vs. Partially Observable MDPs**: In fully observable MDPs, the agent can directly observe the true state of
the environment. Partially Observable MDPs (POMDPs) extend MDPs to situations where the agent cannot directly observe
the true state but must infer it from observations.

**The Objective in MDPs**

The goal in an MDP is to find an optimal policy $\pi^*$ that maximizes the expected cumulative reward. The objective
function, often called the value function, can be formulated in several ways:

**1. Finite-Horizon Undiscounted Return**: For episodic tasks with a fixed time horizon $T$:

$$
J(\pi) = \mathbb{E}*\pi\left[\sum*{t=0}^{T-1} R_{t+1}\right]
$$

**2. Infinite-Horizon Discounted Return**: For continuing tasks or when prioritizing near-term rewards:

$$
J(\pi) = \mathbb{E}*\pi\left[\sum*{t=0}^{\infty} \gamma^t R_{t+1}\right]
$$

**3. Average Reward**: Alternative formulation focusing on the long-term average reward per time step:

$$
J(\pi) = \lim_{T\to\infty} \frac{1}{T}\mathbb{E}*\pi\left[\sum*{t=0}^{T-1} R_{t+1}\right]
$$

**Optimal Solutions to MDPs**

Finding an optimal policy involves computing the optimal value functions:

**1. Optimal State-Value Function**:

$$
V^*(s) = \max_\pi V^\pi(s) \text{ for all } s \in S
$$

**2. Optimal Action-Value Function**:

$$
Q^*(s,a) = \max_\pi Q^\pi(s,a) \text{ for all } s \in S, a \in A
$$

Once we know $Q^*$, the optimal deterministic policy can be derived as:

$$
\pi^*(s) = \arg\max_a Q^*(s,a)
$$

For finite MDPs, it can be proven that there always exists at least one deterministic optimal policy.

**Solving MDPs: Major Approaches**

MDPs can be solved using various approaches, depending on whether the model (transition and reward functions) is known:

**1. Dynamic Programming**: When the model is known, methods like value iteration and policy iteration can find the
optimal policy:

- **Value Iteration**: Iteratively computes the optimal value function:

$$
V_{k+1}(s) = \max_a \sum_{s'} P(s'|s,a)[R(s,a,s') + \gamma V_k(s')]
$$

- **Policy Iteration**: Alternates between policy evaluation and policy improvement:

Evaluation:

$$
V^{\pi_k}(s) = \sum_a \pi_k(a|s) \sum_{s'} P(s'|s,a)[R(s,a,s') + \gamma V^{\pi_k}(s')]
$$

Improvement:

$$
\pi_{k+1}(s) = \arg\max_a \sum_{s'} P(s'|s,a)[R(s,a,s') + \gamma V^{\pi_k}(s')]
$$

The algorithm converges to $\pi^*$ in a finite number of iterations for finite MDPs.

**2. Model-Free Methods**: When the model is unknown, methods like Q-learning or SARSA learn from experience:

- **Q-learning**: Updates Q-values based on observed transitions:

$$
Q(s,a) \leftarrow Q(s,a) + \alpha [R + \gamma \max_{a'} Q(s',a') - Q(s,a)]
$$

This algorithm converges to $Q^*$ under appropriate conditions, without requiring a model.

- **SARSA**: Updates Q-values using the next action actually taken:

$$
Q(s,a) \leftarrow Q(s,a) + \alpha [R + \gamma Q(s',a') - Q(s,a)]
$$

This algorithm learns the action-value function for the policy being followed (on-policy learning).

**3. Model-Based RL**: Learns a model of the environment from experience, then uses planning algorithms to find an
optimal policy:

- Estimate $\hat{P}(s'|s,a)$ and $\hat{R}(s,a,s')$ from observed transitions
- Apply dynamic programming methods to the estimate

The MDP formalization provides a rigorous mathematical foundation for reinforcement learning. It allows us to precisely
define the problem, analyze solution properties, and develop algorithms with theoretical guarantees. Understanding MDPs
is essential for mastering the fundamental principles of reinforcement learning and applying them effectively to
real-world problems.

##### One-Step Dynamics: How Actions Change States

One-step dynamics define how the environment responds to the agent's actions at each time step. These dynamics
constitute the core of the Markov Decision Process (MDP) framework and describe the immediate consequences of actions in
terms of state transitions and rewards.

**Probability Distributions of Transitions**

The one-step dynamics of an MDP are fully specified by the joint probability distribution:

$$
p(s', r | s, a) = Pr(S_{t+1}=s', R_{t+1}=r | S_t=s, A_t=a)
$$

This distribution captures both the next state and the immediate reward resulting from taking action $a$ in state $s$.
It represents the complete specification of the environment's immediate response to the agent's actions.

<div align="center"> <img src="images/robot.png" width="600" height="auto"> <p style="color: #555;">Figure: One-step dynamics for robot battery states with different actions</p> </div>

From this fundamental distribution, we can derive several important quantities:

**1. State-Transition Probabilities**:

$$
p(s' | s, a) = Pr(S_{t+1}=s' | S_t=s, A_t=a) = \sum_r p(s', r | s, a)
$$

This gives the probability of transitioning to state $s'$ after taking action $a$ in state $s$, regardless of the reward
received.

**2. Expected Immediate Rewards**:

$$
r(s, a) = \mathbb{E}[R_{t+1} | S_t=s, A_t=a] = \sum_{s', r} r \cdot p(s', r | s, a)
$$

This gives the expected immediate reward for taking action $a$ in state $s$, averaging over all possible next states.

**3. Expected Rewards for State-Action-Next-State**:

$$
r(s, a, s') = \mathbb{E}[R_{t+1} | S_t=s, A_t=a, S_{t+1}=s'] = \sum_r r \cdot \frac{p(s', r | s, a)}{p(s' | s, a)}
$$

This gives the expected reward when a specific transition from $s$ to $s'$ occurs after taking action $a$.

**The Markov Property in One-Step Dynamics**

The one-step dynamics embody the Markov property by depending only on the current state and action, not on the history
of states and actions. This property is expressed mathematically as:

$$
p(s', r | s, a, s_{t-1}, a_{t-1}, ..., s_0, a_0) = p(s', r | s, a)
$$

This independence from history is what allows us to specify the environment's dynamics concisely using one-step
transition probabilities rather than distributions over entire trajectories.

**Example: Robot Battery Management**

To illustrate one-step dynamics, consider a robot with two battery states: "high" and "low". The robot can perform three
actions: "wait," "search," and "recharge." The one-step dynamics might be represented as follows:

For the "high" battery state:

- Action "wait":
    - 90% chance to remain in "high" state with reward +1
    - 10% chance to transition to "low" state with reward +1
- Action "search":
    - 70% chance to remain in "high" state with reward +4
    - 30% chance to transition to "low" state with reward +4

For the "low" battery state:

- Action "wait":
    - 100% chance to remain in "low" state with reward +1
- Action "search":
    - 20% chance to remain in "low" state with reward +2
    - 80% chance to drain completely and end episode with reward -3
- Action "recharge":
    - 100% chance to transition to "high" state with reward 0

These dynamics fully specify how the environment responds to each action in each state, which is essential for planning
and learning optimal policies.

**Representing One-Step Dynamics**

For finite MDPs, one-step dynamics can be represented in various ways:

**1. Tabular Representation**: A table with entries for each $(s, a, s', r)$ combination, specifying the probability
$p(s', r | s, a)$.

**2. Transition Matrices**: For each action $a$, a matrix $P^a$ where the entry in row $s$ and column $s'$ gives the
probability $p(s' | s, a)$.

**3. Reward Matrices**: Similar matrices specifying expected rewards for each transition.

For large or continuous state spaces, more compact representations are necessary:

**1. Factored Representations**: The state is decomposed into variables, and transitions for each variable depend only
on a subset of other variables.

**2. Dynamic Bayesian Networks**: Graphical models that capture conditional dependencies in state transitions.

**3. Parametric Models**: Mathematical functions with parameters that determine transition probabilities and rewards.

**Deterministic vs. Stochastic Dynamics**

One-step dynamics can be characterized as deterministic or stochastic:

**1. Deterministic Dynamics**: Each state-action pair leads to exactly one next state and reward:

$$
s_{t+1} = f(s_t, a_t) \text{ and } r_{t+1} = R(s_t, a_t, s_{t+1})
$$

For example, in a chess game, each move leads to a deterministic next board position.

**2. Stochastic Dynamics**: State transitions and rewards have probability distributions:

$$
p(s_{t+1}, r_{t+1} | s_t, a_t)
$$

For example, in backgammon, the roll of dice introduces randomness into state transitions.

Most real-world problems involve stochastic dynamics due to inherent randomness, incomplete state information, or
simplified modeling.

**Mathematical Properties of One-Step Dynamics**

One-step dynamics satisfy several important mathematical properties:

**1. Normalization**: For any state-action pair $(s, a)$, the probabilities of all possible outcomes must sum to 1:

$$
\sum_{s', r} p(s', r | s, a) = 1
$$

**2. Non-negativity**: All probabilities are non-negative:

$$
p(s', r | s, a) \geq 0 \text{ for all } s, a, s', r
$$

**3. Conditional Independence**: Given the current state and action, the next state and reward are conditionally
independent of all previous states and actions.

**Engineering One-Step Dynamics**

In designed systems or simulations, one-step dynamics can be engineered to have desirable properties:

**1. Ergodicity**: Ensuring that any state can be reached from any other state under some policy, which facilitates
exploration and learning.

**2. Safety Constraints**: Designing dynamics to avoid catastrophic states or transitions.

**3. Curriculum Structure**: Creating dynamics that gradually increase in complexity as the agent improves.

**4. Reward Shaping**: Modifying the reward function to guide learning without changing the optimal policy.

Understanding one-step dynamics is crucial for developing and analyzing reinforcement learning algorithms. They form the
foundation for both model-based approaches (which directly use these dynamics for planning) and model-free approaches
(which implicitly learn these dynamics through experience). The specification of one-step dynamics fully defines the
environment's behavior, making it the essential bridge between the agent's actions and their consequences.

##### Task Types: Episodic vs Continuing Problems

Reinforcement learning tasks can be categorized into two fundamental types based on their temporal structure: episodic
tasks and continuing tasks. This distinction has important implications for problem formulation, algorithm design, and
evaluation metrics.

**Episodic Tasks: Problems with Natural Endpoints**

Episodic tasks are characterized by experiences that divide naturally into sequences with definite endpoints, called
episodes. Each episode terminates in a special state called a terminal state, after which the environment is reset to an
initial state to begin a new episode.

**Key Characteristics of Episodic Tasks:**

**1. Terminal States**: Episodes conclude when the agent reaches a terminal state, which might represent:

- Task completion (reaching a goal)
- Task failure (violating a constraint)
- A predefined limit (maximum number of steps)

**2. Finite Horizon**: There is a natural end to the interaction sequence, either due to the environment dynamics or
imposed constraints.

**3. Episode Reset**: After reaching a terminal state, the environment is reset to an initial state, which may be fixed
or sampled from a distribution.

**4. Cumulative Return**: The return in episodic tasks is calculated as the finite sum of rewards within an episode:

$$
G_t = R_{t+1} + R_{t+2} + ... + R_T
$$

Where $T$ is the time step at which the terminal state is reached. Note that discounting ($\gamma < 1$) may still be
used in episodic tasks, though it's not required for mathematical convergence.

**Examples of Episodic Tasks:**

- Games with clear endings (chess, Go, video game levels)
- Navigation tasks with goal locations
- Assembly tasks with completion criteria
- Trading simulations with fixed time horizons
- Academic examinations with a fixed set of questions

**Mathematical Formulation of Episodic MDPs:**

An episodic MDP introduces the concept of a terminal state set $\mathcal{S}*{\text{terminal}} \subset \mathcal{S}$. When
the agent enters any state $s \in \mathcal{S}*{\text{terminal}}$, the episode ends. The transition dynamics may be
modified to reflect this structure:

$$
P(s'|s,a) = \begin{cases} P_{\text{original}}(s'|s,a) & \text{if } s \notin \mathcal{S}*{\text{terminal}} \ P*{\text{reset}}(s') & \text{if } s \in \mathcal{S}_{\text{terminal}} \end{cases}
$$

Where $P_{\text{reset}}(s')$ is the probability distribution over initial states for the next episode.

**Continuing Tasks: Problems Without Natural Endpoints**

Continuing tasks represent ongoing processes that have no natural terminal states and theoretically continue
indefinitely. These tasks model scenarios where the agent must make decisions over an extended or infinite time horizon.

**Key Characteristics of Continuing Tasks:**

**1. No Terminal States**: The agent-environment interaction continues without end, or at least without pre-defined
ending conditions.

**2. Infinite Horizon**: The sequence of states, actions, and rewards is unbounded, requiring mathematical techniques to
handle infinite sums.

**3. Discounted Return**: To ensure the return is finite and well-defined, we typically use discounting:

$$
G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + ... = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}
$$

Where $\gamma \in [0,1)$ is the discount factor. This geometric discounting ensures that the infinite sum converges to a
finite value, provided rewards are bounded.

**4. Alternative Formulation**: Instead of discounting, continuing tasks can be evaluated based on the long-term average
reward:

$$
\overline{R} = \lim_{n\to\infty} \frac{1}{n} \sum_{t=1}^{n} \mathbb{E}[R_t]
$$

This approach focuses on the steady-state performance rather than the cumulative discounted reward.

**Examples of Continuing Tasks:**

- Process control in manufacturing
- Power grid management
- Stock portfolio management
- Autonomous driving in operational settings
- Ongoing recommendation systems
- Climate control systems

<div align="center"> <img src="images/summary.png" width="600" height="auto"> <p style="color: #555;">Figure: Summary comparison of episodic vs continuing tasks</p> </div>

**Practical Considerations and Implementation Techniques**

**1. Hybrid Approaches:**

Some problems can be formulated as either episodic or continuing, depending on the specific goals and constraints:

- A robot navigation task could be treated as episodic with explicit goals or continuing with ongoing operation
- Customer interaction systems might be viewed as episodic (per customer session) or continuing (across all customers)
- Financial trading can be framed as episodic (daily trading periods) or continuing (ongoing portfolio management)

The choice between these formulations affects the learning objectives and algorithm selection.

**2. Artificially Episodic Tasks:**

Continuing tasks are sometimes approximated as episodic by imposing artificial episode boundaries:

- Dividing a continuous process into fixed-length segments
- Introducing periodic resets for experimental purposes
- Using a sliding window of recent experience

This approach can simplify algorithm implementation but may introduce biases in learning.

**3. Discount Factor Selection:**

The choice of discount factor $\gamma$ has different implications for episodic and continuing tasks:

- In episodic tasks with guaranteed termination, $\gamma$ can be set to 1 if the focus is on total undiscounted return
- In continuing tasks, $\gamma < 1$ is necessary for mathematical convergence
- Values closer to 1 make the agent more far-sighted, while values closer to 0 make it more myopic
- Typical values in practice range from 0.9 to 0.99, depending on the time scale of the problem

**4. Algorithm Applicability:**

Different algorithms may be better suited to either episodic or continuing tasks:

- Monte Carlo methods require episodic tasks since they update based on complete returns
- Temporal Difference methods can handle both episodic and continuing tasks
- Average-reward methods are specifically designed for continuing tasks

**5. Evaluation Metrics:**

Performance metrics differ between episodic and continuing tasks:

- Episodic: Success rate, episode length, total return per episode
- Continuing: Average reward per time step, discounted return from current state

**Theoretical Connections and Equivalences**

Interestingly, there are theoretical connections between episodic and continuing formulations:

**1. Continuing to Episodic Transformation**: Any continuing task can be transformed into an episodic task by
introducing artificial termination with a small probability at each step. If this probability is $1-\gamma$, the
expected return in the transformed episodic task equals the discounted return in the original continuing task.

**2. Episodic to Continuing Transformation**: Episodic tasks can be viewed as continuing tasks where certain states
(terminal states) transition to initial states with probability 1, effectively resetting the environment.

Understanding whether a problem is episodic or continuing is a fundamental step in formulating it within the
reinforcement learning framework. This categorization guides choices about return calculation, algorithm selection, and
evaluation procedures. Both formulations have their strengths and are valuable tools in the reinforcement learning
toolkit, applicable to different types of real-world problems.

```mermaid
graph TD
    A[Reinforcement Learning Task] --> B[Episodic]
    A --> C[Continuing]

    B --> D[Has terminal states]
    B --> E[Finite horizon]
    B --> F[Return: Sum of rewards]

    C --> G[No terminal states]
    C --> H[Infinite horizon]
    C --> I[Return: Discounted sum<br>or Average reward]

    style A fill:#9AE4F5,stroke:#333,stroke-width:2px
    style B fill:#BCFB89,stroke:#333,stroke-width:2px
    style C fill:#FBF266,stroke:#333,stroke-width:2px
    style D fill:#FE9237,stroke:#333,stroke-width:2px
    style E fill:#FE9237,stroke:#333,stroke-width:2px
    style F fill:#FE9237,stroke:#333,stroke-width:2px
    style G fill:#FA756A,stroke:#333,stroke-width:2px
    style H fill:#FA756A,stroke:#333,stroke-width:2px
    style I fill:#FA756A,stroke:#333,stroke-width:2px
```

Through careful consideration of the episodic or continuing nature of a task, reinforcement learning practitioners can
develop more effective solutions that properly account for the temporal structure of the problem at hand.
