# C-2: Classical RL Methods

1. Dynamic Programming Approaches

    - Policy Evaluation and Improvement
    - Value Iteration
    - Limitations of Model-Based Methods

2. Monte Carlo Methods

    - Exploring Policy Space
    - First-Visit and Every-Visit MC
    - MC Prediction and Control
    - Exploration vs Exploitation

3. Temporal Difference Learning
    - TD(0) and Bootstrapping
    - SARSA: On-Policy TD Control
    - Q-Learning: Off-Policy TD Control
    - Expected SARSA

---

##### Dynamic Programming Approaches

Dynamic Programming (DP) provides a collection of algorithmic techniques for solving reinforcement learning problems by
leveraging the Bellman equations when a complete model of the environment is available. The core of dynamic programming
approaches consists of two fundamental procedures: policy evaluation and policy improvement, which together form the
foundation of policy iteration algorithms.

```mermaid
graph TD
    A["Reinforcement Learning <br>Methods"] --> B["Dynamic Programming"]
    A --> C["Monte Carlo Methods"]
    A --> D["Temporal Difference Learning"]

style A fill:#E0F2F1
style B fill:#B2DFDB
style C fill:#80CBC4
style D fill:#4DB6AC
```

##### Policy Evaluation

Policy evaluation, also called prediction, computes the state-value function $V_\pi$ for a given policy $\pi$. This
process systematically determines how good it is to follow a particular policy by calculating the expected return from
each state.

The iterative policy evaluation algorithm proceeds as follows:

1. Initialize the value function: $V(s) \leftarrow 0$ for all states $s \in \mathcal{S}$, except for terminal
   stateswhere $V(\small\text{terminal}) = 0$
2. Repeat until convergence:
    - For each state $s \in \mathcal{S}$: $$V(s) \leftarrow \sum_a \pi(a|s) \sum_{s',r} p(s',r|s,a)[r + \gamma V(s')]$$

This update rule directly applies the Bellman expectation equation. At each iteration, we sweep through the entire state
space, updating the value estimates for each state based on:

- The policy's action selection probabilities $\pi(a|s)$
- The environment's transition dynamics $p(s',r|s,a)$
- The current estimates of successor state values $V(s')$

---

##### Iterative Policy Evaluation: Step-by-Step Process

**Algorithm: Iterative Policy Evaluation**

$$
\begin{aligned}
&\textbf{Initialize:} \\
&\quad V(s) \leftarrow 0 \text{ for all states } s \in \mathcal{S}, \text{ except } V(\text{terminal}) = 0 \\
\\
&\textbf{Repeat until convergence:} \\
&\quad \textbf{For each } s \in \mathcal{S}: \\
&\quad \quad V(s) \leftarrow \sum_a \pi(a|s) \sum_{s',r} p(s',r|s,a)[r + \gamma V(s')]
\end{aligned}
$$

**Detailed Process Explanation**

The iterative policy evaluation algorithm systematically refines value estimates through repeated sweeps:

**Step 1: Initialization Phase**

- Set all state values to zero: $V_0(s) = 0$ for all non-terminal states
- Terminal states remain at zero throughout: $V(\text{terminal}) = 0$
- This provides a baseline from which to build accurate estimates

**Step 2: Iterative Refinement** Each iteration $k$ updates all state values simultaneously using the Bellman
expectation equation:

$$
V_{k+1}(s) = \sum_a \pi(a|s) \sum_{s',r} p(s',r|s,a)[r + \gamma V_k(s')]
$$

**Breaking down the update computation:**

1. **For each state $s$**: Consider all possible actions available in that state
2. **Weight by policy**: Multiply each action's expected value by $\pi(a|s)$ (probability of selecting that action)
3. **Consider all transitions**: For each action, sum over all possible next states $s'$ and rewards $r$
4. **Immediate + Future value**: Combine immediate reward $r$ with discounted future value $\gamma V_k(s')$
5. **Probability weighting**: Weight each transition by its probability $p(s',r|s,a)$

**Information Propagation Pattern**

The algorithm propagates value information backward through the state space:

**Iteration 1**: States directly connected to terminal states receive non-zero values

$$
V_1(\text{adjacent to terminal}) = \text{immediate reward} + \gamma \times 0
$$

**Iteration 2**: States two steps from terminal incorporate information from iteration 1

$$
V_2(\text{two steps away}) = \text{immediate reward} + \gamma \times V_1(\text{adjacent})
$$

**Iteration $k$**: Information has propagated $k$ steps backward from terminal states

**Convergence Process**

The algorithm continues until the value function stabilizes:

**Convergence criterion**:

$$
\max_s |V_{k+1}(s) - V_k(s)| < \theta
$$

Where $\theta$ is a small threshold (e.g., $10^{-6}$).

**Why convergence occurs**:

- The Bellman operator is a contraction mapping with factor $\gamma < 1$
- Each iteration brings estimates closer to the true value function $V_\pi$
- The maximum error decreases geometrically: $\|V_k - V_\pi\|_\infty \leq \gamma^k \|V_0 - V_\pi\|_\infty$

**Computational Characteristics**

- **Time complexity per iteration**: $O(|\mathcal{S}|^2 |\mathcal{A}|)$ for dense transition matrices
- **Space complexity**: $O(|\mathcal{S}|)$ for storing value function
- **Iterations to convergence**: Depends on discount factor $\gamma$ and desired precision

**Practical Implementation Details**

- **Synchronous updates**: Compute all new values using current estimates, then update all simultaneously
- **Two-array method**: Maintain separate arrays for current and next iteration values
- **Stopping condition**: Monitor maximum change across all states rather than individual convergence

This systematic approach ensures that the value function converges to the true expected return for following policy
$\pi$ from each state, providing the foundation for subsequent policy improvement steps.

###### Initial Value Assignment in Policy Evaluation

**What $V_0(s) = 0$ means:**

The subscript $0$ indicates this is the **initial iteration** (before any updates). Setting $V_0(s) = 0$ means we start
with the assumption that every non-terminal state has zero expected return.

**Why start with zeros?**

1. **No prior knowledge**: We don't know the true values yet, so zero is a neutral starting point
2. **Mathematical convenience**: Starting with zeros simplifies the first iteration calculations
3. **Convergence guarantee**: The algorithm will converge to correct values regardless of initialization

**What this looks like in practice:**

Consider a simple 3-state chain: Start → Middle → Goal

**Initial values (iteration 0):**

- $V_0(\text{Start}) = 0$
- $V_0(\text{Middle}) = 0$
- $V_0(\text{Goal}) = 0$ (terminal state, stays 0)

**After iteration 1:**

- $V_1(\text{Middle}) = -1 + 0.9 \times 0 = -1$ (one step to goal)
- $V_1(\text{Start}) = -1 + 0.9 \times 0 = -1$ (uses old Middle value)

**After iteration 2:**

- $V_2(\text{Start}) = -1 + 0.9 \times (-1) = -1.9$ (uses updated Middle value)
- $V_2(\text{Middle}) = -1 + 0.9 \times 0 = -1$ (unchanged)

**Alternative initialization:**

You could start with different values:

- Random initialization: $V_0(s) \sim \text{random}$
- Optimistic initialization: $V_0(s) = +100$
- Pessimistic initialization: $V_0(s) = -100$

The algorithm converges to the same final answer regardless, but zero initialization is most common because:

- It's simple and neutral
- It often leads to faster convergence
- It provides intuitive progression (values grow from baseline)

**Key insight:** The $V_0(s) = 0$ is just a starting assumption. The iterative process corrects this initial "guess"
through repeated applications of the Bellman equation until the true expected returns are learned.

###### Terminal State Values and Complex Example

**Clarification on Terminal States:**

Terminal states **can** have non-zero values, but in this specific example, the Goal state has zero value by design.
Here are the different cases:

1. **Goal states with rewards**: $V(\text{Goal}) = \text{reward received}$ (e.g., +10 for winning)
2. **Absorbing states with no final reward**: $V(\text{Terminal}) = 0$ (like my simple example)
3. **Failure states**: $V(\text{Failure}) = \text{negative value}$ (e.g., -100 for losing)

---

###### Numerical Example: Policy Evaluation Step-by-Step

Let's walk through a complete numerical example using a simple grid world.

**Setup: 3-State Linear World**

States: $A \rightarrow B \rightarrow \text{Goal}$

**Environment details:**

- Actions: "move right" (only option in each state)
- Rewards: $-1$ for each step (we want to reach goal quickly)
- Discount factor: $\gamma = 0.9$
- Policy: $\pi(\text{right}|s) = 1.0$ for all non-terminal states

**Transition probabilities:**

- From $A$: move to $B$ with probability $1.0$, get reward $-1$
- From $B$: move to Goal with probability $1.0$, get reward $-1$
- Goal: terminal state, no actions, $V(\text{Goal}) = 0$ always

**Step-by-Step Iterations:**

**Iteration 0 (Initialization):**

$$
V_0(A) = 0, \quad V_0(B) = 0, \quad V_0(\text{Goal}) = 0
$$

**Iteration 1:**

For state $B$:

$$
\begin{aligned}
V_1(B) &= \sum_a \pi(a|B) \sum_{s',r} p(s',r|B,a)[r + \gamma V_0(s')] \\
V_1(B) &= 1.0 \times 1.0 \times [-1 + 0.9 \times V_0(\text{Goal})] \\
V_1(B) &= 1.0 \times [-1 + 0.9 \times 0] = -1
\end{aligned}
$$

For state $A$:

$$
\begin{aligned}
V_1(A) &= 1.0 \times 1.0 \times [-1 + 0.9 \times V_0(B)] \\
V_1(A) &= 1.0 \times [-1 + 0.9 \times 0] = -1
\end{aligned}
$$

**After iteration 1:**

$$
V_1(A) = -1, \quad V_1(B) = -1, \quad V_1(\text{Goal}) = 0
$$

**Iteration 2:**

For state $B$:

$$
V_2(B) = 1.0 \times [-1 + 0.9 \times V_1(\text{Goal})] = -1 + 0.9 \times 0 = -1
$$

For state $A$:

$$
V_2(A) = 1.0 \times [-1 + 0.9 \times V_1(B)] = -1 + 0.9 \times (-1) = -1.9
$$

**After iteration 2:**

$$
V_2(A) = -1.9, \quad V_2(B) = -1, \quad V_2(\text{Goal}) = 0
$$

**Iteration 3:**

For state $B$: $V_3(B) = -1$ (unchanged)

For state $A$:

$$
V_3(A) = -1 + 0.9 \times V_2(B) = -1 + 0.9 \times (-1) = -1.9
$$

**After iteration 3:**

$$
V_3(A) = -1.9, \quad V_3(B) = -1, \quad V_3(\text{Goal}) = 0
$$

**Convergence:** The values have stopped changing, so we've converged.

**Final interpretation:**

- $V(B) = -1$: From state $B$, expect total return of $-1$ (one step to goal)
- $V(A) = -1.9$: From state $A$, expect total return of $-1.9$ (two steps to goal)

**Key observations:**

1. **Started with zeros:** All states began with $V_0 = 0$ (our initial guess)

2. **Information propagated backward:**

    - Iteration 1: State $B$ learned its true value ($-1$)
    - Iteration 2: State $A$ incorporated $B$'s updated value

3. **Convergence:** Values stabilized when no further changes occurred

4. **Physical meaning:** The final values represent the expected total discounted reward from each state under this
   policy

The zero initialization was just a starting point - the algorithm discovered the true expected returns through the
iterative process.

---

##### More Complex Example: 4-State Grid World

Let me create a 2×2 grid with more interesting dynamics:

$$
\begin{array}{|c|c|}
\hline
S & A \\
\hline
B & G \\
\hline
\end{array}
$$

**Setup:**

- States: $S$ (start), $A$, $B$, $G$ (goal with +10 reward)
- Actions: up, down, left, right (if possible)
- Movement rewards: $-1$ per step
- Goal reward: $+10$ when reaching $G$
- Policy: $\pi$ = uniform random (25% for each valid action)
- Discount factor: $\gamma = 0.9$

**Transition Details:**

- From $S$: can go right to $A$ or down to $B$
- From $A$: can go left to $S$ or down to $G$
- From $B$: can go up to $S$ or right to $G$
- From $G$: terminal state, $V(G) = 10$

**Iteration 0 (Initialization):**

$$
V_0(S) = 0, \quad V_0(A) = 0, \quad V_0(B) = 0, \quad V_0(G) = 10
$$

$$
\begin{array}{|c|c|}
\hline
S & A \\
\hline
B & G \\
\hline
\end{array}
$$

##### Detailed Policy Evaluation with Formula Breakdown

**The General Bellman Expectation Equation:**

$$
V(s) = \sum_a \pi(a|s) \sum_{s',r} p(s',r|s,a)[r + \gamma V(s')]
$$

Let me break down each calculation step-by-step with explanations for every component.

**Iteration 1:**

**For state $A$ (can go left to $S$ or down to $G$):**

$$
V_1(A) = \sum_a \pi(a|A) \sum_{s',r} p(s',r|A,a)[r + \gamma V_0(s')]
$$

**Breaking this down:**

- **$\pi(a|A)$**: Policy probabilities for actions in state $A$
    - $\pi(\text{left}|A) = 0.5$ (uniform random policy, 2 actions available)
    - $\pi(\text{down}|A) = 0.5$
- **Action "left"**: Goes to state $S$ with reward $-1$
    - $p(S,-1|A,\text{left}) = 1.0$ (deterministic transition)
    - Contribution: $0.5 \times 1.0 \times [-1 + 0.9 \times V_0(S)]$
- **Action "down"**: Goes to goal $G$ with reward $-1$
    - $p(G,-1|A,\text{down}) = 1.0$ (deterministic transition)
    - Contribution: $0.5 \times 1.0 \times [-1 + 0.9 \times V_0(G)]$

$$
\begin{aligned}
V_1(A) &= 0.5 \times 1 \times [-1 + 0.9 \times V_0(S)] + 0.5 \times 1 \times  [-1 + 0.9 \times V_0(G)] \\
&= 0.5 \times [-1 + 0.9 \times 0] + 0.5 \times [-1 + 0.9 \times 10] \\
&= 0.5 \times (-1) + 0.5 \times (8) = -0.5 + 4 = 3.5
\end{aligned}
$$

###### Transition Probabilities and Deterministic vs. Stochastic Transitions

**What $p(S,-1|A,\text{left}) = 1.0$ means:**

This notation represents: "The probability of transitioning to state $S$ and receiving reward $-1$, given that we're in
state $A$ and take action 'left', is 1.0 (100%)."

**Breaking down the notation:**

- $p(\text{next state}, \text{reward} | \text{current state}, \text{action})$
- $p(S, -1 | A, \text{left}) = 1.0$

**Deterministic Transition Explained:**

**Deterministic transition** means that when you take a specific action in a specific state, the outcome is completely
predictable - there's no randomness involved.

**In our grid world example:**

$$
\begin{array}{|c|c|}
\hline
S & A \\
\hline
B & G \\
\hline
\end{array}
$$

When in state $A$:

- Action "left" **always** goes to state $S$ with reward $-1$
- Action "down" **always** goes to state $G$ with reward $-1$

There's no chance of:

- Getting lost and ending up somewhere else
- Receiving a different reward
- The action failing

**Contrast with Stochastic Transitions:**

In a **stochastic** environment, the same action might have different outcomes:

**Example - Slippery grid world:** When in state $A$ and taking action "left":

- 80% chance: Successfully move to $S$, get reward $-1$
- 10% chance: Slip and stay in $A$, get reward $-1$
- 10% chance: Slip and move to $B$, get reward $-1$

**Mathematical representation:**

- $p(S, -1 | A, \text{left}) = 0.8$
- $p(A, -1 | A, \text{left}) = 0.1$
- $p(B, -1 | A, \text{left}) = 0.1$

Notice that all probabilities sum to 1.0: $0.8 + 0.1 + 0.1 = 1.0$

**Why Our Example Uses Deterministic Transitions:**

1. **Simplicity**: Easier to understand the core policy evaluation concepts
2. **Clear calculations**: No need to handle multiple possible outcomes per action
3. **Predictable environment**: Like a well-controlled robot in a laboratory setting

**Real-world examples:**

**Deterministic:**

- Chess moves (moving a piece always succeeds)
- Digital systems (button press always produces same response)
- Mathematical calculations (2+2 always equals 4)

**Stochastic:**

- Robot navigation (sensors/motors have noise)
- Stock trading (market reactions are unpredictable)
- Weather prediction (inherently uncertain systems)

**How this affects the Bellman equation:**

**Deterministic case:**

$$
V(A) = \sum_a \pi(a|A) \times 1.0 \times [r + \gamma V(\text{next state})]
$$

**Stochastic case:**

$$
V(A) = \sum_a \pi(a|A) \sum_{s'} p(s'|A,a) \times [r + \gamma V(s')]
$$

The stochastic case requires summing over all possible next states, weighted by their probabilities.

In our deterministic example, each action leads to exactly one next state with probability 1.0, which simplifies the
calculations significantly.

**Where each number comes from:**

- **0.5**: Uniform policy probability ($\frac{1}{2}$ actions available)
- **-1**: Step reward for any movement
- **0.9**: Discount factor $\gamma$
- **0 and 10**: Previous iteration values $V_0(S) = 0$, $V_0(G) = 10$

**For state $B$ (can go up to $S$ or right to $G$):**

$$
V_1(B) = \sum_a \pi(a|B) \sum_{s',r} p(s',r|B,a)[r + \gamma V_0(s')]
$$

**Breaking this down:**

- **$\pi(a|B)$**: $\pi(\text{up}|B) = 0.5$, $\pi(\text{right}|B) = 0.5$
- **Action "up"**: Goes to $S$ with reward $-1$
- **Action "right"**: Goes to $G$ with reward $-1$

$$
\begin{aligned}
V_1(B) &= 0.5 \times [-1 + 0.9 \times V_0(S)] + 0.5 \times [-1 + 0.9 \times V_0(G)] \\
&= 0.5 \times (-1) + 0.5 \times (8) = 3.5
\end{aligned}
$$

**For state $S$ (can go right to $A$ or down to $B$):**

$$
V_1(S) = \sum_a \pi(a|S) \sum_{s',r} p(s',r|S,a)[r + \gamma V_0(s')]
$$

**Breaking this down:**

- **$\pi(a|S)$**: $\pi(\text{right}|S) = 0.5$, $\pi(\text{down}|S) = 0.5$
- **Action "right"**: Goes to $A$ with reward $-1$
- **Action "down"**: Goes to $B$ with reward $-1$

$$
\begin{aligned}
V_1(S) &= 0.5 \times [-1 + 0.9 \times V_0(A)] + 0.5 \times [-1 + 0.9 \times V_0(B)] \\
&= 0.5 \times [-1 + 0.9 \times 0] + 0.5 \times [-1 + 0.9 \times 0] \\
&= 0.5 \times (-1) + 0.5 \times (-1) = -1
\end{aligned}
$$

**After iteration 1:**

$$
V_1(S) = -1, \quad V_1(A) = 3.5, \quad V_1(B) = 3.5, \quad V_1(G) = 10
$$

**Iteration 2:**

**For state $A$:** Now we use the updated values from iteration 1: $V_1(S) = -1$, $V_1(G) = 10$

$$
\begin{aligned}
V_2(A) &= 0.5 \times [-1 + 0.9 \times V_1(S)] + 0.5 \times [-1 + 0.9 \times V_1(G)] \\
&= 0.5 \times [-1 + 0.9 \times (-1)] + 0.5 \times [-1 + 0.9 \times 10] \\
&= 0.5 \times (-1.9) + 0.5 \times (8) = -0.95 + 4 = 3.05
\end{aligned}
$$

**Calculation details:**

- **Left action**: $-1 + 0.9 \times (-1) = -1 - 0.9 = -1.9$
- **Down action**: $-1 + 0.9 \times 10 = -1 + 9 = 8$
- **Weighted average**: $0.5 \times (-1.9) + 0.5 \times 8 = 3.05$

**For state $B$:**

$$
V_2(B) = 0.5 \times [-1 + 0.9 \times (-1)] + 0.5 \times [-1 + 0.9 \times 10] = 3.05
$$

**For state $S$:**

Now we use $V_1(A) = 3.5$ and $V_1(B) = 3.5$

$$
\begin{aligned}
V_2(S) &= 0.5 \times [-1 + 0.9 \times V_1(A)] + 0.5 \times [-1 + 0.9 \times V_1(B)] \\
&= 0.5 \times [-1 + 0.9 \times 3.5] + 0.5 \times [-1 + 0.9 \times 3.5] \\
&= 0.5 \times 2.15 + 0.5 \times 2.15 = 2.15
\end{aligned}
$$

**Calculation details:**

- **Right action**: $-1 + 0.9 \times 3.5 = -1 + 3.15 = 2.15$
- **Down action**: $-1 + 0.9 \times 3.5 = 2.15$ (same calculation)
- **Since both actions give same result**: $V_2(S) = 2.15$

**After iteration 2:**

$$
V_2(S) = 2.15, \quad V_2(A) = 3.05, \quad V_2(B) = 3.05, \quad V_2(G) = 10
$$

**Key insights:**

- **0.5 factors**: Come from uniform random policy with 2 actions per state
- **Value propagation**: Information about the goal's high value (10) spreads backward through the network
- **Convergence**: Values are stabilizing as the algorithm learns the true expected returns

**Key observations:**

1. **Non-zero terminal state**: $V(G) = 10$ (the goal reward)
2. **Value propagation**: Information about the +10 goal reward spreads backward
3. **Complex interactions**: States $A$ and $B$ have similar values because they're equidistant from the goal
4. **Convergence process**: Values are stabilizing toward their true expected returns

This example shows how the algorithm handles:

- Non-zero terminal states
- Multiple paths between states
- Stochastic policies (uniform random)
- Complex value propagation patterns

---

```mermaid
graph TD
    B["Dynamic Programming"] --> E["Policy Evaluation"]
    B --> F["Policy Improvement"]
    B --> G["Value Iteration"]

    E --> O["V(s) ← Σ π(a|s)Σ p(s',r|s,a)[r + γV(s')]"]
    F --> P["π'(s) = argmax_a Q_π(s,a)"]
    G --> Q["V(s) ← max_a Σ p(s',r|s,a)[r + γV(s')]"]

    style B fill:#BBDEFB
    style E fill:#FFF3E0
    style F fill:#FFE0B2
    style G fill:#FFCC80
    style O fill:#FFB74D
    style P fill:#FFA726
    style Q fill:#FF9800
```

The algorithm terminates when the value function converges, typically determined by:

$$
\max_s |V_{k+1}(s) - V_k(s)| < \theta
$$

where $\theta$ is a small threshold value.

**Example: Grid World Policy Evaluation**

Consider a 4×4 grid world where the agent can move in four directions. For a uniform random policy (25% probability for
each action in each state), the iteration proceeds as follows:

- Initial values: $V_0(s) = 0$ for all states
- First sweep: Each state's value is updated based on immediate rewards and discounted values of adjacent states
- Subsequent sweeps: Values propagate through the grid as information about rewards flows backward from terminal states
- After sufficient iterations: $V_\pi(s)$ accurately reflects the expected return from each state under the random
  policy

---

##### Policy Improvement

Policy improvement uses the evaluated value function to create a better policy. The fundamental question addressed is:
"How can we improve this policy based on what we know about its value?"

The policy improvement theorem provides the theoretical foundation. It states that if we have:

$$
Q_\pi(s, \pi'(s)) \geq V_\pi(s) \text{ for all } s \in \mathcal{S}
$$

then the new policy $\pi'$ is guaranteed to be at least as good as, and typically better than, the original policy:

$$
V_{\pi'}(s) \geq V_\pi(s) \text{ for all } s \in \mathcal{S}
$$

This leads to a natural policy improvement step that creates a greedy policy with respect to the current value function:

$$
\pi'(s) = \arg\max_a \sum_{s',r} p(s',r|s,a)[r + \gamma V_\pi(s')]
$$

Or equivalently, using action-values:

$$
\pi'(s) = \arg\max_a Q_\pi(s,a)
$$

This improvement step will either:

1. Create a strictly better policy, or
2. Verify that the current policy is already optimal (if no improvement is found)

**Understanding Policy Improvement**

Policy improvement answers: "Given what I know about my current strategy's performance, how can I create a better
strategy?"

**The Policy Improvement Theorem Explained**

The theorem has two parts:

**Condition:** If taking a different action in each state gives at least as good results as the current policy:

$$
Q_\pi(s, \pi'(s)) \geq V_\pi(s) \text{ for all } s \in \mathcal{S}
$$

**Guarantee:** Then the new policy will perform at least as well everywhere:

$$
V_{\pi'}(s) \geq V_\pi(s) \text{ for all } s \in \mathcal{S}
$$

**Simple interpretation:** If you can find better individual actions in each state, your overall policy will be better.

**The Improvement Step**

Create a new policy by being greedy with respect to current value estimates:

$$
\pi'(s) = \arg\max_a Q_\pi(s,a)
$$

**Concrete Example: Robot Navigation**

Consider a robot learning to navigate a 3×3 grid to reach a goal:

**Current policy** $\pi$:

- State $(1,1)$: Move right with probability 0.8, move up with probability 0.2
- State $(1,2)$: Move right with probability 0.6, move up with probability 0.4
- State $(2,1)$: Move up with probability 0.9, move right with probability 0.1

**After policy evaluation**, we computed action-values:

- $Q_\pi((1,1), \text{right}) = -2.5$
- $Q_\pi((1,1), \text{up}) = -3.2$
- $Q_\pi((1,2), \text{right}) = -1.8$
- $Q_\pi((1,2), \text{up}) = -1.5$
- $Q_\pi((2,1), \text{up}) = -1.2$
- $Q_\pi((2,1), \text{right}) = -2.1$

**Current state values** (weighted by policy):

- $V_\pi((1,1)) = 0.8 \times (-2.5) + 0.2 \times (-3.2) = -2.64$
- $V_\pi((1,2)) = 0.6 \times (-1.8) + 0.4 \times (-1.5) = -1.68$
- $V_\pi((2,1)) = 0.9 \times (-1.2) + 0.1 \times (-2.1) = -1.29$

**Policy improvement step:**

$$
\pi'(s) = \arg\max_a Q_\pi(s,a)
$$

**New improved policy** $\pi'$:

- State $(1,1)$: Always move right (since $-2.5 > -3.2$)
- State $(1,2)$: Always move up (since $-1.5 > -1.8$)
- State $(2,1)$: Always move up (since $-1.2 > -2.1$)

**Verification of improvement:**

- $Q_\pi((1,1), \text{right}) = -2.5 \geq -2.64 = V_\pi((1,1))$ ✓
- $Q_\pi((1,2), \text{up}) = -1.5 \geq -1.68 = V_\pi((1,2))$ ✓
- $Q_\pi((2,1), \text{up}) = -1.2 \geq -1.29 = V_\pi((2,1))$ ✓

Since the condition is satisfied, the theorem guarantees $V_{\pi'}(s) \geq V_\pi(s)$ for all states.

---

##### Policy Improvement Using the Same Grid World

Now let's use the same grid world for **policy improvement**.

**Current Setup (after policy evaluation):**

$$
\begin{array}{|c|c|}
\hline
S & A \\
\hline
B & G \\
\hline
\end{array}
$$

**Results from policy evaluation with uniform random policy:**

$$
V_\pi(S) = 2.15, \quad V_\pi(A) = 3.05, \quad V_\pi(B) = 3.05, \quad V_\pi(G) = 10
$$

**Step 1: Calculate Action-Values (Q-values) - Detailed Breakdown**

**General Q-value formula:**

$$
Q_\pi(s,a) = \sum_{s',r} p(s',r|s,a)[r + \gamma V_\pi(s')]
$$

For our deterministic environment, this simplifies to:

$$
Q_\pi(s,a) = r + \gamma V_\pi(\text{next state})
$$

$$
\begin{array}{|c|c|}
\hline
S & A \\
\hline
B & G \\
\hline
\end{array}
$$

**For state $S$ (can go right to $A$ or down to $B$):**

**Action "right" to state $A$:**

- **Transition**: $S \xrightarrow{\text{right}} A$
- **Reward**: $r = -1$ (movement cost)
- **Next state value**: $V_\pi(A) = 3.05$
- **Calculation**: $Q_\pi(S, \text{right}) = -1 + 0.9 \times 3.05$

$$
Q_\pi(S, \text{right}) = -1 + 0.9 \times 3.05 = 1.745
$$

**Action "down" to state $B$:**

- **Transition**: $S \xrightarrow{\text{down}} B$
- **Reward**: $r = -1$
- **Next state value**: $V_\pi(B) = 3.05$

$$
Q_\pi(S, \text{down}) = -1 + 0.9 \times 3.05 = 1.745
$$

**For state $A$ (can go left to $S$ or down to $G$):**

**Action "left" to state $S$:**

- **Transition**: $A \xrightarrow{\text{left}} S$
- **Reward**: $r = -1$
- **Next state value**: $V_\pi(S) = 2.15$
- **Calculation**: $Q_\pi(A, \text{left}) = -1 + 0.9 \times 2.15$

$$
Q_\pi(A, \text{left}) = -1 + 0.9 \times 2.15 = 0.935
$$

**Action "down" to goal $G$:**

- **Transition**: $A \xrightarrow{\text{down}} G$
- **Reward**: $r = -1$ (movement cost)
- **Next state value**: $V_\pi(G) = 10$ (terminal state value)
- **Calculation**: $Q_\pi(A, \text{down}) = -1 + 0.9 \times 10$

$$
Q_\pi(A, \text{down}) = -1 + 0.9 \times 10 = 8
$$

**For state $B$ (can go up to $S$ or right to $G$):**

**Action "up" to state $S$:**

- **Transition**: $B \xrightarrow{\text{up}} S$
- **Reward**: $r = -1$
- **Next state value**: $V_\pi(S) = 2.15$
- **Calculation**: $Q_\pi(B, \text{up}) = -1 + 0.9 \times 2.15 = 0.935$

$$
Q_\pi(B, \text{up}) = -1 + 0.9 \times 2.15 = 0.935
$$

**Action "right" to goal $G$:**

- **Transition**: $B \xrightarrow{\text{right}} G$
- **Reward**: $r = -1$
- **Next state value**: $V_\pi(G) = 10$
- **Calculation**: $Q_\pi(B, \text{right}) = -1 + 0.9 \times 10 = 8$

$$
Q_\pi(B, \text{right}) = -1 + 0.9 \times 10 = 8
$$

**Step 2: Verify Consistency with State Values**

**Check**: Does $V_\pi(s) = \sum_a \pi(a|s) Q_\pi(s,a)$?

**For state $S$ with uniform random policy ($\pi(\text{right}|S) = 0.5$, $\pi(\text{down}|S) = 0.5$):**

$$
\begin{aligned}
V_\pi(S) &= 0.5 \times Q_\pi(S, \text{right}) + 0.5 \times Q_\pi(S, \text{down}) \\
&= 0.5 \times 1.745 + 0.5 \times 1.745 = 1.745
\end{aligned}
$$

**Problem identified**: This gives $V_\pi(S) = 1.745$, but our iterative calculation gave $V_\pi(S) = 2.15$.

**Resolution**: Let me use the Q-value derived state values for consistency. The correct values should be:

$$
V_\pi(S) = 1.745, \quad V_\pi(A) = 3.05, \quad V_\pi(B) = 3.05
$$

**Step 3: Policy Improvement**

**Policy improvement rule:**

$$
\pi'(s) = \arg\max_a Q_\pi(s,a)
$$

**For state $S$:**

- $Q_\pi(S, \text{right}) = 1.745$
- $Q_\pi(S, \text{down}) = 1.745$
- **Both actions equal**: Can choose either. Let's choose $\pi'(S) = \text{right}$

**For state $A$:**

- $Q_\pi(A, \text{left}) = 0.935$
- $Q_\pi(A, \text{down}) = 8$
- **$8 > 0.935$**: Choose $\pi'(A) = \text{down}$

**For state $B$:**

- $Q_\pi(B, \text{up}) = 0.935$
- $Q_\pi(B, \text{right}) = 8$
- **$8 > 0.935$**: Choose $\pi'(B) = \text{right}$

**Step 4: Verification of Policy Improvement Theorem**

**Theorem condition**: $Q_\pi(s, \pi'(s)) \geq V_\pi(s)$ for all states

**For state $S$:**

- **New action**: $\pi'(S) = \text{right}$
- **Check**: $Q_\pi(S, \text{right}) = 1.745 \geq 1.745 = V_\pi(S)$ ✓

**For state $A$:**

- **New action**: $\pi'(A) = \text{down}$
- **Check**: $Q_\pi(A, \text{down}) = 8 \geq 3.05 = V_\pi(A)$ ✓

**For state $B$:**

- **New action**: $\pi'(B) = \text{right}$
- **Check**: $Q_\pi(B, \text{right}) = 8 \geq 3.05 = V_\pi(B)$ ✓

**Conclusion**: All conditions satisfied. The new policy is guaranteed to be at least as good as the old policy, and
strictly better for states $A$ and $B$ since the inequalities are strict.

**Improved Policy Summary:**

- **Old**: Random actions (50-50 split)
- **New**: Direct path to goal from $A$ and $B$, arbitrary choice for $S$

This improvement makes intuitive sense: instead of randomly wandering, the agent now goes directly to the goal when
possible.

**The key insight:** $V_\pi(S) = 2.15$ from our iterative calculation, but when computed from Q-values, we should get
the same result. The discrepancy suggests I need to use the converged values from more iterations.

**Correct verification:** If $V_\pi(S) = 2.15$, then:

- $Q_\pi(S, \text{right}) = -1 + 0.9 \times V_\pi(A)$ where $V_\pi(A) = 3.05$
- This gives us the relationship we calculated.

**The improvement holds because:**

- In state $A$: Choose "down" gives $Q_\pi(A,\text{down}) = 8 \geq V_\pi(A) = 3.05$ ✓
- In state $B$: Choose "right" gives $Q_\pi(B,\text{right}) = 8 \geq V_\pi(B) = 3.05$ ✓
- In state $S$: Both actions give same Q-value equal to $V_\pi(S)$ ✓

**Result:** The new policy will perform better by going directly to the goal from states $A$ and $B$, rather than taking
random actions that might move away from the goal.

---

**Why partial improvement works:**

1. **Unchanged states:** Performance stays exactly the same
2. **Improved states:** Performance gets better
3. **Overall result:** The policy is at least as good, often better

**Practical implications:**

- You can improve policies incrementally
- No need to find better actions for every single state
- Even small improvements in a few states can lead to overall policy improvement
- This is how many algorithms work in practice - they don't necessarily change every state's action on each iteration

The key insight is that policy improvement is **monotonic** - you can only make things better (or keep them the same),
never worse, as long as you satisfy the condition for the states you choose to modify.

**Policy Iteration**

Policy iteration combines evaluation and improvement in an iterative process:

1. **Initialization**: Start with an arbitrary policy $\pi_0$
2. **Policy Evaluation**: Compute $V_{\pi_i}$ for all states
3. **Policy Improvement**: Generate $\pi_{i+1}$ by acting greedily with respect to $V_{\pi_i}$
4. **Convergence Check**: If $\pi_{i+1} = \pi_i$, terminate with $\pi^* = \pi_i$; otherwise return to step 2

For finite MDPs, policy iteration converges to the optimal policy and value function in a finite number of iterations.
This is because:

- There are only a finite number of possible deterministic policies
- Each policy improvement step either improves the policy or confirms optimality
- The sequence of policies must eventually terminate at an optimal policy

---

##### Practical Implementation Considerations

Several techniques enhance the efficiency of policy iteration:

- **Partial Policy Evaluation**: Rather than running evaluation to convergence, perform a fixed number of sweeps before
  policy improvement

- **In-Place Updates**: Update value estimates in place, using the most recent values immediately:

$$
\begin{aligned}
&\text{For each } s \in \mathcal{S}: \\
&\quad v \leftarrow 0 \\
&\quad \text{For each } a \in \mathcal{A}(s): \\
&\quad \quad \text{For each } s',r \text{ with } p(s',r|s,a) > 0: \\
&\quad \quad \quad v \leftarrow v + \pi(a|s)p(s',r|s,a)[r + \gamma V(s')] \\
&\quad V(s) \leftarrow v
\end{aligned}
$$

- **Asynchronous Updates**: Update states in an arbitrary order, allowing some states to be updated more frequently than
  others:

$$
\begin{aligned}
&\text{Select a state } s \\
&v \leftarrow 0 \\
&\text{For each } a \in \mathcal{A}(s): \\
&\quad \text{For each } s',r \text{ with } p(s',r|s,a) > 0: \\
&\quad \quad v \leftarrow v + \pi(a|s)p(s',r|s,a)[r + \gamma V(s')] \\
&V(s) \leftarrow v
\end{aligned}
$$

- **Prioritized Sweeping**: Focus computational resources on states with potentially large Bellman errors:
    - Maintain a priority queue of states based on Bellman error magnitude
    - Update the highest-priority state (largest expected change)
    - After updating a state, update priorities of predecessor states that can reach it in one step
    - Remove updated state from queue if priority falls below threshold
    - Repeat until queue is empty or computational budget exhausted

The key insight is that when we update a state's value, the **predecessor states** (those that can transition to it) may
now have larger Bellman errors because their target values have changed.

Policy evaluation and improvement form the theoretical and algorithmic core of dynamic programming approaches to
reinforcement learning. When the environment model is available, these procedures provide a systematic method for
discovering optimal policies through iterative refinement.

---

##### Prioritized Sweeping: Smart Resource Allocation

**Core Concept**

Prioritized sweeping focuses computational effort on states where updates will have the most impact, rather than blindly
updating all states equally. The key insight is that some state updates are much more valuable than others.

**Detailed Example: Robot Navigation in Grid World**

Consider a 5×5 grid where a robot learns to reach goal state G:

$$
\begin{array}{ccccc}
S & \cdot & \cdot & \cdot & \cdot \\
\cdot & \cdot & \cdot & \cdot & \cdot \\
\cdot & \cdot & X & \cdot & \cdot \\
\cdot & \cdot & \cdot & \cdot & \cdot \\
\cdot & \cdot & \cdot & \cdot & G
\end{array}
$$

Initial state values (all start at 0 except goal = +10):

- Goal G: $V = +10$
- All other states: $V = 0$

```mermaid
flowchart TD
    A["Initialize Priority Queue<br>with all states"] --> B["Select state s with<br>highest priority"]
    B --> C["Update V(s) using<br>Bellman equation"]
    C --> D["Remove s from queue<br>if priority < threshold"]
    D --> E["For each predecessor p<br>that can reach s:"]
    E --> F["Calculate new Bellman error<br>for predecessor p"]
    F --> G["Update priority of p<br>in queue"]
    G --> H{"More predecessors?"}
    H -->|<span style='background-color:khaki; color:black; padding:2px 6px; border-radius:3px'>Yes</span>| E
    H -->|<span style='background-color:khaki; color:black; padding:2px 6px; border-radius:3px'>No</span>| I{"Queue empty or<br>budget exhausted?"}
    I -->|<span style='background-color:khaki; color:black; padding:2px 6px; border-radius:3px'>No</span>| B
I -->|<span style='background-color:khaki; color:black; padding:2px 6px; border-radius:3px'>Yes</span>| J["Algorithm Complete"]

style A fill:#E3F2FD
style B fill:#E1F5FE
style C fill:#B3E5FC
style D fill:#81D4FA
style E fill:#4FC3F7
style F fill:#29B6F6
style G fill:#03A9F4
style H fill:#039BE5
style I fill:#0288D1
style J fill:#0277BD
```

**Step 1: Initialize Priority Queue**

Calculate initial Bellman errors for all states. States adjacent to goal have large errors:

- State $(4,4)$ (left of goal): Bellman error = $|-1 + 0.9 \times 10 - 0| = 8.0$
- State $(3,5)$ (above goal): Bellman error = $|-1 + 0.9 \times 10 - 0| = 8.0$
- Distant states have small errors ≈ 0

**Priority queue:** $[(4,4): 8.0, (3,5): 8.0, \text{others}: 0.0]$

**Step 2: Update Highest-Priority State**

Select $(4,4)$ with priority 8.0:

- Old value: $V(4,4) = 0$
- New value: $V(4,4) = -1 + 0.9 \times 10 = 8.0$
- Remove $(4,4)$ from queue (priority now ≈ 0)

**Step 3: Update Predecessor Priorities**

This is the crucial step. When $V(4,4)$ changes from 0 to 8.0, **predecessor states** that can reach $(4,4)$ now have
larger Bellman errors:

**Predecessors of $(4,4)$:**

- $(3,4)$ (can move down to $(4,4)$)
- $(4,3)$ (can move right to $(4,4)$)
- $(4,5)$ would be below, but this is the goal

**Updated predecessor priorities:**

- State $(3,4)$: New error = $|-1 + 0.9 \times 8.0 - 0| = 6.2$
- State $(4,3)$: New error = $|-1 + 0.9 \times 8.0 - 0| = 6.2$

**New priority queue:** $[(3,5): 8.0, (3,4): 6.2, (4,3): 6.2, \text{others}: 0.0]$

**Step 4: Continue Process**

Next iteration selects $(3,5)$ with priority 8.0:

- Update $V(3,5) = 8.0$
- Add/update predecessors: $(2,5)$, $(3,4)$ get higher priorities

**Why This Works**

**Information propagation:** Value information spreads backward from goal like ripples in water. Prioritized sweeping
ensures we process the "ripples" in order of importance.

**Efficiency comparison:**

**Standard sweeping (inefficient):**

$$
\begin{align}
\text{Iteration 1:} &\quad \text{Update all 25 states (24 changes are tiny)} \\
\text{Iteration 2:} &\quad \text{Update all 25 states (23 changes are tiny)} \\
\text{Iteration 3:} &\quad \text{Update all 25 states (22 changes are tiny)} \\
&\quad \vdots
\end{align}
$$

**Prioritized sweeping (efficient):**

$$
\begin{align}
\text{Update (4,4):} &\quad \text{Large change, important} \\
\text{Update (3,5):} &\quad \text{Large change, important} \\
\text{Update (3,4):} &\quad \text{Medium change, somewhat important} \\
\text{Update (4,3):} &\quad \text{Medium change, somewhat important} \\
\text{Update (2,4):} &\quad \text{Small change, less important}
\end{align}
$$

Skip distant states with negligible changes

**Mathematical Justification**

The Bellman error for state $s$ is:

$$
|\text{error}(s)| = \left|\sum_a \pi(a|s) \sum_{s',r} p(s',r|s,a)[r + \gamma V(s')] - V(s)\right|
$$

When we update $V(s')$, the error for any predecessor $s$ that can reach $s'$ changes by approximately:

$$
\Delta \text{error}(s) \approx \pi(a|s) \cdot p(s'|s,a) \cdot \gamma \cdot \Delta V(s')
$$

Large changes in $V(s')$ create proportionally large changes in predecessor errors, making them high-priority candidates
for the next update.

**Algorithm Termination**

The algorithm stops when:

1. **Priority threshold:** All remaining states have priority below some threshold (e.g., 0.01)
2. **Computational budget:** Fixed number of updates completed
3. **Convergence:** Maximum change in any state value becomes negligible

**Real-World Benefits**

In large state spaces, prioritized sweeping can achieve the same accuracy as standard methods with:

- 10-100× fewer state updates
- Dramatically faster convergence
- Better use of limited computational resources

This makes it practical for real-time planning and large-scale problems where standard dynamic programming would be too
slow.

---

##### In-Place Updates vs. Asynchronous Updates in Dynamic Programming

**In-Place Updates: Using Fresh Information Immediately**

$$
\begin{aligned}
&\text{For each } s \in \mathcal{S}: \\
&\quad v \leftarrow 0 \\
&\quad \text{For each } a \in \mathcal{A}(s): \\
&\quad \quad \text{For each } s',r \text{ with } p(s',r|s,a) > 0: \\
&\quad \quad \quad v \leftarrow v + \pi(a|s)p(s',r|s,a)[r + \gamma V(s')] \\
&\quad V(s) \leftarrow v
\end{aligned}
$$

**Key characteristic:** Updates use the most recently computed values, even within the same iteration sweep.

**Comparison with standard updates:**

- **Standard (two-array) method:** Keep old values in one array, compute new values in another, then copy all at once
- **In-place method:** Update values directly in the same array, using new values immediately

**Example: 3-state chain**

States: $s_1 \rightarrow s_2 \rightarrow s_3$ (goal)

**Standard updates (iteration k):**

$$
\begin{align}
V_{new}(s_1) &\leftarrow -1 + \gamma \cdot V_{old}(s_2)  \text{ // Uses old } V(s_2)\\
V_{new}(s_2) &\leftarrow -1 + \gamma \cdot V_{old}(s_3)  \text{ // Uses old } V(s_3)\\
V_{new}(s_3) &\leftarrow 0                            \text{ // Terminal state}
\end{align}
$$

Copy all: $V_{k+1} = V_{new}$

**In-place updates (same iteration):**

$$
\begin{align}
V(s_1) &\leftarrow -1 + \gamma \cdot V(s_2)     \text{ // Uses current } V(s_2)\\
V(s_2) &\leftarrow -1 + \gamma \cdot V(s_3)     \text{ // Uses current } V(s_3)\\
V(s_3) &\leftarrow 0                            \text{ // Terminal state}
\end{align}
$$

**Advantage:** Information propagates faster through the state space. When we update $V(s_2)$, the immediately following
update to $V(s_1)$ uses the fresh $V(s_2)$ value.

**Result:** In-place updates typically converge faster, requiring fewer iterations.

**Asynchronous Updates: Selective and Flexible Updating**

$$
\begin{aligned}
&\text{Select a state } s \\
&v \leftarrow 0 \\
&\text{For each } a \in \mathcal{A}(s): \\
&\quad \text{For each } s',r \text{ with } p(s',r|s,a) > 0: \\
&\quad \quad v \leftarrow v + \pi(a|s)p(s',r|s,a)[r + \gamma V(s')] \\
&V(s) \leftarrow v
\end{aligned}
$$

**Key characteristic:** Update individual states in any order, potentially skipping states or updating some more
frequently than others.

**Contrast with synchronous updates:**

- **Synchronous:** Update all states in each iteration sweep
- **Asynchronous:** Update selected states based on some criteria

**Example: Grid world navigation**

Consider a 4×4 grid with goal at $(4,4)$:

**Synchronous approach:**

$$
\begin{align}
\text{Iteration 1:} &\quad \text{Update all 16 states} \\
\text{Iteration 2:} &\quad \text{Update all 16 states} \\
\text{Iteration 3:} &\quad \text{Update all 16 states} \\
\vdots
\end{align}
$$

**Asynchronous approach:**

$$
\begin{align}
\text{Update } (3,4) &\text{ - close to goal, high impact} \\
\text{Update } (4,3) &\text{ - close to goal, high impact} \\
\text{Update } (2,4) &\text{ - medium distance from goal} \\
\text{Update } (3,4) \text{ again} &\text{ - reinforce important state} \\
\text{Update } (1,1) &\text{ - far from goal, lower priority} \\
&\vdots
\end{align}
$$

**Selection strategies for asynchronous updates:**

1. **Real-time focus:** Update states the agent is currently in or likely to visit
2. **Error-based:** Update states with largest Bellman errors
3. **Random:** Select states randomly but ensure all are eventually updated
4. **Importance-based:** Update critical states more frequently

**Benefits of asynchronous updates:**

- **Computational efficiency:** Focus resources where they matter most
- **Faster convergence:** Critical states converge quickly
- **Flexibility:** Adapt to problem structure or available computational resources

**Requirements for convergence:** Both methods require that all states be updated infinitely often in the limit, but
asynchronous updates allow much more flexibility in the scheduling.

**Practical implementation difference:**

**Algorithm: In-Place Updates**

$$
\begin{aligned}
&\textbf{For each } s \in \mathcal{S}: \\
&\quad \text{new\_value} \leftarrow \text{compute\_bellman\_update}(s) \\
&\quad V[s] \leftarrow \text{new\_value} \quad \text{// Immediate update}
\end{aligned}
$$

**Algorithm: Asynchronous Updates**

$$
\begin{aligned}
&\textbf{While not converged:} \\
&\quad s \leftarrow \text{select\_state}() \quad \text{// Based on some criterion} \\
&\quad \text{new\_value} \leftarrow \text{compute\_bellman\_update}(s) \\
&\quad V[s] \leftarrow \text{new\_value}
\end{aligned}
$$

**Alternative detailed format:**

**Algorithm: In-Place Value Updates**

$$
\begin{array}{l}
\textbf{For each } s \in \mathcal{S}: \\
\quad \text{1. } v \leftarrow \text{compute\_bellman\_update}(s) \\
\quad \text{2. } V(s) \leftarrow v \quad \text{(use fresh values immediately)}
\end{array}
$$

**Algorithm: Asynchronous Value Updates**

$$
\begin{array}{l}
\textbf{Repeat until convergence:} \\
\quad \text{1. } s \leftarrow \text{select\_state\_by\_criterion}() \\
\quad \text{2. } v \leftarrow \text{compute\_bellman\_update}(s) \\
\quad \text{3. } V(s) \leftarrow v
\end{array}
$$

Both techniques can be combined: asynchronous selection with in-place updates for maximum efficiency.

---

##### Value Iteration

Value iteration represents an elegant and efficient dynamic programming algorithm that combines aspects of policy
evaluation and policy improvement into a single update operation. Instead of maintaining and improving an explicit
policy through separate evaluation and improvement steps, value iteration directly computes the optimal value function,
from which the optimal policy can be derived.

**The Value Iteration Algorithm**

Value iteration applies the Bellman optimality operator in a sequence of sweeps through the state space:

- Initialize $V(s)$ arbitrarily for all states $s \in \mathcal{S}$, except for terminal states where $V(terminal) = 0$
- Repeat until convergence: - For each state $s \in \mathcal{S}$:
    $$
    V(s) \leftarrow \max_a \sum_{s',r} p(s',r|s,a)[r + \gamma V(s')]
    $$
- Once $V$ closely approximates $V^*$, extract the optimal policy:
    $$
    \pi^*(s) = \arg\max_a \sum_{s',r} p(s',r|s,a)[r + \gamma V(s')]
    $$

The core update rule combines a simplified policy improvement step (the $\max_a$ operation) with a one-step policy
evaluation. This update directly implements the Bellman optimality equation, which characterizes the optimal value
function $V^*$.

**Theoretical Basis**

Value iteration can be understood as applying the Bellman optimality operator $T^*$ repeatedly:

$$
T^* V(s) = \max_a \sum_{s',r} p(s',r|s,a)[r + \gamma V(s')]
$$

The Bellman optimality operator has two critical properties:

- It is a contraction mapping in the supremum norm with factor $\gamma$:

$$
|T^* V - T^* U|*\infty \leq \gamma |V - U|*\infty
$$

- The optimal value function $V^*$ is the unique fixed point of this operator:

$$
T^* V^* = V^*
$$

These properties guarantee that repeated application of $T^*$ will converge to $V^*$ regardless of the initial value
function, with a convergence rate determined by $\gamma$.

**Comparison with Policy Iteration**

Value iteration differs from policy iteration in several key aspects:

- **Computational Efficiency**:
    - Policy iteration requires multiple sweeps through the state space for each policy evaluation phase
    - Value iteration performs a single sweep per iteration, making each iteration less computationally intensive
- **Update Structure**:
    - Policy iteration alternates between complete policy evaluation and policy improvement
    - Value iteration combines partial policy evaluation (one sweep) with policy improvement in each update
- **Policy Representation**:
    - Policy iteration explicitly maintains a policy that improves with each iteration
    - Value iteration implicitly improves a policy with each update but only extracts it at the end
- **Convergence Characteristics**:
    - Policy iteration typically requires fewer iterations but more computation per iteration
    - Value iteration requires more iterations but less computation per iteration

**Example: Solving a Simple MDP**

Consider a simple grid world with rewards of -1 for each move, +10 for reaching a goal state, and -10 for falling into a
pit. Starting with all zeros for non-terminal states:

- **Iteration 1**: States adjacent to the goal acquire positive values; states adjacent to pits acquire negative values
- **Iteration 2**: These values propagate one step further through the grid
- **Subsequent iterations**: Information about rewards continues to propagate through the state space
- **Convergence**: After sufficient iterations, $V(s)$ closely approximates $V^*(s)$ for all states

Once value iteration converges, the optimal policy can be extracted by selecting actions that maximize the expected next
state value:

$$
\pi^*(s) = \arg\max_a \sum_{s',r} p(s',r|s,a)[r + \gamma V^*(s')]
$$

**Optimality Guarantees and Convergence Analysis**

The error in the value function after $k$ iterations is bounded by:

$$
|V_k - V^*|_\infty \leq \frac{\gamma^k}{1-\gamma}|V_0 - V^*|_\infty
$$

This bound shows that convergence is faster with smaller $\gamma$ values, as distant rewards have less influence on
current state values.

A practical stopping criterion is:

$$
\max_s |V_{k+1}(s) - V_k(s)| < \frac{\epsilon(1-\gamma)}{2\gamma}
$$

This ensures that the resulting policy is within $\epsilon$ of the optimal policy.

**Implementation Variants**

Several variants of value iteration enhance performance for specific applications:

1. **Gauss-Seidel Value Iteration**: Updates values in-place, using the most recent values immediately:

$$
\begin{aligned}
&\text{For each } s \in \mathcal{S}: \\
&\quad V(s) \leftarrow \max_a \sum_{s',r} p(s',r|s,a)[r + \gamma V(s')]
\end{aligned}
$$

2. **Asynchronous Value Iteration**: Updates states in an arbitrary order, potentially focusing on important states:

$$
\begin{aligned}
&\text{Select a state } s \\
&V(s) \leftarrow \max_a \sum_{s',r} p(s',r|s,a)[r + \gamma V(s')]
\end{aligned}
$$

3. **Prioritized Sweeping**: Updates states with the largest potential Bellman errors first:

$$
\begin{aligned}
&\text{Maintain a priority queue based on } |V_{new}(s) - V_{old}(s)| \\
&\text{Update highest-priority state}
\end{aligned}
$$

4. **Real-Time Dynamic Programming (RTDP)**: Focuses updates on states likely to be visited when following the current
   greedy policy:

$$
\begin{aligned}
&\text{Generate sample trajectories using the current greedy policy} \\
&\text{Update states encountered in these trajectories}
\end{aligned}
$$

Value iteration stands as one of the foundational algorithms in reinforcement learning, providing an elegant and
efficient approach to solving MDPs when the model is known. By directly computing the optimal value function through
repeated application of the Bellman optimality operator, it circumvents the need for separate policy evaluation and
improvement steps, offering a streamlined solution method with strong theoretical guarantees.

##### Limitations of Model-Based Methods

While dynamic programming approaches like policy iteration and value iteration provide elegant and theoretically sound
solutions to reinforcement learning problems, they come with significant limitations that restrict their practical
applicability. Understanding these limitations is crucial for selecting appropriate algorithms for real-world problems
and appreciating the motivations behind model-free reinforcement learning methods.

**Complete Model Requirement**

The most fundamental limitation of model-based methods is their requirement for a complete and accurate model of the
environment. Specifically, they need:

- **Transition Probabilities**: $p(s'|s,a)$ for all state-action pairs, representing the probability of transitioning to
  state $s'$ after taking action $a$ in state $s$.
- **Reward Function**: $r(s,a,s')$ for all state-action-next-state triplets, representing the expected reward received
  when transitioning from $s$ to $s'$ via action $a$.

In practical applications, obtaining this complete model presents several challenges:

- **Acquisition Difficulty**: For many real-world problems, accurately specifying transition probabilities is extremely
  difficult or impossible.
- **Domain Expertise Requirement**: Creating accurate models typically requires significant domain expertise and careful
  system identification.
- **Model Error Sensitivity**: Even small errors in the model can lead to significantly suboptimal policies when using
  dynamic programming.

**Example: Robot Navigation** Consider a robot navigating through an office building. A complete model would require:

- Probabilities of successful movement for each action in each location
- Probabilities of different sensor readings in each state
- Expected battery consumption for each movement
- Interaction effects with dynamic elements (e.g., people, doors)

Accurately specifying all these probabilities is prohibitively difficult, making pure model-based approaches
impractical.

**Computational Complexity Challenges**

Dynamic programming methods face severe computational challenges as problem size increases:

- **Curse of Dimensionality**: The computational complexity grows exponentially with the number of state variables.
    - **State Space Explosion**: For a problem with $n$ binary state variables, there are $2^n$ possible states.
    - **Tabular Representation Limitations**: Each sweep through the state space requires updates for all states, which
      becomes infeasible as the state space grows.
- **Memory Requirements**: Storing value functions for large state spaces demands substantial memory.
    - A 2D grid world of size 100×100 has 10,000 states, manageable for basic tabular methods
    - Adding just a few dimensions (e.g., orientation, velocity, object positions) can expand this to millions of states
    - Real-world problems often have continuous state spaces with infinite states
- **Update Complexity**: Each dynamic programming update requires evaluating all possible next states and their
  probabilities.
    - For states with many possible transitions, this becomes computationally intensive
    - The branching factor of the MDP directly impacts computation time

**Example: Autonomous Driving** For autonomous driving, relevant state variables include:

- Position, orientation, and velocity of the vehicle (continuous)
- Positions and velocities of other vehicles (numerous, continuous)
- Road conditions, traffic signals, and lane markings
- Weather conditions affecting visibility and traction

The resulting state space is astronomical, making exact dynamic programming methods computationally intractable.

**Adaptation to Changing Environments**

Model-based methods struggle with dynamic environments:

- **Model Staleness**: When environment dynamics change, the entire model must be updated, which can be:
    - Computationally expensive
    - Difficult to detect without external validation
    - Potentially requiring extensive retraining or recalibration
- **Recomputation Cost**: After model updates, value functions typically need to be recomputed from scratch or
  significantly revised.
- **Adaptation Lag**: The time required to update the model and recompute optimal policies can cause performance
  degradation during adaptation periods.

**Handling Model Uncertainty**

Traditional dynamic programming approaches assume the model is correct and do not naturally handle uncertainty:

- **No Exploration Mechanisms**: DP assumes the model is correct and does not actively explore to improve it.
- **Sensitivity to Model Errors**: Small errors in transition or reward models can lead to significantly suboptimal
  policies.
- **Deterministic Focus**: Classic DP methods are designed for deterministic or stochastic environments with known
  probability distributions, not for environments with unknown or uncertain dynamics.

**Alternative Approaches**

To address these limitations, several alternative approaches have been developed:

- **Model-Free Methods**: Methods like Q-learning and SARSA learn directly from experience without requiring a model:
    - Eliminate the need for prior knowledge of transition probabilities and rewards
    - Learn through direct interaction with the environment
    - Scale more gracefully to certain large problems
    - Examples include Monte Carlo methods and temporal difference learning
- **Approximate Dynamic Programming**: Uses function approximation to handle large state spaces:
    - Represents value functions using parametric forms (linear functions, neural networks)
    - Generalizes across similar states
    - Significantly reduces memory requirements
    - Examples include fitted value iteration and approximate policy iteration
- **Sample-Based Planning**: Methods like Monte Carlo Tree Search use sampling to plan without explicitly representing
  the entire model:
    - Focus computational resources on relevant parts of the state space
    - Scale to larger problems by avoiding exhaustive computation
    - Combine the strengths of model-based planning with sampling efficiency
    - Examples include AlphaGo and POMCP (Partially Observable Monte Carlo Planning)
- **Model Learning**: Reinforcement learning approaches that learn a model from experience:
    - Build approximate models through interaction
    - Update models continuously as more data becomes available
    - Use model-based planning with the learned model
    - Examples include Dyna-Q and model-based reinforcement learning
- **Bayesian Reinforcement Learning**: Explicitly represents uncertainty in the model and balances exploration with
  exploitation:
    - Maintains a probability distribution over possible models
    - Naturally encourages exploration of uncertain regions
    - Makes decisions that account for model uncertainty
    - Examples include Bayes-Adaptive MDPs and Thompson Sampling

Understanding the limitations of pure model-based methods highlights why reinforcement learning has evolved beyond
simple dynamic programming approaches. By developing algorithms that can learn from experience, handle uncertainty, and
scale to large problems, the field has created solutions that can address real-world challenges beyond the reach of
traditional dynamic programming.

#### Monte Carlo Methods

##### Exploring Policy Space

Monte Carlo (MC) methods represent a fundamentally different approach to solving reinforcement learning problems
compared to dynamic programming. Rather than assuming complete knowledge of the environment model, Monte Carlo methods
learn directly from experience through sampling complete episodes and averaging returns. This sampling-based approach
offers a powerful alternative for problems where a complete model is unavailable or impractical to specify.

**The Fundamental Principle of Monte Carlo Methods**

At their core, Monte Carlo methods estimate the value of each state (or state-action pair) by averaging the actual
returns observed after visiting that state in many complete episodes. This approach:

- **Requires no prior model**: No knowledge of transition probabilities or reward functions is needed
- **Learns from complete episodes**: Only applicable to episodic tasks with well-defined termination
- **Uses averaging of actual returns**: Estimates converge to true expectations as sample size increases

**Monte Carlo Methods:**

```mermaid
graph TD
    C["Monte Carlo Methods"] --> H["First-Visit MC"]
    C --> I["Every-Visit MC"]
    C --> J["Exploration Strategies"]

    H --> R["Only first state visit counted"]
    I --> S["All state visits counted"]
    J --> T["ε-greedy, softmax, optimistic initialization"]

    style C fill:#90CAF9,stroke:#333,stroke-width:2px
    style H fill:#F3E5F5,stroke:#333,stroke-width:2px
    style I fill:#E1BEE7,stroke:#333,stroke-width:2px
    style J fill:#CE93D8,stroke:#333,stroke-width:2px
    style R fill:#BA68C8,stroke:#333,stroke-width:1px
    style S fill:#AB47BC,stroke:#333,stroke-width:1px
    style T fill:#9C27B0,stroke:#333,stroke-width:1px
```

The fundamental Monte Carlo update for state values is:

$$
V(s) \leftarrow V(s) + \alpha[G - V(s)]
$$

Where:

- $G$ is the actual return observed from state $s$ to the end of the episode
- $\alpha$ is a step-size parameter (learning rate)

**Exploring Starts: A Theoretical Approach**

One theoretical approach to ensuring adequate exploration is the "exploring starts" technique:

- Initialize episodes from randomly selected state-action pairs
- Ensure every state-action pair has a non-zero probability of being selected as a starting point
- Follow the current policy thereafter

This approach guarantees that all state-action pairs are eventually visited, ensuring the exploration necessary for
convergence to optimal policies.

**Example**: In a card game like blackjack, exploring starts would mean beginning episodes with different player hands
and dealer up-cards, rather than always starting with cards dealt from a shuffled deck.

While theoretically sound, exploring starts is often impractical in real environments for several reasons:

- Cannot arbitrarily initialize the environment to any state-action pair
- Real systems may have physical constraints on initialization
- Some states may only be reachable through specific sequences of actions

**Stochastic Policies for Practical Exploration**

A more practical approach to exploration involves using stochastic policies that naturally visit different state-action
pairs:

- **ε-greedy Policies**: Mix exploitation with random exploration:

$$
\pi(a|s) = \begin{cases} 1 - \varepsilon + \frac{\varepsilon}{|\mathcal{A}(s)|}, & \text{if } a = \arg\max_{a'} Q(s,a') \ \frac{\varepsilon}{|\mathcal{A}(s)|}, & \text{otherwise} \end{cases}
$$

With probability $1-\varepsilon$, select the action with highest estimated value, and with probability $\varepsilon$,
select a random action.

- **Softmax Policies**: Select actions with probabilities proportional to their estimated values:

$$
\pi(a|s) = \frac{e^{Q(s,a)/\tau}}{\sum_{a'} e^{Q(s,a')/\tau}}
$$

The temperature parameter $\tau$ controls exploration:

- Higher $\tau$: More uniform action selection (more exploration)
- Lower $\tau$: More deterministic selection of high-value actions (more exploitation)

**On-Policy vs. Off-Policy Exploration**

Monte Carlo methods can explore policy space using two distinct approaches:

- **On-Policy Methods**: Learn the value of the policy being used for action selection
    - The same policy is used for both behavior (action selection) and learning
    - Policy gradually improves through small, incremental changes
    - More stable but potentially slower to converge
    - Example: Monte Carlo with ε-greedy policy improvement
- **Off-Policy Methods**: Learn the value of a target policy different from the behavior policy
    - Behavior policy (more exploratory) generates actions
    - Target policy (often more deterministic) is being evaluated and improved
    - More efficient use of exploratory actions but more complex to implement
    - Requires importance sampling to correct for the distribution mismatch
    - Example: Off-policy Monte Carlo with weighted importance sampling

**Greedy in the Limit with Infinite Exploration (GLIE)**

For Monte Carlo methods to converge to optimal policies, exploration strategies need to satisfy GLIE conditions:

- All state-action pairs are visited infinitely often in the limit
- The policy converges to a greedy policy in the limit

A common approach is to use an ε-greedy policy where ε decreases with time:

$$
\varepsilon_i = \frac{1}{i}
$$

Where $i$ is the episode number. This ensures:

- Extensive exploration in early episodes when knowledge is limited
- Gradual transition to exploitation as knowledge improves
- Convergence to the optimal deterministic policy in the limit

<div align="center"> <img src="images/Pseudocode.png" width="600" height="auto"> <p style="color: #555;">Figure: First-Visit GLIE Monte Carlo Control algorithm</p> </div>

**Strategic Exploration Techniques**

Beyond basic random exploration, more sophisticated strategies can enhance exploration efficiency:

- **Optimistic Initialization**: Initialize value estimates optimistically (higher than likely true values)

    - Encourages exploration of less-visited states and actions
    - Actions must be tried several times before their estimates fall to realistic levels
    - Automatically balances exploration and exploitation
    - Example: Initialize all Q-values to +10 in an environment with typical rewards of -1 to +1

- **Count-Based Exploration**: Provide intrinsic rewards for visiting less-frequent states

$$
R_{\text{intrinsic}}(s,a) = \frac{\beta}{\sqrt{N(s,a)}}
$$

Where $N(s,a)$ is the count of visits to state-action pair $(s,a)$ and $\beta$ is a scaling parameter.

- **Uncertainty-Based Exploration**: Direct exploration toward uncertain value estimates

$$
a = \arg\max_a \left(Q(s,a) + c\sqrt{\frac{\ln t}{N(s,a)}}\right)
$$

Where $N(s,a)$ is the count of visits to state-action pair $(s,a)$ and $c$ is a constant that controls the weight given
to exploration.

- **Curiosity-Driven Exploration**: Generate intrinsic rewards based on prediction errors

    - Learn a model to predict state transitions
    - Reward visits to states where the model makes large prediction errors
    - Encourage exploration of poorly understood regions of the environment

**Example: Policy Space Exploration in Grid World**

Consider a simple grid world navigation task:

- **Early exploration (high ε)**:
    - Agent tries many different paths, including suboptimal ones
    - Builds rough estimates of state values throughout the grid
    - Discovers multiple potential routes to the goal
- **Middle episodes (moderate ε)**:
    - More frequently selects actions along promising paths
    - Refines value estimates for commonly visited states
    - Begins to prefer shorter paths to the goal
- **Late episodes (low ε)**:
    - Exploits accumulated knowledge, following optimal path most of the time
    - Occasionally explores to verify no better paths exist
    - Value estimates converge to their true values

Monte Carlo methods excel at exploring policy space without requiring prior knowledge of the environment. By sampling
complete trajectories and averaging returns, they provide a model-free approach to reinforcement learning that is
particularly effective for episodic tasks with manageable state spaces. The stochastic nature of these methods naturally
balances exploration and exploitation, making them robust in many practical applications where a model is unavailable or
difficult to specify.

##### First-Visit and Every-Visit MC

Monte Carlo methods estimate value functions by averaging returns from actual or simulated episodes. Two principal
variants exist for implementing this approach: first-visit MC and every-visit MC. These methods differ in how they
handle multiple occurrences of a state or state-action pair within a single episode, leading to different statistical
properties and practical considerations.

**First-Visit Monte Carlo**

In first-visit MC, only the first occurrence of a state (or state-action pair) in an episode is considered when
computing average returns. This approach follows these steps:

**Algorithm: First-Visit MC Prediction (State-Value Estimation)**

$$
\begin{aligned}
&\textbf{Initialize:} \\
&\quad V(s) \text{ arbitrarily for all states} \\
&\quad \text{Returns}(s) \text{ as an empty list for each state} \\
\\
&\textbf{For each episode:} \\
&\quad \text{Generate an episode: } S_0, A_0, R_1, S_1, \ldots, S_{T-1}, A_{T-1}, R_T, S_T \\
&\quad G \leftarrow 0 \\
&\quad \textbf{For } t = T-1, T-2, \ldots, 0: \\
&\quad \quad G \leftarrow \gamma G + R_{t+1} \text{ (accumulate return by working backward)} \\
&\quad \quad \textbf{If } S_t \text{ appears for the first time at time step } t: \\
&\quad \quad \quad \text{Append } G \text{ to Returns}(S_t) \\
&\quad \quad \quad V(S_t) \leftarrow \text{average}(\text{Returns}(S_t))
\end{aligned}
$$

**Alternative compact format using array environment:**

$$
\begin{array}{l}
\textbf{Algorithm: First-Visit MC Prediction} \\
\hline \\
\textbf{Initialize:} \\
\quad V(s) \text{ arbitrarily for all states} \\
\quad \text{Returns}(s) \text{ as empty list for each state} \\
\\
\textbf{Repeat for each episode:} \\
\quad \text{1. Generate episode: } S_0, A_0, R_1, \ldots, S_T \\
\quad \text{2. } G \leftarrow 0 \\
\quad \text{3. For } t = T-1 \text{ down to } 0: \\
\quad \quad G \leftarrow \gamma G + R_{t+1} \\
\quad \quad \textbf{if } S_t \text{ first occurrence then:} \\
\quad \quad \quad \text{Returns}(S_t).\text{append}(G) \\
\quad \quad \quad V(S_t) \leftarrow \text{mean}(\text{Returns}(S_t))
\end{array}
$$

This approach ensures that returns for each state are statistically independent, which simplifies theoretical analysis
and provides unbiased estimates of the true value function.

**Example**: In a maze navigation task, if the agent visits a particular junction three times during an episode,
first-visit MC would only consider the return following the first visit when updating that junction's value.

**Every-Visit Monte Carlo**

In every-visit MC, every occurrence of a state (or state-action pair) in an episode is considered when computing average
returns:

**Algorithm: Every-Visit MC Prediction (State-Value Estimation)**

$$
\begin{aligned}
&\textbf{Initialize:} \\
&\quad V(s) \text{ arbitrarily for all states} \\
&\quad \text{Returns}(s) \text{ as an empty list for each state} \\
\\
&\textbf{For each episode:} \\
&\quad \text{Generate an episode: } S_0, A_0, R_1, S_1, \ldots, S_{T-1}, A_{T-1}, R_T, S_T \\
&\quad G \leftarrow 0 \\
&\quad \textbf{For } t = T-1, T-2, \ldots, 0: \\
&\quad \quad G \leftarrow \gamma G + R_{t+1} \text{ (accumulate return by working backward)} \\
&\quad \quad \text{Append } G \text{ to Returns}(S_t) \\
&\quad \quad V(S_t) \leftarrow \text{average}(\text{Returns}(S_t))
\end{aligned}
$$

The key difference from First-Visit MC is that this algorithm appends the return $G$ to Returns$(S_t)$ for every visit
to state $S_t$ within an episode, rather than only the first occurrence. This means if a state is revisited multiple
times in the same episode, each visit contributes a separate return sample to the value estimate.

Every-visit MC uses more samples per episode, potentially accelerating learning in the early stages, but introduces
statistical dependencies between samples.

**Example**: In the same maze navigation task, every-visit MC would update the junction's value three times in a single
episode, once for each visit, using the return following each respective visit.

**Comparative Analysis**

**Theoretical Properties:**

**1. Convergence**: Both methods converge to the true value function as the number of episodes approaches infinity, but
with different properties:

- First-visit MC produces unbiased estimates from the beginning
- Every-visit MC initially produces biased estimates that become asymptotically unbiased with sufficient samples

For first-visit MC, each return sample is an independent estimate of the expected return, leading directly to unbiased
estimation. For every-visit MC, returns following different visits to the same state within an episode are correlated,
initially creating bias.

**2. Statistical Independence**: Samples in first-visit MC are more likely to be independent, which simplifies
theoretical analysis and can lead to more stable learning.

**3. Sample Efficiency**: Every-visit MC generally collects more samples per episode, which can be advantageous when
experiences are costly or limited.

**Practical Considerations:**

- **Early Learning Speed**: Every-visit MC often shows faster initial learning due to using more samples per episode,
  particularly in environments where certain states are visited frequently.
- **Computation**: Every-visit MC requires more updates per episode, increasing computational overhead.
- **Memory Requirements**: Both methods traditionally require storing returns for each state or state-action pair,
  though incremental implementations can mitigate this.
- **Long-Term Performance**: First-visit MC often performs better in the long run due to using more statistically
  independent samples.
- **State Revisitation**: The relative performance depends significantly on how often states are revisited within
  episodes:
    - In environments with few state revisits, the methods perform similarly
    - In environments with many revisits, the differences become more pronounced

**Extending to Action-Value Estimation**

Both methods can be extended to estimate action values ($Q(s,a)$) rather than state values:

**For First-Visit MC:**

- Track the first occurrence of each state-action pair in an episode
- Update $Q(s,a)$ based on the return following that first occurrence

**For Every-Visit MC:**

- Update $Q(s,a)$ based on the return following every occurrence of the state-action pair

This extension enables direct policy improvement by selecting actions that maximize the estimated Q-values.

**Incremental Implementation**

For efficiency, both methods can be implemented incrementally to avoid storing all returns:

$$
V(S_t) \leftarrow V(S_t) + \frac{1}{N(S_t)}[G_t - V(S_t)]
$$

Where $N(S_t)$ is the count of visits (first visits for first-visit MC, all visits for every-visit MC) to state $S_t$.

This can be further simplified with a constant step-size parameter:

$$
V(S_t) \leftarrow V(S_t) + \alpha[G_t - V(S_t)]
$$

Where $\alpha \in (0,1]$ controls the weight given to recent returns versus older estimates.

**Example: Blackjack Value Estimation**

In the game of blackjack, consider estimating the value of standing with a player sum of 15 against a dealer showing a
10:

- **First-visit MC**: Only the first occurrence in each episode would contribute to the value estimate
- **Every-visit MC**: If the player encounters this situation multiple times in an episode (e.g., after splitting
  pairs), each occurrence would contribute

First-visit and every-visit MC methods offer different approaches to estimating value functions from experience. The
choice between them depends on the specific application, with first-visit MC providing unbiased estimates with stronger
theoretical guarantees, and every-visit MC potentially offering faster initial learning through more efficient use of
experience data.

##### MC Prediction and Control

Monte Carlo (MC) methods address two fundamental problems in reinforcement learning: prediction (estimating the value
function for a given policy) and control (finding the optimal policy). These methods learn directly from complete
episodes of experience without requiring a model of the environment, making them particularly valuable for complex
environments where accurate models are difficult to obtain.

**Monte Carlo Prediction (Policy Evaluation)**

Monte Carlo prediction estimates the value function of a given policy by averaging returns observed from real or
simulated episodes. The fundamental process consists of:

- **Episode Generation**: Generate episodes by following the policy $\pi$ being evaluated.
- **Return Calculation**: For each state $s$ visited in an episode, calculate the return $G_t$ (the sum of discounted
  rewards) from that time step onward.
- **Value Estimation**: Estimate the value function $V_\pi(s)$ by averaging the returns observed after visiting state
  $s$.

**Implementation Approaches**:

- **Batch Averaging**:

$$
V(s) = \frac{\sum_{i=1}^{N(s)} G_i(s)}{N(s)}
$$

Where $N(s)$ is the number of visits to state $s$, and $G_i(s)$ is the return following the $i$-th visit.

- **Incremental Updating**:

$$
V(s) \leftarrow V(s) + \alpha[G_t - V(s)]
$$

Where $\alpha$ is a step-size parameter (learning rate) that controls how quickly new information replaces old
estimates.

The same approach extends to action-value estimation by calculating $Q_\pi(s,a)$ as the average of returns following
state-action pairs rather than states alone.

**Example: Blackjack Policy Evaluation**

Consider evaluating a policy for the game of blackjack:

1. Play thousands of blackjack hands following a fixed policy (e.g., "hit on 16 or less, stand on 17 or more")
2. Record the player's sum, dealer's showing card, and whether the player has a usable ace in each state
3. Track the return (win: +1, lose: -1, draw: 0) from each state
4. Average these returns to estimate state values under the policy

The resulting value function would indicate the player's expected return from each state when following the given
policy.

**Monte Carlo Control**

Monte Carlo control aims to find the optimal policy through iterative policy evaluation and improvement. The general
approach follows:

- **Policy Evaluation**: Estimate the action-value function $Q_\pi$ for the current policy $\pi$ using MC prediction.
- **Policy Improvement**: Improve the policy based on the estimated action-value function, typically by making it greedy
  with respect to $Q_\pi$: $$\pi'(s) = \arg\max_a Q_\pi(s,a)$$
- **Iteration**: Alternate between evaluation and improvement until convergence.

Monte Carlo control comes in two main variants: on-policy and off-policy methods.

**On-Policy Monte Carlo Control**

On-policy methods evaluate and improve the same policy that is used to generate episodes. To ensure adequate
exploration, these methods typically use ε-greedy policies:

<div align="center"> <img src="images/Pseudocode.png" width="600" height="auto"> <p style="color: #555;">Figure: First-Visit GLIE Monte Carlo Control algorithm</p> </div>

The algorithm proceeds as follows:

**Algorithm: First-Visit Monte Carlo Control**

$$
\begin{aligned}
&\textbf{Initialize:} \\
&\quad Q(s,a) \text{ arbitrarily for all state-action pairs} \\
&\quad \pi \leftarrow \varepsilon\text{-greedy policy with respect to } Q \\
\\
&\textbf{Repeat for each episode:} \\
&\quad \text{Generate an episode using } \pi \\
&\quad \textbf{For each } (s,a) \text{ in the episode:} \\
&\quad \quad \text{Calculate return } G \text{ following first occurrence of } (s,a) \\
&\quad \quad \text{Update: } Q(s,a) \leftarrow Q(s,a) + \alpha[G - Q(s,a)] \\
&\quad \text{Improve policy: } \pi \leftarrow \varepsilon\text{-greedy with respect to } Q
\end{aligned}
$$

**Alternative compact format:**

$$
\begin{array}{l}
\textbf{Algorithm: First-Visit MC Control} \\
\hline \\
\textbf{Initialize:} Q(s,a) \text{ arbitrarily, } \pi \text{ as } \varepsilon\text{-greedy} \\
\\
\textbf{Repeat for each episode:} \\
\quad \text{1. Generate episode using } \pi \\
\quad \text{2. For each first occurrence of } (s,a): \\
\quad \quad \text{a. Calculate return } G \\
\quad \quad \text{b. } Q(s,a) \leftarrow Q(s,a) + \alpha[G - Q(s,a)] \\
\quad \text{3. Update } \pi \text{ to be } \varepsilon\text{-greedy w.r.t. } Q
\end{array}
$$

This algorithm combines policy evaluation (updating Q-values from returns) with policy improvement (making the policy
greedy with respect to updated Q-values) in each episode.

For convergence to the optimal policy, ε-greedy policies typically follow a GLIE (Greedy in the Limit with Infinite
Exploration) schedule, where ε decreases over time:

$$
\varepsilon_i = \frac{1}{i}
$$

Where $i$ is the episode number.

**Off-Policy Monte Carlo Control**

Off-policy methods evaluate and improve a target policy different from the behavior policy used to generate episodes:

1. Use a behavior policy $b$ (e.g., ε-greedy with fixed ε) that ensures exploration
2. Learn about a target policy $\pi$ (typically the greedy policy with respect to current $Q$ values)
3. Correct for the distribution mismatch using importance sampling

The importance sampling ratio for a trajectory is:

$$
\rho_{t:T-1} = \prod_{k=t}^{T-1} \frac{\pi(A_k|S_k)}{b(A_k|S_k)}
$$

This ratio adjusts the update to account for the difference in action probabilities under the two policies. The
off-policy update becomes:

$$
Q(S_t,A_t) \leftarrow Q(S_t,A_t) + \alpha[\rho_{t:T-1}G_t - Q(S_t,A_t)]
$$

**Weighted Importance Sampling**

A common variant uses weighted importance sampling to reduce variance:

$$
Q(S_t,A_t) \leftarrow \frac{\sum_{i=1}^{N(S_t,A_t)} \rho_i G_i}{\sum_{i=1}^{N(S_t,A_t)} \rho_i}
$$

Where $\rho_i$ is the importance sampling ratio for the $i$-th visit to $(S_t,A_t)$.

**Incremental Implementation**

In practice, both on-policy and off-policy methods often use incremental updates:

**Running Average**:

$$
Q(s,a) \leftarrow Q(s,a) + \frac{1}{N(s,a)}[G - Q(s,a)]
$$

**Constant-α**:

$$
Q(s,a) \leftarrow Q(s,a) + \alpha[G - Q(s,a)]
$$

Where $\alpha \in (0,1]$ is fixed.

The constant-α approach adapts more quickly to non-stationary environments and gives more weight to recent experiences.

**Example: Learning to Play Blackjack**

Consider learning an optimal blackjack strategy:

1. Initialize action values arbitrarily for all states (e.g., player sums, dealer showing cards, usable aces)
2. For each episode:
    - Play a hand of blackjack using the current ε-greedy policy
    - For each state-action pair encountered:
        - Update its Q-value based on the final outcome of the hand
    - Update the policy to be ε-greedy with respect to the new Q-values
3. After sufficient episodes, the policy converges to near-optimal play

**Advantages of Monte Carlo Methods**

- **Model-Free Learning**: Learn from actual experience without requiring a model of the environment
- **Low Bias**: Learn from complete returns without bootstrapping from estimated values
- **Focus on Visited States**: Computational effort concentrates on states actually encountered
- **Handling Non-Markovian Environments**: Effectiveness doesn't depend on the Markov property
- **Simple Implementation**: Conceptually straightforward and easy to implement

**Limitations**

- **Episodic Tasks Only**: Require complete episodes, not suitable for continuing tasks
- **High Variance**: Return estimates have high variance, especially with long episodes
- **Delayed Learning**: Must wait for episode completion before updating values
- **Sample Inefficiency**: Don't use information from state transitions within episodes

Monte Carlo prediction and control offer powerful model-free approaches to reinforcement learning that learn directly
from experience. By averaging returns from complete episodes, these methods provide effective tools for solving episodic
tasks without requiring knowledge of environment dynamics, making them particularly valuable for complex real-world
problems where accurate models are unavailable.

##### Exploration vs Exploitation

The exploration-exploitation dilemma represents one of the fundamental challenges in reinforcement learning. It captures
the tension between gathering new information about the environment (exploration) and maximizing reward based on current
knowledge (exploitation). Balancing these competing objectives is critical for effective learning and optimal
performance.

**Conceptualizing the Dilemma**

At each decision point, the agent faces a choice:

**1. Exploitation**: Select actions that are currently believed to yield the highest rewards based on existing
knowledge.

- **Advantages**: Maximizes immediate expected reward; utilizes learned information
- **Disadvantages**: May miss better strategies due to incomplete information; can get stuck in local optima

**2. Exploration**: Select actions that might not be optimal according to current estimates but provide valuable
information.

- **Advantages**: Discovers potentially better policies; avoids premature convergence to suboptimal solutions
- **Disadvantages**: Sacrifices immediate reward; may waste resources on irrelevant actions

**The Importance of Balance**

Finding the appropriate balance between exploration and exploitation is critical:

- **Too little exploration**: The agent may converge prematurely to a suboptimal policy, never discovering better
  alternatives.

**Example**: In a maze with multiple paths to the goal, an agent that exploits too early might always follow the first
decent path it finds, never discovering a significantly shorter route.

- **Too much exploration**: The agent may waste resources testing suboptimal actions repeatedly, failing to capitalize
  on what it has already learned.

**Example**: An agent that continues random exploration despite having found an excellent policy will perform poorly due
to frequently selecting inferior actions.

- **Optimal balance**: Depends on factors like:

    - Problem complexity and structure
    - Available time horizon (total number of episodes or steps)
    - Prior knowledge about the environment
    - Cost of exploration (how much reward is sacrificed)

**Exploration Strategies in Monte Carlo Methods**

Monte Carlo methods employ several strategies to address the exploration-exploitation dilemma:

**1. ε-greedy Policies**: The most common approach due to its simplicity and effectiveness.

With probability 1-ε, select the greedy action (exploitation):

$$
a = \arg\max_a Q(s,a)
$$

With probability ε, select a random action (exploration):

$$
a \sim \text{Uniform}(\mathcal{A}(s))
$$

Formally, the action selection probability is:

$$
\pi(a|s) = \begin{cases} 1 - \varepsilon + \frac{\varepsilon}{|\mathcal{A}(s)|}, & \text{if } a = \arg\max_{a'} Q(s,a') \\
\frac{\varepsilon}{|\mathcal{A}(s)|}, & \text{otherwise} \end{cases}
$$

**2. Decaying ε-greedy**: Reduce the exploration rate over time to transition from exploration to exploitation.

$$
\varepsilon_t = \frac{\varepsilon_0}{t} \text{ or } \varepsilon_t = \varepsilon_0 \cdot \gamma^t
$$

Where $t$ is the episode number or time step, $\varepsilon_0$ is the initial exploration rate, and $\gamma$ is a decay
factor.

This approach allows for extensive exploration early in learning when uncertainty is high, gradually shifting toward
exploitation as knowledge improves.

**3. Softmax Policies**: Select actions with probabilities proportional to their estimated values.

$$
\pi(a|s) = \frac{e^{Q(s,a)/\tau}}{\sum_{a'} e^{Q(s,a')/\tau}}
$$

The temperature parameter $\tau$ controls the degree of exploration:

- High temperature: Actions selected with nearly equal probability (more exploration)
- Low temperature: Higher-valued actions selected with much higher probability (more exploitation)
- As $\tau \rightarrow 0$, softmax approaches greedy selection

Unlike ε-greedy, softmax policies explore intelligently by favoring more promising actions over clearly inferior ones.

**4. Optimistic Initialization**: Initialize action-value estimates optimistically to encourage early exploration.

- Set initial Q-values higher than the likely true values
- Actions must be tried multiple times before their estimates decrease to realistic levels
- Naturally balances exploration and exploitation without explicit random action selection

This approach frontloads exploration, as the agent is initially optimistic about untried actions.

**5. **Upper Confidence Bound (UCB)\*\*: Select actions based on both their estimated value and their uncertainty.

$$
a_t = \arg\max_a \left[ Q(s_t,a) + c\sqrt{\frac{\ln t}{N(s_t,a)}} \right]
$$

Where:

- $N(s_t,a)$ is the number of times action $a$ has been selected in state $s_t$
- $c$ controls the weight given to exploration
- $\ln t$ represents increasing confidence as more total actions are taken

The UCB algorithm systematically balances exploration and exploitation by adding an exploration bonus that decreases
with the number of times an action has been tried.

**Greedy in the Limit with Infinite Exploration (GLIE)**

For Monte Carlo methods to converge to optimal policies, exploration strategies should satisfy the GLIE conditions:

- All state-action pairs must be visited infinitely often as the number of episodes approaches infinity.
- The policy must converge to a greedy policy with respect to the current action-value function in the limit.

A common approach to satisfying these conditions is using an ε-greedy policy where ε decreases according to:

$$
\varepsilon_i = \frac{1}{i}
$$

Where $i$ is the episode number. This ensures:

- Sufficient early exploration when uncertainty is high
- Gradual reduction in exploration as knowledge improves
- Convergence to the optimal deterministic policy in the limit

**Practical Implementations vs. Theoretical Guarantees**

While theory suggests decreasing ε to zero to guarantee convergence to optimal policies, practical implementations often
diverge from this approach:

- **Practical ε Schedules**:
    - Start with a high ε (e.g., 1.0 or 0.5) to encourage thorough initial exploration
    - Decrease ε gradually to a small positive value (e.g., 0.1 or 0.01) rather than to zero
    - Maintain this minimal exploration to allow adaptation to potential changes in the environment
- **Adaptive Exploration**:
    - Increase exploration in states with high uncertainty or inconsistent rewards
    - Decrease exploration in well-understood regions of the state space
    - Adjust exploration based on observed learning progress

**Example: Exploration Impact in Grid World Navigation**

Consider a Monte Carlo agent learning to navigate a grid world with multiple paths to a goal:

- **Initial exploration (high ε = 0.8)**:
    - Agent tries many different paths, including clearly suboptimal ones
    - Discovers multiple routes to the goal, including a rare shortcut
    - Value estimates are rough but cover much of the state space
- **Mid-learning (moderate ε = 0.3)**:
    - Agent follows promising paths more frequently
    - Refines value estimates for common states
    - Exploration occasionally leads to trying alternative routes
- **Late learning (low ε = 0.05)**:
    - Agent exploits the discovered shortcut most of the time
    - Rare exploration verifies no better paths exist
    - Performance stabilizes near optimal
- **Fixed low ε**:
    - Maintains minimal exploration indefinitely
    - Allows adaptation if the environment changes (e.g., if the shortcut is blocked)
    - Sacrifices a small amount of optimal performance for robustness

**Balance in Different Reinforcement Learning Methods**

Different RL methods handle the exploration-exploitation dilemma in characteristic ways:

- **Monte Carlo**: Explicitly implements exploration through policy (e.g., ε-greedy, softmax) and typically requires
  GLIE conditions for convergence.
- **Temporal Difference**: Similar to Monte Carlo but can adapt more quickly due to bootstrapping, potentially requiring
  less exploration.
- **Off-Policy Methods**: Can learn about optimal policy while following exploratory behavior policy, separating
  exploration from target policy optimization.

The exploration-exploitation dilemma represents a fundamental trade-off in reinforcement learning. Monte Carlo methods
handle this trade-off through various exploration strategies, with ε-greedy policies being the most common due to their
simplicity and effectiveness. The right balance depends on the specific problem, with the goal of satisfying GLIE
conditions to ensure convergence to optimal policies while maintaining practical performance during learning.

#### Temporal Difference Learning

##### TD(0) and Bootstrapping

Temporal Difference (TD) learning represents a fundamental breakthrough in reinforcement learning, creating a bridge
between Monte Carlo methods and dynamic programming. TD(0), the simplest TD algorithm, introduces the powerful concept
of bootstrapping—using estimated returns of future states to update the values of current states, rather than waiting
for complete episodes to finish.

**The Core Principle of TD Learning**

TD methods learn directly from experience like Monte Carlo methods, but instead of waiting for the end of an episode to
determine the actual return, TD methods update estimates based on other learned estimates—a process called
bootstrapping. bootstrapping.

**Temporal Difference Learning:**

```mermaid
graph TD
    D["Temporal Difference Learning"] --> K["TD(0)"]
    D --> L["SARSA"]
    D --> M["Q-Learning"]
    D --> N["Expected SARSA"]

    K --> U["V(s) ← V(s) + α[r + γV(s') - V(s)]"]
    L --> V["Q(s,a) ← Q(s,a) + α[r + γQ(s',a') - Q(s,a)]"]
    M --> W["Q(s,a) ← Q(s,a) + α[r + γmax_a' Q(s',a') - Q(s,a)]"]
    N --> X["Q(s,a) ← Q(s,a) + α[r + γΣ π(a'|s')Q(s',a') - Q(s,a)]"]

    style D fill:#64B5F6
    style K fill:#FCE4EC
    style L fill:#F8BBD9
    style M fill:#F48FB1
    style N fill:#F06292
    style U fill:#EC407A
    style V fill:#E91E63
    style W fill:#D81B60
    style X fill:#C2185B
```

The fundamental TD learning principle can be expressed as:

"Learn a guess from a guess"

This principle allows TD methods to:

- Learn online during episodes (not just at their conclusion)
- Update estimates incrementally after each step
- Converge to accurate estimates through repeated refinement

**Algorithm: TD(0) Prediction**

$$
\begin{aligned}
&\textbf{Initialize:} \\
&\quad V(s) \text{ arbitrarily for all states, except } V(\text{terminal}) = 0 \\
\\
&\textbf{For each episode:} \\
&\quad \text{Initialize state } S \\
&\quad \textbf{For each step of the episode:} \\
&\quad \quad \text{Take action } A \text{ according to policy } \pi(\cdot|S) \\
&\quad \quad \text{Observe reward } R \text{ and next state } S' \\
&\quad \quad \text{Update: } V(S) \leftarrow V(S) + \alpha[R + \gamma V(S') - V(S)] \\
&\quad \quad S \leftarrow S' \\
&\quad \textbf{Until } S \text{ is terminal}
\end{aligned}
$$

**Alternative compact format:**

$$
\begin{array}{l}
\textbf{Algorithm: TD(0) Prediction} \\
\hline \\
\textbf{Initialize:} V(s) \text{ arbitrarily, } V(\text{terminal}) = 0 \\
\\
\textbf{Repeat for each episode:} \\
\quad \text{1. Initialize state } S \\
\quad \text{2. Repeat until } S \text{ is terminal:} \\
\quad \quad \text{a. Take action } A \sim \pi(\cdot|S) \\
\quad \quad \text{b. Observe } R, S' \\
\quad \quad \text{c. } V(S) \leftarrow V(S) + \alpha[R + \gamma V(S') - V(S)] \\
\quad \quad \text{d. } S \leftarrow S'
\end{array}
$$

The key insight in TD(0) is the bootstrap update using the current estimate $V(S')$ of the next state's value, rather
than waiting for the complete return as in Monte Carlo methods.

The key update rule, $V(S) \leftarrow V(S) + \alpha[R + \gamma V(S') - V(S)]$, encapsulates the essence of TD learning:

- $R + \gamma V(S')$ is the TD target—an estimate of the expected return
- $R + \gamma V(S') - V(S)$ is the TD error—the difference between the current estimate and the updated estimate
- $\alpha$ is the learning rate that controls how quickly estimates change

**Understanding Bootstrapping**

Bootstrapping refers to updating estimates based on other estimates rather than actual complete returns. This concept is
central to TD learning and distinguishes it from Monte Carlo methods:

- **Monte Carlo Update (No Bootstrapping)**:

$$
V(S_t) \leftarrow V(S_t) + \alpha[G_t - V(S_t)]
$$

Where $G_t$ is the actual return observed from state $S_t$ to the end of the episode. Monte Carlo methods must wait
until the episode ends to perform updates.

- **TD(0) Update (Bootstrapping)**:

$$
V(S_t) \leftarrow V(S_t) + \alpha[R_{t+1} + \gamma V(S_{t+1}) - V(S_t)]
$$

The TD target $R_{t+1} + \gamma V(S_{t+1})$ is based on a single observed reward and the current estimate of the next
state's value. Updates can be performed immediately after observing the next state.

Bootstrapping creates a form of learning where predictions are updated based on other predictions—a recursive process
that propagates information backward through sequences of states, similar to dynamic programming but without requiring a
complete model.

**The TD Error as a Learning Signal**

The TD error $\delta_t = R_{t+1} + \gamma V(S_{t+1}) - V(S_t)$ serves as the fundamental learning signal in TD methods:

- **Positive TD error**: Indicates the current state's value is underestimated; the update increases it
- **Negative TD error**: Indicates the current state's value is overestimated; the update decreases it
- **Zero TD error**: Indicates the current estimate satisfies the Bellman equation; no change needed

**Example**: In a grid world navigation task:

- Agent moves from state A to B, receiving a reward of -1
- If $V(A) = -10$ and $V(B) = -5$ with $\gamma = 0.9$
- TD error = $-1 + 0.9 \times (-5) - (-10) = -1 - 4.5 + 10 = 4.5$
- This positive error indicates $V(A)$ should be increased, as the next state and reward suggest it's better than
  currently estimated

**The Bias-Variance Trade-off**

TD methods occupy a middle ground in the bias-variance trade-off between Monte Carlo and dynamic programming:

**1. Monte Carlo methods**:

- No bias (when using first-visit MC)
- High variance due to randomness in complete episode returns
- Slower convergence due to high variance

**2. Dynamic Programming**:

- Biased when using approximate or incomplete models
- Zero variance when the model is completely known
- Computationally expensive for large state spaces

**3. TD methods**:

- Some bias due to bootstrapping from estimated values
- Lower variance than MC due to using single-step transitions
- Typically faster convergence in practice
- No model required

This trade-off is often advantageous in practice, leading to faster convergence compared to Monte Carlo methods,
especially in long episodes where the variance of returns is high.

**Formal Convergence Properties**

TD(0) converges to the true value function $V_\pi$ under the following conditions:

1. The step sizes $\alpha$ follow a schedule where $\sum_{t=1}^{\infty} \alpha_t = \infty$ and
   $\sum_{t=1}^{\infty} \alpha_t^2 < \infty$
2. All states are visited infinitely often

In practice, a constant step size is often used for non-stationary environments, sacrificing guaranteed convergence for
better tracking of changes.

**TD(0) for Action-Value Estimation**

TD(0) can be extended to estimate action-values ($Q$-values) by using the update rule:

$$
Q(S_t,A_t) \leftarrow Q(S_t,A_t) + \alpha[R_{t+1} + \gamma Q(S_{t+1},A_{t+1}) - Q(S_t,A_t)]
$$

This forms the basis for SARSA, an on-policy TD control algorithm.

**Advantages of TD Learning**

- **Online Learning**: Updates estimates at each step, not just at episode end
- **Handles Continuing Tasks**: Can learn without episode boundaries
- **Lower Variance**: Typically converges faster than Monte Carlo methods
- **Model-Free**: Learns directly from experience without environment model
- **Efficient Use of Experience**: Each transition contributes to learning immediately

**Limitations**

- **Bootstrapping Bias**: Using estimated values introduces bias
- **Function Approximation Challenges**: May diverge with function approximation if not carefully designed
- **Requires Markov Property**: More sensitive to violations of the Markov property than Monte Carlo methods

TD learning, particularly TD(0), represents a breakthrough in reinforcement learning by combining the model-free
learning of Monte Carlo methods with the bootstrapping efficiency of dynamic programming. By learning from immediate
rewards and estimated future values, TD methods offer a powerful approach that has proven effective across a wide range
of reinforcement learning applications.

##### SARSA: On-Policy TD Control

SARSA (State-Action-Reward-State-Action) is an on-policy temporal difference control algorithm that learns action-value
functions and determines optimal policies through direct interaction with the environment. The name SARSA reflects the
quintuple of events $(S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1})$ used in the algorithm's core update rule.

**The SARSA Algorithm**

SARSA extends TD(0) prediction to control by learning action-values and improving policies simultaneously:

<div align="center"> <img src="images/sarsa.png" width="600" height="auto"> <p style="color: #555;">Figure: SARSA algorithm pseudocode</p> </div>

The algorithm proceeds as follows:

**Algorithm: SARSA (On-Policy TD Control)**

$$
\begin{aligned}
&\textbf{Initialize:} \\
&\quad Q(s,a) \text{ arbitrarily for all states } s \text{ and actions } a \\
\\
&\textbf{For each episode:} \\
&\quad \text{Initialize state } S \\
&\quad \text{Choose action } A \text{ from } S \text{ using policy derived from } Q \text{ (e.g., } \varepsilon\text{-greedy)} \\
&\quad \textbf{For each step of the episode:} \\
&\quad \quad \text{Take action } A, \text{ observe reward } R \text{ and next state } S' \\
&\quad \quad \text{Choose next action } A' \text{ from } S' \text{ using policy derived from } Q \\
&\quad \quad \text{Update: } Q(S,A) \leftarrow Q(S,A) + \alpha[R + \gamma Q(S',A') - Q(S,A)] \\
&\quad \quad S \leftarrow S', A \leftarrow A' \\
&\quad \textbf{Until } S \text{ is terminal}
\end{aligned}
$$

**Alternative compact format:**

$$
\begin{array}{l}
\textbf{Algorithm: SARSA} \\
\hline \\
\textbf{Initialize:} Q(s,a) \text{ arbitrarily} \\
\\
\textbf{Repeat for each episode:} \\
\quad \text{1. Initialize } S, \text{ choose } A \sim \pi(\cdot|S) \\
\quad \text{2. Repeat until } S \text{ is terminal:} \\
\quad \quad \text{a. Take action } A, \text{ observe } R, S' \\
\quad \quad \text{b. Choose } A' \sim \pi(\cdot|S') \\
\quad \quad \text{c. } Q(S,A) \leftarrow Q(S,A) + \alpha[R + \gamma Q(S',A') - Q(S,A)] \\
\quad \quad \text{d. } S \leftarrow S', A \leftarrow A'
\end{array}
$$

The key characteristic of SARSA is that it updates using the actual next action $A'$ selected by the policy, making it
an on-policy algorithm that learns about the same policy it uses for action selection.

The core update rule is:

$$
Q(S_t,A_t) \leftarrow Q(S_t,A_t) + \alpha[R_{t+1} + \gamma Q(S_{t+1},A_{t+1}) - Q(S_t,A_t)]
$$

This update bootstraps from the actual next action A' selected by the current policy, making SARSA an on-policy
algorithm that learns about the same policy it uses to make decisions.

**On-Policy Learning Characteristics**

SARSA is classified as an on-policy method because:

- **Policy Consistency**: It learns the value of the same policy that it uses to select actions during learning.
- **Action Selection**: The next action A' used in the update is selected according to the current policy.
- **Policy Improvement**: The policy gradually improves as Q-values improve, typically through ε-greedy selection.

This on-policy nature makes SARSA more conservative than off-policy methods like Q-learning, as it accounts for the
exploratory behavior of the policy in its value estimates.

**Mathematical Interpretation**

SARSA approximates the action-value Bellman equation for the current policy $\pi$:

$$
Q_\pi(s,a) = \mathbb{E}*\pi[R*{t+1} + \gamma Q_\pi(S_{t+1},A_{t+1}) | S_t = s, A_t = a]
$$

The Q-value update using sample transitions serves as a stochastic approximation to this expectation.

**Exploration Strategy**

SARSA typically uses ε-greedy exploration:

1. With probability 1-ε, select the action with highest Q-value (exploitation): $$a = \arg\max_a Q(s,a)$$
2. With probability ε, select a random action (exploration): $$a \sim \text{Uniform}(\mathcal{A}(s))$$

To satisfy the Greedy in the Limit with Infinite Exploration (GLIE) conditions, ε is typically annealed over time:

$$
\varepsilon_i = \frac{1}{i}
$$

Where i is the episode number. This ensures:

- Extensive exploration in early episodes
- Gradual shift toward exploitation as learning progresses
- Convergence to optimal deterministic policy in the limit

**Control Flow and Bootstrapping**

SARSA's control flow involves:

1. **Action Selection**: Choose actions using the current policy (typically ε-greedy)
2. **Experience Collection**: Observe rewards and next states
3. **Bootstrapping**: Update Q-values using the current estimate of next state-action values
4. **Policy Improvement**: Implicit improvement as Q-values become more accurate

Bootstrapping allows SARSA to learn during episodes rather than waiting for completion, making it more efficient than
Monte Carlo methods and applicable to continuing tasks.

**The Cliff Walking Example**

A classic example demonstrating SARSA's behavior is the Cliff Walking problem:

- Grid world with a cliff along one edge
- Agent must navigate from start to goal without falling off the cliff
- Falling off results in large negative reward (-100) and a reset to the start
- Each step incurs small negative reward (-1)

In this environment:

- SARSA learns a safer path further from the cliff edge
- Q-learning (off-policy) learns the optimal but riskier path along the cliff
- This difference occurs because SARSA accounts for the exploration policy in its value estimates
- If the agent occasionally explores randomly (ε > 0), paths near the cliff have higher risk of catastrophic failure

**Example: SARSA Learning in Grid Navigation**

Consider an agent learning to navigate a grid with obstacles:

**1.Initial Exploration**:

- Agent starts with high ε (e.g., 0.3)
- Takes many exploratory actions, building initial Q-value estimates
- Discovers multiple potential paths to the goal

**2. Mid-Learning**:

- As Q-values improve, the agent follows promising paths more frequently
- Exploration rate gradually decreases (e.g., ε = 0.1)
- Q-values for commonly taken actions converge toward their true values

**3. Late Learning**:

- With low exploration (e.g., ε = 0.01), the agent mostly follows its learned policy
- Rare exploratory actions help verify no better alternatives exist
- The policy converges to a near-optimal solution that accounts for exploration risk

**Convergence Properties**

SARSA converges to the optimal policy and action-value function under the following conditions:

1. The state-action space is finite
2. All state-action pairs are visited infinitely often (exploration)
3. The policy converges to greedy in the limit (GLIE conditions)
4. The learning rate α satisfies the Robbins-Monro conditions:
    - $\sum_{t=1}^{\infty} \alpha_t = \infty$
    - $\sum_{t=1}^{\infty} \alpha_t^2 < \infty$

In practice, a small constant learning rate is often used, which sacrifices theoretical convergence guarantees for
better performance in non-stationary environments.

**Practical Implementations and Extensions**

Several variations of SARSA enhance its performance in practice:

- **SARSA with Eligibility Traces (SARSA(λ))**: Extends updates to multiple previous states based on eligibility traces,
  balancing between TD(0) and Monte Carlo methods.
- **Double SARSA**: Maintains two sets of Q-values to reduce maximization bias, similar to Double Q-learning.
- **Expected SARSA**: Uses the expected value of the next state-action pair rather than a sample, reducing variance in
  updates (discussed in a later section).

**Advantages of SARSA**

- **Safety**: More conservative in risky environments due to accounting for exploratory actions
- **Stability**: Less prone to oscillations during learning
- **Direct Experience**: Learns from the actual experience trajectory
- **Simplicity**: Straightforward implementation and intuitive update rule
- **Online Learning**: Updates values during episodes, not just after completion

**Limitations**

- **Efficiency**: May be less sample-efficient than off-policy methods
- **Exploration Dependence**: Value estimates are tied to the specific exploration strategy
- **Policy Constraints**: Improvements in the policy are constrained by the current policy, potentially leading to
  slower convergence to optimal policies

SARSA provides an effective on-policy control algorithm that combines the advantages of TD learning with policy
improvement. By updating based on actual action selections, it offers a conservative approach to reinforcement learning
that works well in many practical applications, especially those with potentially costly exploratory actions or where
safety considerations are important.

##### Q-Learning: Off-Policy TD Control

Q-learning, also known as Sarsamax, represents a landmark off-policy temporal difference control algorithm that directly
learns the optimal action-value function without requiring a model of the environment. Its ability to learn optimal
policies while following exploratory behavior has made it one of the most widely used and influential reinforcement
learning algorithms.

**The Q-Learning Algorithm**

Q-learning follows these steps:

**Algorithm: Q-Learning (Off-Policy TD Control)**

$$
\begin{aligned}
&\textbf{Initialize:} \\
&\quad Q(s,a) \text{ arbitrarily for all states } s \text{ and actions } a \\
\\
&\textbf{For each episode:} \\
&\quad \text{Initialize state } S \\
&\quad \textbf{For each step of the episode:} \\
&\quad \quad \text{Choose action } A \text{ from } S \text{ using policy derived from } Q \text{ (e.g., } \varepsilon\text{-greedy)} \\
&\quad \quad \text{Take action } A, \text{ observe reward } R \text{ and next state } S' \\
&\quad \quad \text{Update: } Q(S,A) \leftarrow Q(S,A) + \alpha[R + \gamma \max_a Q(S',a) - Q(S,A)] \\
&\quad \quad S \leftarrow S' \\
&\quad \textbf{Until } S \text{ is terminal}
\end{aligned}
$$

**Alternative compact format:**

$$
\begin{array}{l}
\textbf{Algorithm: Q-Learning} \\
\hline \\
\textbf{Initialize:} Q(s,a) \text{ arbitrarily} \\
\\
\textbf{Repeat for each episode:} \\
\quad \text{1. Initialize state } S \\
\quad \text{2. Repeat until } S \text{ is terminal:} \\
\quad \quad \text{a. Choose } A \sim \pi(\cdot|S) \text{ (e.g., } \varepsilon\text{-greedy)} \\
\quad \quad \text{b. Take action } A, \text{ observe } R, S' \\
\quad \quad \text{c. } Q(S,A) \leftarrow Q(S,A) + \alpha[R + \gamma \max_a Q(S',a) - Q(S,A)] \\
\quad \quad \text{d. } S \leftarrow S'
\end{array}
$$

The key distinction of Q-learning is the $\max_a Q(S',a)$ term in the update, which makes it off-policy by learning
about the greedy policy while following an exploratory behavior policy.

The core update rule is:

$$
Q(S_t,A_t) \leftarrow Q(S_t,A_t) + \alpha[R_{t+1} + \gamma \max_{a} Q(S_{t+1},a) - Q(S_t,A_t)]
$$

This update uses the maximum Q-value for the next state, regardless of which action might actually be taken next,
creating a fundamental difference from SARSA.

**Off-Policy Learning Characteristics**

Q-learning is classified as an off-policy method because:

- **Policy Separation**: It learns about the optimal (greedy) policy while following a different, exploratory behavior
  policy.
- **Action Selection**: The max operation in the update rule corresponds to the greedy policy, not the actual next
  action taken.
- **Direct Optimization**: It directly approximates the optimal action-value function Q\* without going through a series
  of gradually improving policies.

This off-policy nature allows Q-learning to learn optimal behavior while still exploring, creating a more aggressive
approach compared to SARSA.

**Mathematical Foundation**

Q-learning directly implements a sample-based approximation of the Bellman optimality equation:

$$
Q^_(s,a) = \mathbb{E}[R_{t+1} + \gamma \max_{a'} Q^_(S_{t+1},a') | S_t = s, A_t = a]
$$

The algorithm uses experienced transitions to update its estimates toward this optimal value function. The max operation
represents a key distinction from SARSA, as it considers what would happen under the optimal policy rather than the
exploratory policy actually being followed.

**Exploration Strategies**

Like SARSA, Q-learning typically uses ε-greedy exploration:

- With probability 1-ε, select the action with highest Q-value: $$a = \arg\max_a Q(s,a)$$
- With probability ε, select a random action: $$a \sim \text{Uniform}(\mathcal{A}(s))$$

However, because Q-learning is off-policy, its convergence is less sensitive to the exploration schedule. It will
converge to the optimal policy as long as all state-action pairs are visited infinitely often, regardless of how ε is
annealed.

**Theoretical Convergence Properties**

Q-learning converges to the optimal action-value function Q\* under the following conditions:

- Finite MDP (finite states and actions)
- Sufficient exploration (all state-action pairs visited infinitely often)
- Learning rate α satisfies the Robbins-Monro conditions:
    - $\sum_{t=1}^{\infty} \alpha_t = \infty$ (learning continues indefinitely)
    - $\sum_{t=1}^{\infty} \alpha_t^2 < \infty$ (learning rate decreases sufficiently)

The separation between the behavior policy (used for exploration) and the target policy (greedy policy being learned)
allows Q-learning to learn the optimal policy regardless of how actions are selected during training, as long as
sufficient exploration occurs.

**Q-Learning vs. SARSA: A Comparative Analysis**

The fundamental differences between Q-learning and SARSA lead to distinct behaviors:

- **Update Target**:
    - Q-learning: Uses max Q-value of next state (optimistic)
    - SARSA: Uses Q-value of actually selected next action (realistic)
- **Learning Approach**:
    - Q-learning: Learns the optimal policy regardless of exploration
    - SARSA: Learns a policy that accounts for exploration costs
- **Behavior in Risky Environments**:
    - Q-learning: May learn policies that are optimal but risky during exploration
    - SARSA: Tends to learn safer policies that account for exploratory moves
- **Convergence Characteristics**:
    - Q-learning: Often converges faster to optimal values in simple environments
    - SARSA: May require more careful tuning of exploration schedule but produces safer policies

**Example: Cliff Walking Revisited**

The cliff walking example clearly illustrates the difference between Q-learning and SARSA:

- **Environment**: Grid world with a cliff along one edge between start and goal positions
- **Reward Structure**: -1 per step, -100 for falling off cliff, 0 at goal
- **Q-learning Behavior**: Learns the optimal path along the cliff edge, maximizing expected return under perfect
  execution
- **SARSA Behavior**: Learns a safer path further from the cliff, accounting for occasional random actions during
  exploration

This difference occurs because Q-learning learns the optimal policy assuming perfect execution (no exploration), while
SARSA learns the optimal policy given the exploration strategy being used.

**Double Q-Learning: Addressing Maximization Bias**

A notable extension of Q-learning is Double Q-learning, which addresses the algorithm's tendency to overestimate action
values due to the max operation:

- Maintain two separate Q-functions: $Q_1$ and $Q_2$
- For each update, randomly update either $Q_1$ or $Q_2$
- When updating $Q_1$, use $Q_2$ to determine the best action:

$$
Q_1(S_t,A_t) \leftarrow Q_1(S_t,A_t) + \alpha[R_{t+1} + \gamma Q_2(S_{t+1},\arg\max_a Q_1(S_{t+1},a)) - Q_1(S_t,A_t)]
$$

This decoupling reduces the positive bias in Q-value estimates by using one set of Q-values to select the action and the
other to evaluate it.

**Practical Implementation Considerations**

Several techniques enhance Q-learning's performance in practice:

- **Experience Replay**: Store transitions in a replay buffer and sample randomly for updates, breaking correlations
  between consecutive samples and improving data efficiency.
- **Target Networks**: Use a separate network for generating targets that updates more slowly than the main network,
  improving stability in deep Q-learning.
- **Prioritized Experience Replay**: Sample transitions with higher TD errors more frequently, focusing learning on
  experiences that provide more information.
- **Dueling Networks**: Decompose Q-values into state values and advantage functions, allowing more stable learning of
  state values across actions.

**Extended Example: Q-Learning in a Grid World**

Consider an agent learning to navigate a grid world with obstacles and a goal state:

- Initialization:

    - All Q(s,a) values set to 0
    - Agent starts with high exploration (ε = 0.9)

- Early Learning:

    - Agent takes many random actions due to high exploration
    - Q-values begin to reflect immediate rewards
    - States near the goal start to gain higher values

- Mid-Learning:

    - Exploration decreases (ε = 0.3)
    - Value information propagates backward from goal
    - Agent finds path to goal but still explores alternatives

- Late Learning:
    - Low exploration (ε = 0.05)
    - Q-values closely approximate optimal values
    - Agent follows near-optimal path most of the time
    - Occasional exploration verifies no better paths exist

The Q-values gradually converge to their optimal values through repeated updates, with information about rewards
propagating backward from the goal state through the bootstrapping process.

**Advantages of Q-Learning**

- **Direct Optimization**: Learns optimal policy regardless of exploration strategy
- **Simplicity**: Conceptually straightforward and easy to implement
- **Flexibility**: Works with any exploratory policy that ensures sufficient state-action visits
- **Foundational**: Forms the basis for many advanced algorithms (DQN, Double Q-learning, etc.)
- **Off-Policy Learning**: Can learn from demonstrations or from a different agent's experience

**Limitations**

- **Maximization Bias**: Tends to overestimate action values due to the max operation
- **Sample Efficiency**: Can be sample-inefficient in large state spaces without additional techniques
- **Function Approximation Challenges**: May diverge when used with function approximation without careful design
- **Exploratory Risk**: May learn policies that are risky during exploration

Q-learning's ability to learn optimal policies while exploring has made it one of the most influential algorithms in
reinforcement learning. Its directness and theoretical guarantees, combined with its practical effectiveness, have
established it as a cornerstone of modern reinforcement learning approaches. The algorithm's extensions and variants
continue to drive advances in the field, particularly in combination with deep learning techniques.

##### Expected SARSA

Expected SARSA represents a sophisticated temporal difference control algorithm that combines elements of both SARSA and
Q-learning. By using the expected value of the next state-action pair rather than a sample, it offers reduced variance
while maintaining flexibility in its policy approach.

**The Expected SARSA Algorithm**

Expected SARSA follows these steps:

**Algorithm: Expected SARSA**

$$
\begin{aligned}
&\textbf{Initialize:} \\
&\quad Q(s,a) \text{ arbitrarily for all states } s \text{ and actions } a \\
\\
&\textbf{For each episode:} \\
&\quad \text{Initialize state } S \\
&\quad \text{Choose action } A \text{ from } S \text{ using policy derived from } Q \text{ (e.g., } \varepsilon\text{-greedy)} \\
&\quad \textbf{For each step of the episode:} \\
&\quad \quad \text{Take action } A, \text{ observe reward } R \text{ and next state } S' \\
&\quad \quad \text{Update: } Q(S,A) \leftarrow Q(S,A) + \alpha\left[R + \gamma \sum_{a'} \pi(a'|S')Q(S',a') - Q(S,A)\right] \\
&\quad \quad \text{Choose next action } A' \text{ from } S' \text{ using policy derived from } Q \\
&\quad \quad S \leftarrow S', A \leftarrow A' \\
&\quad \textbf{Until } S \text{ is terminal}
\end{aligned}
$$

**Alternative compact format:**

$$
\begin{array}{l}
\textbf{Algorithm: Expected SARSA} \\
\hline \\
\textbf{Initialize:} Q(s,a) \text{ arbitrarily} \\
\\
\textbf{Repeat for each episode:} \\
\quad \text{1. Initialize } S, \text{ choose } A \sim \pi(\cdot|S) \\
\quad \text{2. Repeat until } S \text{ is terminal:} \\
\quad \quad \text{a. Take action } A, \text{ observe } R, S' \\
\quad \quad \text{b. } Q(S,A) \leftarrow Q(S,A) + \alpha\left[R + \gamma \sum_{a'} \pi(a'|S')Q(S',a') - Q(S,A)\right] \\
\quad \quad \text{c. Choose } A' \sim \pi(\cdot|S') \\
\quad \quad \text{d. } S \leftarrow S', A \leftarrow A'
\end{array}
$$

The key feature of Expected SARSA is using the expected value $\sum_{a'} \pi(a'|S')Q(S',a')$ instead of either the
sampled next action (SARSA) or the maximum (Q-learning), which reduces variance while maintaining flexibility in policy
choice.

The core update rule is:

$$
Q(S_t,A_t) \leftarrow Q(S_t,A_t) + \alpha\left[R_{t+1} + \gamma \sum_{a'} \pi(a'|S_{t+1})Q(S_{t+1},a') - Q(S_t,A_t)\right]
$$

The key distinction is that instead of using either the Q-value of the actual next action (SARSA) or the maximum Q-value
(Q-learning), Expected SARSA uses the expected value under the current policy.

**Key Characteristics and Mathematical Foundation**

Expected SARSA has several distinctive properties:

- **Expected Value Calculation**: Instead of using a single sample Q(S',A') or the maximum max<sub>a'</sub> Q(S',a'), it
  uses the expected value under the current policy.

- **Mathematical Basis**: The algorithm implements a sample-based approximation of the expectation:

$$
\mathbb{E}*\pi[R*{t+1} + \gamma Q(S_{t+1},A_{t+1}) | S_t, A_t] = \mathbb{E}[R_{t+1} | S_t, A_t] + \gamma \sum_{s'} P(s'|S_t,A_t) \sum_{a'} \pi(a'|s') Q(s',a')
$$

- **Variance Reduction**: By using expected values rather than samples, Expected SARSA reduces update variance compared
  to SARSA, potentially leading to faster learning.

- **Policy Flexibility**: Can function as either on-policy or off-policy depending on the target policy used in the
  expectation calculation.

**Policy Variations in Expected SARSA**

Expected SARSA can be implemented with different policies for the expectation calculation:

- **On-Policy Expected SARSA**: Uses the same policy for action selection and expectation calculation (e.g., ε-greedy)

$$
\sum_{a'} \pi_{\text{ε-greedy}}(a'|S_{t+1})Q(S_{t+1},a')
$$

- **Off-Policy Expected SARSA**: Uses a different policy for expectation calculation than for action selection

$$
\sum_{a'} \pi_{\text{target}}(a'|S_{t+1})Q(S_{t+1},a')
$$

- **Expected SARSA as Q-Learning**: If the target policy is greedy, Expected SARSA becomes equivalent to Q-learning

$$
\sum_{a'} \pi_{\text{greedy}}(a'|S_{t+1})Q(S_{t+1},a') = \max_{a'} Q(S_{t+1},a')
$$

This flexibility allows Expected SARSA to span the spectrum from pure SARSA to Q-learning, or to use any other policy of
interest.

**Implementation for ε-Greedy Policies**

For the common case of ε-greedy policies, the expected value calculation becomes:

$$
\sum_{a'} \pi(a'|S_{t+1})Q(S_{t+1},a') = (1-\varepsilon)\max_{a'} Q(S_{t+1},a') + \varepsilon \sum_{a'} \frac{1}{|\mathcal{A}(S_{t+1})|} Q(S_{t+1},a')
$$

This formula shows how Expected SARSA smoothly interpolates between the greedy action's value (weighted by 1-ε) and the
average value of all actions (weighted by ε).

**Example: Grid World Navigation**

Consider an agent navigating a grid world with a goal and obstacles:

- **Standard SARSA**:
    - Updates based on the actual next action
    - If exploration leads to a bad action, this single sample significantly impacts the update
    - High variance in updates, especially with high exploration rates
- **Q-Learning**:
    - Always assumes the best action will be taken next
    - May be overly optimistic if exploration is necessary
    - Can lead to risky behavior near negative rewards
- **Expected SARSA**:
    - Accounts for the probability of each possible next action
    - With ε-greedy policy and ε = 0.1:
        - 90% weight on best action's value
        - 10% weight distributed across all possible actions
    - Creates more stable updates that realistically account for exploration

**Theoretical Advantages of Expected SARSA**

Expected SARSA offers several theoretical advantages:

- **Unification**: It generalizes both SARSA and Q-learning as special cases, providing a flexible framework that can
  adapt to different requirements.
- **Reduced Variance**: By using expected values rather than samples, it reduces the variance in updates without
  introducing additional bias, leading to potentially faster and more stable learning.
- **Policy-Aware Updates**: The updates account for the actual policy being followed, including exploration, leading to
  more realistic value estimates.
- **Convergence Properties**: Like SARSA and Q-learning, Expected SARSA converges to the optimal policy under standard
  conditions, but often does so more reliably due to lower variance.

###### Computational Considerations

The primary computational trade-off in Expected SARSA is:

- **Increased Computation Per Update**: Calculating the expected value requires summing over all possible actions, which
  is more computationally intensive than SARSA or Q-learning.
- **Potentially Fewer Updates Required**: The reduced variance often leads to faster convergence in terms of the number
  of updates needed, potentially offsetting the increased computation per update.

For problems with small action spaces, the computational overhead is minimal. For large action spaces, approximation
techniques or special cases (like using a greedy target policy) can be employed to maintain efficiency.

###### Practical Performance Comparison

Research has shown that Expected SARSA often outperforms both SARSA and Q-learning, particularly in:

- **Stochastic Environments**: Where the reduced variance is especially beneficial
- **Safety-Critical Domains**: Where exploration risks need to be accounted for
- **Non-Stationary Settings**: Where stable updates lead to better tracking
- **Problems with Significant Randomness**: Where sample-based methods suffer from high variance

**Extended Example: Cliff Walking with Expected SARSA**

In the classic cliff walking problem:

- **SARSA**: Learns a path further from the cliff, sacrificing optimality for safety given exploration
- **Q-learning**: Learns the optimal path along the cliff edge, ignoring exploration risk
- **Expected SARSA**:
    - With high ε: Behaves similarly to SARSA, learning a safer path
    - With decreasing ε: Gradually approaches the optimal path as exploration decreases
    - Combines safety during learning with optimality in the limit
    - Provides more stable updates throughout the learning process

**Variants and Extensions**

Several variants of Expected SARSA have been developed:

- **Expected SARSA(λ)**: Combines Expected SARSA with eligibility traces to propagate updates to multiple previous
  states.
- **Double Expected SARSA**: Uses two sets of Q-values to reduce maximization bias, similar to Double Q-learning.
- **Expected SARSA with Function Approximation**: Extends the algorithm to handle large state spaces using function
  approximators like neural networks.

Expected SARSA represents an elegant advancement in temporal difference learning, combining the best aspects of SARSA
and Q-learning while mitigating their weaknesses. By using expected values to reduce variance without introducing bias,
it offers a more stable and often more efficient approach to reinforcement learning control. Its flexibility allows it
to be adapted to various problems and requirements, making it a valuable tool in the reinforcement learning toolkit.
